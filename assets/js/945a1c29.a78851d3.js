"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[669],{6206(e,n,t){t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>d,frontMatter:()=>l,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline","title":"Lab 2: Cognitive Planning Pipeline","description":"Overview","source":"@site/docs/module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline.md","sourceDirName":"module-4-vision-language-action/labs","slug":"/module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Lab 2: Cognitive Planning Pipeline","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Lab 1: Voice Command Recognition System","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-1-voice-command-recognition-system"},"next":{"title":"Lab 3: Vision-Language Perception Integration","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-3-vision-language-perception-integration"}}');var s=t(4848),i=t(8453);const l={title:"Lab 2: Cognitive Planning Pipeline",sidebar_position:2},o="Lab 2: Cognitive Planning Pipeline",r={},p=[{value:"Overview",id:"overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Lab Setup",id:"lab-setup",level:2},{value:"1. Install Required Dependencies",id:"1-install-required-dependencies",level:3},{value:"2. Set Up LLM Access",id:"2-set-up-llm-access",level:3},{value:"3. Create Planning Package",id:"3-create-planning-package",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Create the LLM Planner Node",id:"step-1-create-the-llm-planner-node",level:3},{value:"Step 2: Create the Plan Validator Node",id:"step-2-create-the-plan-validator-node",level:3},{value:"Step 3: Create the Plan Executor Node",id:"step-3-create-the-plan-executor-node",level:3},{value:"Step 4: Create the Plan Monitor Node",id:"step-4-create-the-plan-monitor-node",level:3},{value:"Step 5: Create Custom Message Definitions",id:"step-5-create-custom-message-definitions",level:3},{value:"Step 6: Create Launch File",id:"step-6-create-launch-file",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"1. Basic Functionality Test",id:"1-basic-functionality-test",level:3},{value:"2. Complex Task Test",id:"2-complex-task-test",level:3},{value:"3. Plan Validation Test",id:"3-plan-validation-test",level:3},{value:"4. Performance Evaluation",id:"4-performance-evaluation",level:3},{value:"Optional Extensions",id:"optional-extensions",level:2},{value:"1. Context-Aware Planning",id:"1-context-aware-planning",level:3},{value:"2. Plan Refinement",id:"2-plan-refinement",level:3},{value:"3. Learning from Execution",id:"3-learning-from-execution",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"What You Learned",id:"what-you-learned",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lab-2-cognitive-planning-pipeline",children:"Lab 2: Cognitive Planning Pipeline"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This lab focuses on implementing a cognitive planning system that translates natural language commands into sequences of robotic actions using Large Language Models (LLMs). You will configure LLM integration for task planning, implement natural language to action sequence translation, and create planning interfaces with ROS 2 navigation stack."}),"\n",(0,s.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure LLM integration for task planning"}),"\n",(0,s.jsx)(n.li,{children:"Implement natural language to action sequence translation"}),"\n",(0,s.jsx)(n.li,{children:"Create planning interfaces with ROS 2 navigation stack"}),"\n",(0,s.jsx)(n.li,{children:"Validate planning accuracy and execution reliability"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed Lab 1 (Voice Command Recognition System)"}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of ROS 2 navigation (Nav2)"}),"\n",(0,s.jsx)(n.li,{children:"OpenAI API account or access to LLM API"}),"\n",(0,s.jsx)(n.li,{children:"Python programming experience"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of task planning concepts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,s.jsx)(n.h3,{id:"1-install-required-dependencies",children:"1. Install Required Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"Ensure you have the necessary packages installed:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai\npip install langchain\npip install langchain-openai\npip install numpy\npip install networkx\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-set-up-llm-access",children:"2. Set Up LLM Access"}),"\n",(0,s.jsx)(n.p,{children:"Set up your OpenAI API key or other LLM access:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export OPENAI_API_KEY="your-api-key-here"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-create-planning-package",children:"3. Create Planning Package"}),"\n",(0,s.jsx)(n.p,{children:"Create a new ROS 2 package for the cognitive planning system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/voice_command_ws/src\nros2 pkg create --build-type ament_python cognitive_planning\ncd cognitive_planning\n"})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-create-the-llm-planner-node",children:"Step 1: Create the LLM Planner Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a ROS 2 node that uses LLMs for task decomposition and planning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom cognitive_planning_interfaces.msg import PlanStep, Plan\nimport json\nimport openai\nfrom typing import List, Dict, Any\nimport time\n\nclass LLMPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planner_node\')\n\n        # Create subscribers and publishers\n        self.command_sub = self.create_subscription(\n            String,\n            \'natural_language_command\',\n            self.command_callback,\n            10\n        )\n\n        self.plan_pub = self.create_publisher(\n            Plan,\n            \'generated_plan\',\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            \'planner_status\',\n            10\n        )\n\n        # Robot capabilities for planning context\n        self.robot_capabilities = [\n            "navigate to location",\n            "pick up object",\n            "place object",\n            "detect object",\n            "open gripper",\n            "close gripper",\n            "move arm to position",\n            "take photo",\n            "stop robot"\n        ]\n\n        # Environment knowledge\n        self.environment_knowledge = {\n            "locations": ["kitchen", "living room", "bedroom", "office", "dining room", "hallway"],\n            "objects": ["cup", "book", "pen", "bottle", "phone", "keys", "laptop", "chair", "table"],\n            "navigation_constraints": {\n                "kitchen_to_bedroom": "through hallway",\n                "office_to_kitchen": "through hallway and dining room"\n            }\n        }\n\n        self.get_logger().info("LLM Planner Node initialized")\n\n    def command_callback(self, msg):\n        """Handle incoming natural language commands"""\n        command = msg.data\n        self.get_logger().info(f"Received command: {command}")\n\n        try:\n            # Generate plan using LLM\n            plan_steps = self.generate_plan_with_llm(command)\n\n            if plan_steps:\n                # Create and publish plan message\n                plan_msg = self.create_plan_message(plan_steps, command)\n                self.plan_pub.publish(plan_msg)\n\n                self.get_logger().info(f"Published plan with {len(plan_steps)} steps")\n            else:\n                self.get_logger().error("Failed to generate plan for command")\n\n        except Exception as e:\n            self.get_logger().error(f"Error generating plan: {str(e)}")\n            status_msg = String()\n            status_msg.data = f"Planning failed: {str(e)}"\n            self.status_pub.publish(status_msg)\n\n    def generate_plan_with_llm(self, command: str) -> List[Dict[str, Any]]:\n        """Generate a plan using LLM"""\n        # Create a structured prompt for the LLM\n        prompt = f"""\n        You are a robot task planner. Given the following command, decompose it into a sequence of executable steps.\n\n        Command: {command}\n\n        Robot capabilities: {\', \'.join(self.robot_capabilities)}\n\n        Environment: {json.dumps(self.environment_knowledge, indent=2)}\n\n        Please provide the plan as a JSON array of steps, where each step has:\n        - id: integer step identifier\n        - action: the specific action to perform\n        - parameters: any required parameters for the action\n        - description: human-readable description of the step\n\n        Example format:\n        [\n            {{\n                "id": 1,\n                "action": "navigate_to",\n                "parameters": {{"location": "kitchen"}},\n                "description": "Move to the kitchen"\n            }},\n            {{\n                "id": 2,\n                "action": "detect_object",\n                "parameters": {{"object_type": "cup"}},\n                "description": "Look for the cup"\n            }}\n        ]\n\n        Only return the JSON array, nothing else.\n        """\n\n        try:\n            # Call the LLM to generate the plan\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[\n                    {"role": "system", "content": "You are a robot task planner. Return only valid JSON as specified."},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.3,\n                max_tokens=1000\n            )\n\n            # Extract the plan from the response\n            plan_text = response.choices[0].message.content.strip()\n\n            # Clean up the response if it contains markdown code block markers\n            if plan_text.startswith("```json"):\n                plan_text = plan_text[7:]  # Remove ```json\n            if plan_text.endswith("```"):\n                plan_text = plan_text[:-3]  # Remove ```\n\n            # Parse the JSON response\n            plan_steps = json.loads(plan_text)\n\n            self.get_logger().info(f"Generated plan with {len(plan_steps)} steps")\n            return plan_steps\n\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f"Failed to parse LLM response as JSON: {str(e)}")\n            self.get_logger().info(f"LLM response: {plan_text}")\n            return []\n        except Exception as e:\n            self.get_logger().error(f"Error calling LLM: {str(e)}")\n            return []\n\n    def create_plan_message(self, plan_steps: List[Dict], original_command: str) -> Plan:\n        """Create a Plan message from plan steps"""\n        plan_msg = Plan()\n        plan_msg.header.stamp = self.get_clock().now().to_msg()\n        plan_msg.header.frame_id = "map"\n        plan_msg.original_command = original_command\n\n        for step_dict in plan_steps:\n            step_msg = PlanStep()\n            step_msg.id = step_dict.get(\'id\', 0)\n            step_msg.action = step_dict.get(\'action\', \'\')\n            step_msg.description = step_dict.get(\'description\', \'\')\n\n            # Convert parameters to JSON string\n            params = step_dict.get(\'parameters\', {})\n            step_msg.parameters = json.dumps(params)\n\n            plan_msg.steps.append(step_msg)\n\n        return plan_msg\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-create-the-plan-validator-node",children:"Step 2: Create the Plan Validator Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a node that validates and refines plans generated by the LLM:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom cognitive_planning_interfaces.msg import Plan, PlanStep\nfrom std_msgs.msg import String\nimport json\nfrom typing import List, Dict, Any\n\nclass PlanValidatorNode(Node):\n    def __init__(self):\n        super().__init__('plan_validator_node')\n\n        # Create subscribers and publishers\n        self.plan_sub = self.create_subscription(\n            Plan,\n            'generated_plan',\n            self.plan_callback,\n            10\n        )\n\n        self.validated_plan_pub = self.create_publisher(\n            Plan,\n            'validated_plan',\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            'validator_status',\n            10\n        )\n\n        # Known valid actions for the robot\n        self.valid_actions = {\n            'navigate_to', 'detect_object', 'grasp_object', 'place_object',\n            'move_arm', 'take_photo', 'stop_robot', 'open_gripper', 'close_gripper'\n        }\n\n        # Known locations in the environment\n        self.known_locations = {\n            'kitchen', 'living room', 'bedroom', 'office', 'dining room', 'hallway'\n        }\n\n        self.get_logger().info(\"Plan Validator Node initialized\")\n\n    def plan_callback(self, msg: Plan):\n        \"\"\"Validate incoming plan\"\"\"\n        self.get_logger().info(f\"Validating plan with {len(msg.steps)} steps\")\n\n        try:\n            # Validate each step in the plan\n            validated_plan = self.validate_plan(msg)\n\n            if validated_plan:\n                # Publish validated plan\n                self.validated_plan_pub.publish(validated_plan)\n                self.get_logger().info(\"Plan validation successful\")\n\n                status_msg = String()\n                status_msg.data = f\"Plan validated successfully: {len(validated_plan.steps)} steps\"\n                self.status_pub.publish(status_msg)\n            else:\n                self.get_logger().error(\"Plan validation failed\")\n\n                status_msg = String()\n                status_msg.data = \"Plan validation failed\"\n                self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error validating plan: {str(e)}\")\n\n    def validate_plan(self, plan_msg: Plan) -> Plan:\n        \"\"\"Validate a plan and return a validated version\"\"\"\n        validated_plan = Plan()\n        validated_plan.header = plan_msg.header\n        validated_plan.original_command = plan_msg.original_command\n        validated_plan.steps = []\n\n        for step in plan_msg.steps:\n            # Validate action\n            if step.action not in self.valid_actions:\n                self.get_logger().warn(f\"Invalid action in plan: {step.action}\")\n                continue\n\n            # Validate parameters\n            try:\n                params = json.loads(step.parameters)\n\n                # Validate location if present\n                if 'location' in params and params['location'] not in self.known_locations:\n                    self.get_logger().warn(f\"Unknown location: {params['location']}\")\n                    continue\n\n                # Validate object type if present\n                if 'object_type' in params:\n                    # Object validation can be more complex, for now just accept\n                    pass\n\n                # If all validations pass, add to validated plan\n                validated_plan.steps.append(step)\n\n            except json.JSONDecodeError:\n                self.get_logger().error(f\"Invalid JSON parameters in step {step.id}\")\n                continue\n\n        # If no valid steps remain, return None\n        if len(validated_plan.steps) == 0:\n            self.get_logger().error(\"No valid steps in plan after validation\")\n            return None\n\n        return validated_plan\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PlanValidatorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-create-the-plan-executor-node",children:"Step 3: Create the Plan Executor Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a node that executes validated plans by interfacing with ROS 2 navigation and other systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom cognitive_planning_interfaces.msg import Plan, PlanStep\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport json\nimport threading\nimport time\n\nclass PlanExecutorNode(Node):\n    def __init__(self):\n        super().__init__(\'plan_executor_node\')\n\n        # Create subscribers\n        self.plan_sub = self.create_subscription(\n            Plan,\n            \'validated_plan\',\n            self.plan_callback,\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            \'executor_status\',\n            10\n        )\n\n        # Create action client for navigation\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # State management\n        self.current_plan = None\n        self.is_executing = False\n        self.execution_thread = None\n\n        # CV bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # Location coordinates (in a real system, these would come from a map)\n        self.location_coordinates = {\n            \'kitchen\': {\'x\': 2.0, \'y\': 1.0, \'theta\': 0.0},\n            \'living room\': {\'x\': 0.0, \'y\': 0.0, \'theta\': 0.0},\n            \'bedroom\': {\'x\': -2.0, \'y\': 1.0, \'theta\': 3.14},\n            \'office\': {\'x\': 0.0, \'y\': 2.0, \'theta\': 1.57},\n            \'dining room\': {\'x\': 1.0, \'y\': -1.0, \'theta\': -1.57},\n            \'hallway\': {\'x\': 0.0, \'y\': 1.0, \'theta\': 0.0}\n        }\n\n        self.get_logger().info("Plan Executor Node initialized")\n\n    def plan_callback(self, msg: Plan):\n        """Handle incoming validated plans"""\n        if self.is_executing:\n            self.get_logger().warn("Plan execution already in progress, skipping new plan")\n            return\n\n        self.get_logger().info(f"Received plan with {len(msg.steps)} steps")\n        self.current_plan = msg\n        self.is_executing = True\n\n        # Start execution in a separate thread\n        self.execution_thread = threading.Thread(target=self.execute_plan, daemon=True)\n        self.execution_thread.start()\n\n    def execute_plan(self):\n        """Execute the current plan step by step"""\n        if not self.current_plan:\n            return\n\n        for i, step_msg in enumerate(self.current_plan.steps):\n            if not self.is_executing:\n                self.get_logger().info("Plan execution stopped")\n                break\n\n            self.get_logger().info(f"Executing step {i+1}/{len(self.current_plan.steps)}: {step_msg.description}")\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f"Executing step {i+1}: {step_msg.description}"\n            self.status_pub.publish(status_msg)\n\n            # Execute the step based on its action\n            success = self.execute_step(step_msg)\n\n            if not success:\n                self.get_logger().error(f"Failed to execute step {i+1}")\n                status_msg = String()\n                status_msg.data = f"Failed to execute step {i+1}: {step_msg.description}"\n                self.status_pub.publish(status_msg)\n                break\n\n        self.is_executing = False\n        self.get_logger().info("Plan execution completed")\n\n        # Final status\n        status_msg = String()\n        status_msg.data = "Plan execution completed"\n        self.status_pub.publish(status_msg)\n\n    def execute_step(self, step_msg: PlanStep) -> bool:\n        """Execute a single plan step"""\n        try:\n            params = json.loads(step_msg.parameters)\n\n            if step_msg.action == \'navigate_to\':\n                return self.execute_navigation_step(params)\n            elif step_msg.action == \'detect_object\':\n                return self.execute_detection_step(params)\n            elif step_msg.action == \'grasp_object\':\n                return self.execute_grasp_step(params)\n            elif step_msg.action == \'place_object\':\n                return self.execute_place_step(params)\n            elif step_msg.action == \'take_photo\':\n                return self.execute_photo_step(params)\n            elif step_msg.action == \'stop_robot\':\n                return self.execute_stop_step(params)\n            else:\n                self.get_logger().warn(f"Unknown action: {step_msg.action}")\n                return False\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f"Invalid JSON parameters in step: {step_msg.parameters}")\n            return False\n        except Exception as e:\n            self.get_logger().error(f"Error executing step: {str(e)}")\n            return False\n\n    def execute_navigation_step(self, params: Dict) -> bool:\n        """Execute a navigation step"""\n        location = params.get(\'location\')\n        if not location:\n            self.get_logger().error("No location specified for navigation")\n            return False\n\n        if location not in self.location_coordinates:\n            self.get_logger().error(f"Unknown location: {location}")\n            return False\n\n        # Wait for navigation server\n        if not self.nav_client.wait_for_server(timeout_sec=5.0):\n            self.get_logger().error("Navigation server not available")\n            return False\n\n        # Create navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n\n        coords = self.location_coordinates[location]\n        goal_msg.pose.pose.position.x = coords[\'x\']\n        goal_msg.pose.pose.position.y = coords[\'y\']\n        goal_msg.pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion (simplified for this example)\n        import math\n        theta = coords[\'theta\']\n        goal_msg.pose.pose.orientation.z = math.sin(theta / 2.0)\n        goal_msg.pose.pose.orientation.w = math.cos(theta / 2.0)\n\n        # Send navigation goal\n        future = self.nav_client.send_goal_async(goal_msg)\n\n        # Wait for result\n        rclpy.spin_until_future_complete(self, future, timeout_sec=30.0)\n\n        if future.result() is not None:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                result_future = goal_handle.get_result_async()\n                rclpy.spin_until_future_complete(self, result_future, timeout_sec=30.0)\n\n                if result_future.result() is not None:\n                    result = result_future.result().result\n                    self.get_logger().info(f"Navigation completed: {result}")\n                    return True\n                else:\n                    self.get_logger().error("Navigation result future was None")\n                    return False\n            else:\n                self.get_logger().error("Navigation goal was rejected")\n                return False\n        else:\n            self.get_logger().error("Navigation goal future was None")\n            return False\n\n    def execute_detection_step(self, params: Dict) -> bool:\n        """Execute a detection step"""\n        object_type = params.get(\'object_type\', \'object\')\n        self.get_logger().info(f"Detecting {object_type}")\n\n        # In a real system, this would interface with perception system\n        # For now, simulate detection\n        time.sleep(2.0)\n        self.get_logger().info(f"Detection of {object_type} completed")\n        return True\n\n    def execute_grasp_step(self, params: Dict) -> bool:\n        """Execute a grasp step"""\n        object_type = params.get(\'object_type\', \'object\')\n        self.get_logger().info(f"Grasping {object_type}")\n\n        # In a real system, this would interface with manipulation system\n        # For now, simulate grasping\n        time.sleep(3.0)\n        self.get_logger().info(f"Grasping of {object_type} completed")\n        return True\n\n    def execute_place_step(self, params: Dict) -> bool:\n        """Execute a place step"""\n        location = params.get(\'location\', \'default\')\n        self.get_logger().info(f"Placing object at {location}")\n\n        # In a real system, this would interface with manipulation system\n        # For now, simulate placing\n        time.sleep(2.5)\n        self.get_logger().info(f"Placing object at {location} completed")\n        return True\n\n    def execute_photo_step(self, params: Dict) -> bool:\n        """Execute a photo step"""\n        self.get_logger().info("Taking photo")\n\n        # In a real system, this would interface with camera system\n        # For now, simulate photo capture\n        time.sleep(1.0)\n        self.get_logger().info("Photo taken")\n        return True\n\n    def execute_stop_step(self, params: Dict) -> bool:\n        """Execute a stop step"""\n        self.get_logger().info("Stopping robot")\n        # In a real system, this would send stop commands to robot\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PlanExecutorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Shutting down Plan Executor Node")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-create-the-plan-monitor-node",children:"Step 4: Create the Plan Monitor Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a node that monitors plan execution and provides feedback:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom cognitive_planning_interfaces.msg import Plan, PlanStep\nfrom std_msgs.msg import String, Bool\nfrom std_msgs.msg import Empty\nimport time\nfrom typing import Dict, Any\n\nclass PlanMonitorNode(Node):\n    def __init__(self):\n        super().__init__(\'plan_monitor_node\')\n\n        # Create subscribers\n        self.plan_sub = self.create_subscription(\n            Plan,\n            \'generated_plan\',\n            self.plan_received_callback,\n            10\n        )\n\n        self.validated_plan_sub = self.create_subscription(\n            Plan,\n            \'validated_plan\',\n            self.validated_plan_callback,\n            10\n        )\n\n        self.executor_status_sub = self.create_subscription(\n            String,\n            \'executor_status\',\n            self.executor_status_callback,\n            10\n        )\n\n        # Create publishers\n        self.monitor_status_pub = self.create_publisher(\n            String,\n            \'monitor_status\',\n            10\n        )\n\n        self.plan_success_pub = self.create_publisher(\n            Bool,\n            \'plan_success\',\n            10\n        )\n\n        # State tracking\n        self.current_plan = None\n        self.plan_start_time = None\n        self.steps_completed = 0\n        self.total_steps = 0\n\n        self.get_logger().info("Plan Monitor Node initialized")\n\n    def plan_received_callback(self, msg: Plan):\n        """Handle when a plan is received"""\n        self.current_plan = msg\n        self.plan_start_time = time.time()\n        self.steps_completed = 0\n        self.total_steps = len(msg.steps)\n\n        self.get_logger().info(f"Monitoring plan with {self.total_steps} steps")\n\n        status_msg = String()\n        status_msg.data = f"Plan received with {self.total_steps} steps"\n        self.monitor_status_pub.publish(status_msg)\n\n    def validated_plan_callback(self, msg: Plan):\n        """Handle when a plan is validated"""\n        self.total_steps = len(msg.steps)\n        self.get_logger().info(f"Plan validated with {self.total_steps} steps")\n\n        status_msg = String()\n        status_msg.data = f"Plan validated with {self.total_steps} steps"\n        self.monitor_status_pub.publish(status_msg)\n\n    def executor_status_callback(self, msg: String):\n        """Handle executor status updates"""\n        status_text = msg.data\n\n        # Check if this is a step completion message\n        if "Executing step" in status_text:\n            # Extract step number from message\n            try:\n                # Simple parsing: "Executing step X: description"\n                parts = status_text.split(\':\')\n                if len(parts) > 0:\n                    step_part = parts[0]\n                    if \'step\' in step_part:\n                        step_num = int(step_part.split()[2])  # Get the step number\n                        self.steps_completed = step_num - 1  # Update completed steps\n            except (ValueError, IndexError):\n                pass  # Could not parse step number\n\n        # Check if execution completed\n        if "Plan execution completed" in status_text:\n            success_msg = Bool()\n            success_msg.data = True\n            self.plan_success_pub.publish(success_msg)\n\n            elapsed_time = time.time() - self.plan_start_time if self.plan_start_time else 0\n            self.get_logger().info(f"Plan completed in {elapsed_time:.2f} seconds")\n\n        # Update monitor status\n        progress = f"{self.steps_completed}/{self.total_steps}" if self.total_steps > 0 else "0/0"\n        status_msg = String()\n        status_msg.data = f"Progress: {progress}. Status: {status_text}"\n        self.monitor_status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PlanMonitorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-create-custom-message-definitions",children:"Step 5: Create Custom Message Definitions"}),"\n",(0,s.jsx)(n.p,{children:"Create the custom message definitions needed for the planning system. First, create the package structure:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/voice_command_ws/src/cognitive_planning/cognitive_planning_interfaces/msg\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:"PlanStep.msg"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"# PlanStep.msg\nint32 id\nstring action\nstring parameters\nstring description\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:"Plan.msg"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"# Plan.msg\nstd_msgs/Header header\nstring original_command\nPlanStep[] steps\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Update the ",(0,s.jsx)(n.code,{children:"package.xml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>cognitive_planning_interfaces</name>\n  <version>0.0.0</version>\n  <description>Custom message definitions for cognitive planning</description>\n  <maintainer email="user@example.com">user</maintainer>\n  <license>Apache-2.0</license>\n\n  <buildtool_depend>ament_cmake</buildtool_depend>\n  <buildtool_depend>rosidl_default_generators</buildtool_depend>\n\n  <depend>std_msgs</depend>\n\n  <exec_depend>rosidl_default_runtime</exec_depend>\n\n  <test_depend>ament_lint_auto</test_depend>\n  <test_depend>ament_lint_common</test_depend>\n\n  <member_of_group>rosidl_interface_packages</member_of_group>\n\n  <export>\n    <build_type>ament_cmake</build_type>\n  </export>\n</package>\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Update the ",(0,s.jsx)(n.code,{children:"setup.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\n\npackage_name = 'cognitive_planning_interfaces'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='user',\n    maintainer_email='user@example.com',\n    description='Custom message definitions for cognitive planning',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n        ],\n    },\n)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-6-create-launch-file",children:"Step 6: Create Launch File"}),"\n",(0,s.jsx)(n.p,{children:"Create a launch file to start all cognitive planning nodes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # LLM Planner node\n        Node(\n            package='cognitive_planning',\n            executable='llm_planner_node',\n            name='llm_planner_node',\n            output='screen'\n        ),\n\n        # Plan Validator node\n        Node(\n            package='cognitive_planning',\n            executable='plan_validator_node',\n            name='plan_validator_node',\n            output='screen'\n        ),\n\n        # Plan Executor node\n        Node(\n            package='cognitive_planning',\n            executable='plan_executor_node',\n            name='plan_executor_node',\n            output='screen'\n        ),\n\n        # Plan Monitor node\n        Node(\n            package='cognitive_planning',\n            executable='plan_monitor_node',\n            name='plan_monitor_node',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"1-basic-functionality-test",children:"1. Basic Functionality Test"}),"\n",(0,s.jsx)(n.p,{children:"Test the cognitive planning system with simple commands:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Go to the kitchen and find the red cup"'}),"\n",(0,s.jsx)(n.li,{children:'"Navigate to the office and take a photo"'}),"\n",(0,s.jsx)(n.li,{children:'"Move to the living room and stop"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-complex-task-test",children:"2. Complex Task Test"}),"\n",(0,s.jsx)(n.p,{children:"Test with more complex multi-step commands:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Go to the kitchen, pick up the cup, then go to the living room and place it on the table"'}),"\n",(0,s.jsx)(n.li,{children:'"Find the keys in the bedroom, then navigate to the office"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-plan-validation-test",children:"3. Plan Validation Test"}),"\n",(0,s.jsx)(n.p,{children:"Test the system's ability to validate and reject invalid plans:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Commands with unknown locations"}),"\n",(0,s.jsx)(n.li,{children:"Commands with invalid actions"}),"\n",(0,s.jsx)(n.li,{children:"Commands that conflict with robot capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-performance-evaluation",children:"4. Performance Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Record and analyze:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Planning time (from command to plan generation)"}),"\n",(0,s.jsx)(n.li,{children:"Plan validation accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Execution success rate"}),"\n",(0,s.jsx)(n.li,{children:"Error recovery capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"optional-extensions",children:"Optional Extensions"}),"\n",(0,s.jsx)(n.h3,{id:"1-context-aware-planning",children:"1. Context-Aware Planning"}),"\n",(0,s.jsx)(n.p,{children:"Enhance the system to consider context such as:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Robot's current location and state"}),"\n",(0,s.jsx)(n.li,{children:"Environmental conditions"}),"\n",(0,s.jsx)(n.li,{children:"Previous commands and outcomes"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-plan-refinement",children:"2. Plan Refinement"}),"\n",(0,s.jsx)(n.p,{children:"Implement a plan refinement system that:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitors execution in real-time"}),"\n",(0,s.jsx)(n.li,{children:"Adjusts plans based on environmental changes"}),"\n",(0,s.jsx)(n.li,{children:"Handles partial plan failures gracefully"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-learning-from-execution",children:"3. Learning from Execution"}),"\n",(0,s.jsx)(n.p,{children:"Add a learning component that:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Records successful plan executions"}),"\n",(0,s.jsx)(n.li,{children:"Improves future planning based on execution outcomes"}),"\n",(0,s.jsx)(n.li,{children:"Adapts to user preferences over time"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"How does the LLM-based planning approach compare to traditional symbolic planning methods?"}),"\n",(0,s.jsx)(n.li,{children:"What are the main challenges in translating natural language to executable robotic actions?"}),"\n",(0,s.jsx)(n.li,{children:"How could you improve the system's ability to handle ambiguous or incomplete commands?"}),"\n",(0,s.jsx)(n.li,{children:"What safety considerations should be addressed in LLM-driven robotic planning?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"what-you-learned",children:"What You Learned"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you implemented a comprehensive cognitive planning system that translates natural language commands into executable robotic actions using Large Language Models. You learned how to integrate LLMs with ROS 2 navigation systems, validate and execute complex multi-step plans, and monitor plan execution for reliability. You also explored the challenges and opportunities in LLM-driven robotic planning and considered potential improvements for real-world applications."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>l,x:()=>o});var a=t(6540);const s={},i=a.createContext(s);function l(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);