"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[768],{4727(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-2-digital-twin/chapter-4","title":"Chapter 4: Unity for High-Fidelity Interaction","description":"Introduction","source":"@site/docs/module-2-digital-twin/chapter-4.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/chapter-4","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-4.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Chapter 4: Unity for High-Fidelity Interaction","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Robot Modeling with URDF & Sensors","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-3"},"next":{"title":"Chapter 2: Gazebo for Robotics Simulation","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-2"}}');var o=i(4848),a=i(8453);const s={title:"Chapter 4: Unity for High-Fidelity Interaction",sidebar_position:1},r="Chapter 4: Unity for High-Fidelity Interaction",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Unity 3D Environment and Physics Engine Capabilities",id:"unity-3d-environment-and-physics-engine-capabilities",level:2},{value:"Rendering Pipeline",id:"rendering-pipeline",level:3},{value:"Physics Engine",id:"physics-engine",level:3},{value:"Scene Management",id:"scene-management",level:3},{value:"High-Fidelity Graphics Rendering and Lighting Systems",id:"high-fidelity-graphics-rendering-and-lighting-systems",level:2},{value:"Lighting Systems",id:"lighting-systems",level:3},{value:"Material and Shader Systems",id:"material-and-shader-systems",level:3},{value:"Post-Processing Effects",id:"post-processing-effects",level:3},{value:"Unity-ROS 2 Integration Setup",id:"unity-ros-2-integration-setup",level:2},{value:"Unity Robotics Hub",id:"unity-robotics-hub",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Basic Connection Code",id:"basic-connection-code",level:3},{value:"Message Types and Communication",id:"message-types-and-communication",level:3},{value:"Advanced Sensor Simulation (RGB, Depth, Semantic Segmentation)",id:"advanced-sensor-simulation-rgb-depth-semantic-segmentation",level:2},{value:"RGB Camera Simulation",id:"rgb-camera-simulation",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"User Interface Development for Robot Control",id:"user-interface-development-for-robot-control",level:2},{value:"Unity UI System",id:"unity-ui-system",level:3},{value:"Robot Control Interface Example",id:"robot-control-interface-example",level:3},{value:"Dashboard and Visualization",id:"dashboard-and-visualization",level:3},{value:"VR/AR Integration for Immersive Interaction",id:"vrar-integration-for-immersive-interaction",level:2},{value:"VR Setup for Robot Teleoperation",id:"vr-setup-for-robot-teleoperation",level:3},{value:"AR Integration",id:"ar-integration",level:3},{value:"What You Learned",id:"what-you-learned",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-4-unity-for-high-fidelity-interaction",children:"Chapter 4: Unity for High-Fidelity Interaction"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Unity is a powerful 3D development platform that provides high-fidelity graphics rendering and physics simulation capabilities. While traditionally used for game development, Unity has emerged as an important tool for robotics simulation, offering photorealistic environments and advanced sensor simulation. This chapter explores Unity's capabilities for robotics applications, integration with ROS 2, and implementation of advanced sensor systems for immersive robot interaction."}),"\n",(0,o.jsx)(n.h2,{id:"unity-3d-environment-and-physics-engine-capabilities",children:"Unity 3D Environment and Physics Engine Capabilities"}),"\n",(0,o.jsx)(n.p,{children:"Unity's architecture provides several key capabilities that make it valuable for robotics simulation:"}),"\n",(0,o.jsx)(n.h3,{id:"rendering-pipeline",children:"Rendering Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Unity offers multiple rendering pipeline options:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Built-in Render Pipeline"}),": Standard rendering with good performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Universal Render Pipeline (URP)"}),": Optimized for multi-platform deployment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High Definition Render Pipeline (HDRP)"}),": Maximum visual fidelity for high-end systems"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Each pipeline offers different trade-offs between visual quality and performance, allowing developers to select the appropriate option based on their hardware constraints and fidelity requirements."}),"\n",(0,o.jsx)(n.h3,{id:"physics-engine",children:"Physics Engine"}),"\n",(0,o.jsx)(n.p,{children:"Unity's physics engine is based on NVIDIA's PhysX technology, providing:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Realistic collision detection and response"}),"\n",(0,o.jsx)(n.li,{children:"Advanced joint systems for articulated bodies"}),"\n",(0,o.jsx)(n.li,{children:"Cloth and fluid simulation capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Raycasting and geometric queries"}),"\n",(0,o.jsx)(n.li,{children:"Custom physics materials with friction and bounciness properties"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"scene-management",children:"Scene Management"}),"\n",(0,o.jsx)(n.p,{children:"Unity's scene system allows for complex environment construction:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Hierarchical object organization"}),"\n",(0,o.jsx)(n.li,{children:"Prefab instantiation for reusable components"}),"\n",(0,o.jsx)(n.li,{children:"Lighting systems with real-time and baked solutions"}),"\n",(0,o.jsx)(n.li,{children:"Terrain generation and modification tools"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"high-fidelity-graphics-rendering-and-lighting-systems",children:"High-Fidelity Graphics Rendering and Lighting Systems"}),"\n",(0,o.jsx)(n.p,{children:"Unity excels at creating photorealistic environments through advanced rendering techniques:"}),"\n",(0,o.jsx)(n.h3,{id:"lighting-systems",children:"Lighting Systems"}),"\n",(0,o.jsx)(n.p,{children:"Unity provides multiple lighting approaches:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Lighting"}),": Dynamic lights that respond to scene changes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Baked Lighting"}),": Precomputed lighting for optimal performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mixed Lighting"}),": Combination of real-time and baked lighting"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:"// Example of configuring a light source for realistic rendering\npublic class RobotLighting : MonoBehaviour\n{\n    void Start()\n    {\n        Light robotLight = GetComponent<Light>();\n        robotLight.type = LightType.Spot;\n        robotLight.spotAngle = 60f;\n        robotLight.range = 10f;\n        robotLight.intensity = 2f;\n        robotLight.shadows = LightShadows.Soft;\n    }\n}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"material-and-shader-systems",children:"Material and Shader Systems"}),"\n",(0,o.jsx)(n.p,{children:"Unity's material system allows for realistic surface properties:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Physically Based Rendering (PBR) materials"}),"\n",(0,o.jsx)(n.li,{children:"Custom shader development for specialized effects"}),"\n",(0,o.jsx)(n.li,{children:"Texture mapping with normal, specular, and roughness maps"}),"\n",(0,o.jsx)(n.li,{children:"Real-time reflection and refraction effects"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"post-processing-effects",children:"Post-Processing Effects"}),"\n",(0,o.jsx)(n.p,{children:"High-fidelity rendering includes post-processing effects:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Ambient occlusion for realistic shadowing"}),"\n",(0,o.jsx)(n.li,{children:"Bloom for bright light sources"}),"\n",(0,o.jsx)(n.li,{children:"Depth of field for camera focus effects"}),"\n",(0,o.jsx)(n.li,{children:"Motion blur for realistic movement"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"unity-ros-2-integration-setup",children:"Unity-ROS 2 Integration Setup"}),"\n",(0,o.jsx)(n.p,{children:"Unity can communicate with ROS 2 systems through several integration approaches:"}),"\n",(0,o.jsx)(n.h3,{id:"unity-robotics-hub",children:"Unity Robotics Hub"}),"\n",(0,o.jsx)(n.p,{children:"The Unity Robotics Hub provides official integration tools:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"ROS-TCP-Connector for message communication"}),"\n",(0,o.jsx)(n.li,{children:"URDF Importer for robot model loading"}),"\n",(0,o.jsx)(n.li,{children:"Sample environments and tutorials"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,o.jsx)(n.p,{children:"To set up Unity-ROS 2 integration:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Install Unity 2021.3 LTS or later"}),"\n",(0,o.jsx)(n.li,{children:"Import the Unity Robotics packages via the Package Manager"}),"\n",(0,o.jsx)(n.li,{children:"Configure the ROS-TCP-Connector settings"}),"\n",(0,o.jsx)(n.li,{children:"Set up the ROS 2 bridge"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"basic-connection-code",children:"Basic Connection Code"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:'using Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class RobotController : MonoBehaviour\n{\n    ROSConnection ros;\n    string rosIP = "127.0.0.1";\n    int rosPort = 10000;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Initialize(rosIP, rosPort);\n    }\n\n    void SendJointCommand()\n    {\n        var jointMsg = new Sensor_msgs.JointStateMsg();\n        jointMsg.name = new string[] { "joint1", "joint2" };\n        jointMsg.position = new double[] { 0.5, -0.3 };\n\n        ros.Send("joint_commands", jointMsg);\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"message-types-and-communication",children:"Message Types and Communication"}),"\n",(0,o.jsx)(n.p,{children:"Unity-ROS 2 integration supports standard ROS message types:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Sensor data (images, LIDAR, IMU)"}),"\n",(0,o.jsx)(n.li,{children:"Control commands (joint positions, velocities)"}),"\n",(0,o.jsx)(n.li,{children:"Transform data (TF trees)"}),"\n",(0,o.jsx)(n.li,{children:"Custom message types"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"advanced-sensor-simulation-rgb-depth-semantic-segmentation",children:"Advanced Sensor Simulation (RGB, Depth, Semantic Segmentation)"}),"\n",(0,o.jsx)(n.p,{children:"Unity provides sophisticated sensor simulation capabilities:"}),"\n",(0,o.jsx)(n.h3,{id:"rgb-camera-simulation",children:"RGB Camera Simulation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:"using UnityEngine;\nusing Unity.Robotics.Sensoring;\n\npublic class RGBCamera : MonoBehaviour\n{\n    public int width = 640;\n    public int height = 480;\n    public float fieldOfView = 60f;\n\n    Camera cam;\n    RenderTexture renderTexture;\n\n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        cam.fieldOfView = fieldOfView;\n\n        renderTexture = new RenderTexture(width, height, 24);\n        cam.targetTexture = renderTexture;\n    }\n\n    void Update()\n    {\n        // Capture and process image data\n        RenderTexture.active = renderTexture;\n        Texture2D image = new Texture2D(width, height, TextureFormat.RGB24, false);\n        image.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n        image.Apply();\n\n        // Convert to ROS message format\n        // Send via ROS connection\n    }\n}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,o.jsx)(n.p,{children:"Unity can generate depth maps using the camera's depth buffer:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:"using UnityEngine;\n\npublic class DepthCamera : MonoBehaviour\n{\n    public int width = 640;\n    public int height = 480;\n    public float maxRange = 10.0f;\n\n    Camera cam;\n    RenderTexture depthTexture;\n\n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        cam.depthTextureMode = DepthTextureMode.Depth;\n\n        depthTexture = new RenderTexture(width, height, 24, RenderTextureFormat.RFloat);\n        cam.targetTexture = depthTexture;\n    }\n\n    float[] GetDepthData()\n    {\n        RenderTexture.active = depthTexture;\n        Texture2D depthTex = new Texture2D(width, height, TextureFormat.RFloat, false);\n        depthTex.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n        depthTex.Apply();\n\n        Color[] depthColors = depthTex.GetPixels();\n        float[] depthValues = new float[width * height];\n\n        for (int i = 0; i < depthValues.Length; i++)\n        {\n            // Convert from 0-1 range to actual distance\n            depthValues[i] = depthColors[i].r * maxRange;\n        }\n\n        DestroyImmediate(depthTex);\n        return depthValues;\n    }\n}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,o.jsx)(n.p,{children:"Semantic segmentation assigns class labels to each pixel:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:"using UnityEngine;\n\npublic class SemanticSegmentation : MonoBehaviour\n{\n    public int width = 640;\n    public int height = 480;\n\n    // Dictionary mapping materials to class IDs\n    public Dictionary<Material, int> classMapping = new Dictionary<Material, int>();\n\n    Camera cam;\n    RenderTexture segTexture;\n\n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        cam.backgroundColor = Color.black;\n        cam.clearFlags = CameraClearFlags.SolidColor;\n\n        segTexture = new RenderTexture(width, height, 0, RenderTextureFormat.ARGB32);\n        cam.targetTexture = segTexture;\n    }\n\n    int[] GetSegmentationData()\n    {\n        // Render objects with their class-specific colors\n        // Convert to class IDs\n        RenderTexture.active = segTexture;\n        Texture2D segTex = new Texture2D(width, height, TextureFormat.RGB24, false);\n        segTex.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n        segTex.Apply();\n\n        Color[] segColors = segTex.GetPixels();\n        int[] classIds = new int[width * height];\n\n        for (int i = 0; i < segColors.Length; i++)\n        {\n            // Map color to class ID\n            classIds[i] = ColorToClassId(segColors[i]);\n        }\n\n        DestroyImmediate(segTex);\n        return classIds;\n    }\n\n    int ColorToClassId(Color color)\n    {\n        // Convert color to closest class ID\n        // Implementation depends on color encoding scheme\n        return 0;\n    }\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"user-interface-development-for-robot-control",children:"User Interface Development for Robot Control"}),"\n",(0,o.jsx)(n.p,{children:"Unity provides comprehensive tools for creating intuitive user interfaces:"}),"\n",(0,o.jsx)(n.h3,{id:"unity-ui-system",children:"Unity UI System"}),"\n",(0,o.jsx)(n.p,{children:"Unity's UI system includes:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Canvas for UI element positioning"}),"\n",(0,o.jsx)(n.li,{children:"Various UI controls (buttons, sliders, toggles)"}),"\n",(0,o.jsx)(n.li,{children:"Event system for user interaction"}),"\n",(0,o.jsx)(n.li,{children:"Layout groups for responsive design"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"robot-control-interface-example",children:"Robot Control Interface Example"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Geometry;\n\npublic class RobotControlUI : MonoBehaviour\n{\n    public Slider linearVelocitySlider;\n    public Slider angularVelocitySlider;\n    public Button moveButton;\n\n    ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        moveButton.onClick.AddListener(OnMoveButtonClicked);\n        linearVelocitySlider.onValueChanged.AddListener(OnLinearVelocityChanged);\n        angularVelocitySlider.onValueChanged.AddListener(OnAngularVelocityChanged);\n    }\n\n    void OnMoveButtonClicked()\n    {\n        var twistMsg = new Geometry_msgs.TwistMsg();\n        twistMsg.linear = new Geometry_msgs.Vector3Msg(0, 0, 0);\n        twistMsg.angular = new Geometry_msgs.Vector3Msg(0, 0, 0);\n\n        twistMsg.linear.x = (float)linearVelocitySlider.value;\n        twistMsg.angular.z = (float)angularVelocitySlider.value;\n\n        ros.Send("cmd_vel", twistMsg);\n    }\n\n    void OnLinearVelocityChanged(float value)\n    {\n        // Update UI feedback\n    }\n\n    void OnAngularVelocityChanged(float value)\n    {\n        // Update UI feedback\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"dashboard-and-visualization",children:"Dashboard and Visualization"}),"\n",(0,o.jsx)(n.p,{children:"Creating comprehensive dashboards for robot monitoring:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\n\npublic class RobotDashboard : MonoBehaviour\n{\n    public Text positionText;\n    public Text batteryText;\n    public Text statusText;\n    public Image lidarDisplay;\n\n    // Update dashboard with robot data\n    public void UpdateDashboard(Vector3 position, float batteryLevel, string status, float[] lidarData)\n    {\n        positionText.text = $"Position: {position.x:F2}, {position.y:F2}, {position.z:F2}";\n        batteryText.text = $"Battery: {batteryLevel:F1}%";\n        statusText.text = $"Status: {status}";\n\n        UpdateLidarDisplay(lidarData);\n    }\n\n    void UpdateLidarDisplay(float[] lidarData)\n    {\n        // Process LIDAR data for visualization\n        // Create 2D representation of LIDAR scan\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"vrar-integration-for-immersive-interaction",children:"VR/AR Integration for Immersive Interaction"}),"\n",(0,o.jsx)(n.p,{children:"Unity's support for VR and AR platforms enables immersive robot teleoperation:"}),"\n",(0,o.jsx)(n.h3,{id:"vr-setup-for-robot-teleoperation",children:"VR Setup for Robot Teleoperation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:"using UnityEngine;\nusing UnityEngine.XR;\n\npublic class VRTeleoperation : MonoBehaviour\n{\n    public Transform robotModel;\n    public Camera vrCamera;\n\n    void Update()\n    {\n        // Map VR controller inputs to robot commands\n        if (XRSettings.enabled)\n        {\n            HandleVRInput();\n        }\n    }\n\n    void HandleVRInput()\n    {\n        // Get VR controller poses\n        // Calculate robot movement based on hand position\n        // Send commands to robot\n    }\n}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"ar-integration",children:"AR Integration"}),"\n",(0,o.jsx)(n.p,{children:"For augmented reality applications:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"World tracking and environment mapping"}),"\n",(0,o.jsx)(n.li,{children:"Overlay of robot data onto real-world view"}),"\n",(0,o.jsx)(n.li,{children:"Gesture recognition for intuitive control"}),"\n",(0,o.jsx)(n.li,{children:"Spatial awareness for safe operation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"what-you-learned",children:"What You Learned"}),"\n",(0,o.jsx)(n.p,{children:"In this chapter, you learned about Unity's capabilities for high-fidelity robotics simulation, including its rendering pipeline, physics engine, and lighting systems. You explored how to integrate Unity with ROS 2 for bidirectional communication, implement advanced sensor simulation including RGB, depth, and semantic segmentation cameras, and develop user interfaces for robot control. You also discovered how to leverage VR/AR technologies for immersive robot interaction. This knowledge enables you to create sophisticated, photorealistic simulation environments for robotics applications."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>r});var t=i(6540);const o={},a=t.createContext(o);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);