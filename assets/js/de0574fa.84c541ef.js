"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[4589],{3591(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vision-language-action/labs/lab-3-vision-language-perception-integration","title":"Lab 3: Vision-Language Perception Integration","description":"Overview","source":"@site/docs/module-4-vision-language-action/labs/lab-3-vision-language-perception-integration.md","sourceDirName":"module-4-vision-language-action/labs","slug":"/module-4-vision-language-action/labs/lab-3-vision-language-perception-integration","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-3-vision-language-perception-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-3-vision-language-perception-integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Lab 3: Vision-Language Perception Integration","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Lab 2: Cognitive Planning Pipeline","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline"},"next":{"title":"Lab 4: Capstone - The Autonomous Humanoid","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid"}}');var s=i(4848),o=i(8453);const r={title:"Lab 3: Vision-Language Perception Integration",sidebar_position:3},a="Lab 3: Vision-Language Perception Integration",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Lab Setup",id:"lab-setup",level:2},{value:"1. Install Required Dependencies",id:"1-install-required-dependencies",level:3},{value:"2. Create Vision-Language Package",id:"2-create-vision-language-package",level:3},{value:"3. Set Up Model Downloads",id:"3-set-up-model-downloads",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Create the Vision-Language Perception Node",id:"step-1-create-the-vision-language-perception-node",level:3},{value:"Step 2: Create the Object Identification Node",id:"step-2-create-the-object-identification-node",level:3},{value:"Step 3: Create the LLM Integration Node",id:"step-3-create-the-llm-integration-node",level:3},{value:"Step 4: Create the Performance Monitor Node",id:"step-4-create-the-performance-monitor-node",level:3},{value:"Step 5: Create Launch File",id:"step-5-create-launch-file",level:3},{value:"Testing and Evaluation",id:"testing-and-evaluation",level:2},{value:"1. Basic Functionality Test",id:"1-basic-functionality-test",level:3},{value:"2. Complex Query Test",id:"2-complex-query-test",level:3},{value:"3. Performance Evaluation",id:"3-performance-evaluation",level:3},{value:"4. Accuracy Assessment",id:"4-accuracy-assessment",level:3},{value:"Optional Extensions",id:"optional-extensions",level:2},{value:"1. Real-time Processing Optimization",id:"1-real-time-processing-optimization",level:3},{value:"2. Advanced Scene Understanding",id:"2-advanced-scene-understanding",level:3},{value:"3. Multimodal Fusion",id:"3-multimodal-fusion",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"What You Learned",id:"what-you-learned",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lab-3-vision-language-perception-integration",children:"Lab 3: Vision-Language Perception Integration"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This lab focuses on implementing vision-language perception systems that combine computer vision with language understanding. You will combine computer vision with language understanding, implement object identification from natural language queries, integrate visual perception with LLM decision making, and test recognition accuracy and response time."}),"\n",(0,s.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combine computer vision with language understanding"}),"\n",(0,s.jsx)(n.li,{children:"Implement object identification from natural language queries"}),"\n",(0,s.jsx)(n.li,{children:"Integrate visual perception with LLM decision making"}),"\n",(0,s.jsx)(n.li,{children:"Test recognition accuracy and response time"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed Lab 1 and Lab 2"}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of computer vision concepts"}),"\n",(0,s.jsx)(n.li,{children:"Python programming experience with OpenCV and PyTorch"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of transformer models and CLIP"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 environment with vision processing capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,s.jsx)(n.h3,{id:"1-install-required-dependencies",children:"1. Install Required Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"Install the necessary packages for vision-language integration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision\npip install transformers\npip install openai\npip install opencv-python\npip install pillow\npip install clip @ git+https://github.com/openai/CLIP.git\npip install supervision\npip install ultralytics\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-create-vision-language-package",children:"2. Create Vision-Language Package"}),"\n",(0,s.jsx)(n.p,{children:"Create a new ROS 2 package for vision-language integration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/voice_command_ws/src\nros2 pkg create --build-type ament_python vision_language_perception\ncd vision_language_perception\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-set-up-model-downloads",children:"3. Set Up Model Downloads"}),"\n",(0,s.jsx)(n.p,{children:"For this lab, we'll use pre-trained models. The CLIP model will be downloaded automatically when first used."}),"\n",(0,s.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-create-the-vision-language-perception-node",children:"Step 1: Create the Vision-Language Perception Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a ROS 2 node that integrates computer vision with language understanding:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nimport clip\nfrom PIL import Image as PILImage\nfrom typing import List, Dict, Tuple\nimport json\n\nclass VisionLanguagePerceptionNode(Node):\n    def __init__(self):\n        super().__init__('vision_language_perception_node')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Load CLIP model\n        self.get_logger().info(\"Loading CLIP model...\")\n        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.get_device())\n        self.clip_model.eval()\n\n        # Create subscribers and publishers\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.query_sub = self.create_subscription(\n            String,\n            'vision_language_query',\n            self.query_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            'vision_language_detections',\n            10\n        )\n\n        self.result_pub = self.create_publisher(\n            String,\n            'vision_language_result',\n            10\n        )\n\n        # Store the latest image for processing\n        self.latest_image = None\n        self.latest_query = None\n\n        # Object detection model (using a simple approach for this example)\n        self.object_detector = self._initialize_object_detector()\n\n        self.get_logger().info(\"Vision-Language Perception Node initialized\")\n\n    def get_device(self):\n        \"\"\"Get the appropriate device (GPU if available)\"\"\"\n        if torch.cuda.is_available():\n            device = \"cuda\"\n        else:\n            device = \"cpu\"\n        return device\n\n    def _initialize_object_detector(self):\n        \"\"\"Initialize object detection model (using YOLOv5 as example)\"\"\"\n        # In a real implementation, you would load a pre-trained model\n        # For this example, we'll create a mock detector\n        class MockDetector:\n            def detect(self, image):\n                # This would be replaced with actual object detection\n                # For now, we'll simulate detection of common objects\n                height, width = image.shape[:2]\n                detections = []\n\n                # Simulate some common object detections\n                detections.append({\n                    'label': 'cup',\n                    'confidence': 0.85,\n                    'bbox': [width//4, height//4, width//3, height//3]  # [x, y, w, h]\n                })\n                detections.append({\n                    'label': 'book',\n                    'confidence': 0.78,\n                    'bbox': [width//2, height//3, width//5, height//4]\n                })\n                detections.append({\n                    'label': 'phone',\n                    'confidence': 0.92,\n                    'bbox': [width//3, height//2, width//6, height//6]\n                })\n\n                return detections\n\n        return MockDetector()\n\n    def image_callback(self, msg: Image):\n        \"\"\"Handle incoming camera images\"\"\"\n        try:\n            # Convert ROS Image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.latest_image = cv_image\n            self.get_logger().info(\"Received image for vision-language processing\")\n        except Exception as e:\n            self.get_logger().error(f\"Error converting image: {str(e)}\")\n\n    def query_callback(self, msg: String):\n        \"\"\"Handle incoming vision-language queries\"\"\"\n        query = msg.data\n        self.latest_query = query\n        self.get_logger().info(f\"Received vision-language query: {query}\")\n\n        # Process the query if we have a recent image\n        if self.latest_image is not None:\n            self.process_vision_language_query(self.latest_image, query)\n\n    def process_vision_language_query(self, image: np.ndarray, query: str):\n        \"\"\"Process a vision-language query using CLIP\"\"\"\n        try:\n            # Convert OpenCV image to PIL format for CLIP\n            pil_image = PILImage.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n            # Prepare text for CLIP\n            texts = [query, \"background\"]  # Include background as negative class\n            text_tokens = clip.tokenize(texts).to(self.get_device())\n\n            # Preprocess image for CLIP\n            image_input = self.clip_preprocess(pil_image).unsqueeze(0).to(self.get_device())\n\n            # Get CLIP predictions\n            with torch.no_grad():\n                logits_per_image, logits_per_text = self.clip_model(image_input, text_tokens)\n                probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n\n            # Get the probability that the query matches the image content\n            query_match_prob = probs[0]\n\n            # Also run object detection to get bounding boxes\n            detections = self.object_detector.detect(image)\n\n            # Use CLIP to score each detected object against the query\n            scored_detections = []\n            for detection in detections:\n                # Create a cropped image of the detected object\n                x, y, w, h = detection['bbox']\n                obj_image = image[y:y+h, x:x+w]\n\n                # Convert to PIL and preprocess for CLIP\n                obj_pil = PILImage.fromarray(cv2.cvtColor(obj_image, cv2.COLOR_BGR2RGB))\n                obj_input = self.clip_preprocess(obj_pil).unsqueeze(0).to(self.get_device())\n\n                # Get CLIP similarity for this specific object\n                text_for_obj = [query, f\"not {query}\", \"background\"]\n                text_tokens_obj = clip.tokenize(text_for_obj).to(self.get_device())\n\n                with torch.no_grad():\n                    logits_per_image_obj, _ = self.clip_model(obj_input, text_tokens_obj)\n                    obj_probs = logits_per_image_obj.softmax(dim=-1).cpu().numpy()[0]\n\n                # Use the probability of the query matching the object\n                obj_score = obj_probs[0]\n\n                scored_detection = {\n                    'label': detection['label'],\n                    'confidence': detection['confidence'],\n                    'bbox': detection['bbox'],\n                    'clip_score': float(obj_score),\n                    'full_image_score': float(query_match_prob)\n                }\n                scored_detections.append(scored_detection)\n\n            # Sort detections by CLIP score\n            scored_detections.sort(key=lambda x: x['clip_score'], reverse=True)\n\n            # Publish results\n            self.publish_vision_language_results(scored_detections, query)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error processing vision-language query: {str(e)}\")\n\n    def publish_vision_language_results(self, detections: List[Dict], query: str):\n        \"\"\"Publish vision-language results\"\"\"\n        # Create Detection2DArray message\n        detection_array = Detection2DArray()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = \"camera_frame\"\n\n        for det in detections:\n            detection_msg = Detection2D()\n            detection_msg.header.stamp = detection_array.header.stamp\n            detection_msg.header.frame_id = detection_array.header.frame_id\n\n            # Set bounding box (convert from [x, y, w, h] to center + size)\n            x, y, w, h = det['bbox']\n            detection_msg.bbox.center.x = x + w / 2\n            detection_msg.bbox.center.y = y + h / 2\n            detection_msg.bbox.size_x = w\n            detection_msg.bbox.size_y = h\n\n            # Set results (object hypothesis with score)\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = det['label']\n            hypothesis.score = det['clip_score']  # Use CLIP score as confidence\n\n            detection_msg.results.append(hypothesis)\n            detection_array.detections.append(detection_msg)\n\n        self.detection_pub.publish(detection_array)\n\n        # Also publish detailed results as JSON string\n        result_msg = String()\n        result_data = {\n            'query': query,\n            'detections': detections,\n            'image_match_score': detections[0]['full_image_score'] if detections else 0.0\n        }\n        result_msg.data = json.dumps(result_data, indent=2)\n        self.result_pub.publish(result_msg)\n\n        # Log the results\n        if detections:\n            top_detection = detections[0]\n            self.get_logger().info(\n                f\"Top match: {top_detection['label']} with CLIP score: {top_detection['clip_score']:.3f}\"\n            )\n        else:\n            self.get_logger().info(\"No objects detected matching the query\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionLanguagePerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\"Shutting down Vision-Language Perception Node\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-create-the-object-identification-node",children:"Step 2: Create the Object Identification Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a node that specializes in identifying objects based on natural language descriptions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nimport clip\nfrom PIL import Image as PILImage\nfrom typing import List, Dict, Tuple\nimport json\nimport re\n\nclass ObjectIdentificationNode(Node):\n    def __init__(self):\n        super().__init__(\'object_identification_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Load CLIP model\n        self.get_logger().info("Loading CLIP model for object identification...")\n        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.get_device())\n        self.clip_model.eval()\n\n        # Create subscribers and publishers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.identification_query_sub = self.create_subscription(\n            String,\n            \'object_identification_query\',\n            self.identification_query_callback,\n            10\n        )\n\n        self.identification_result_pub = self.create_publisher(\n            String,\n            \'object_identification_result\',\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            \'identified_objects\',\n            10\n        )\n\n        # Store the latest image\n        self.latest_image = None\n        self.latest_query = None\n\n        # Object detection model\n        self.object_detector = self._initialize_object_detector()\n\n        self.get_logger().info("Object Identification Node initialized")\n\n    def get_device(self):\n        """Get the appropriate device (GPU if available)"""\n        if torch.cuda.is_available():\n            device = "cuda"\n        else:\n            device = "cpu"\n        return device\n\n    def _initialize_object_detector(self):\n        """Initialize object detection model"""\n        # For this example, we\'ll use a mock detector\n        # In a real implementation, you\'d use YOLO, Detectron2, or similar\n        class MockDetector:\n            def detect(self, image):\n                height, width = image.shape[:2]\n                # Return mock detections: [label, confidence, [x, y, w, h]]\n                return [\n                    [\'red cup\', 0.85, [width//4, height//4, width//6, height//6]],\n                    [\'blue book\', 0.78, [width//2, height//3, width//5, height//5]],\n                    [\'phone\', 0.92, [width//3, height//2, width//8, width//8]],\n                    [\'pen\', 0.65, [width//2, height//2, width//10, height//3]]\n                ]\n\n        return MockDetector()\n\n    def image_callback(self, msg: Image):\n        """Handle incoming camera images"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            self.latest_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f"Error converting image: {str(e)}")\n\n    def identification_query_callback(self, msg: String):\n        """Handle object identification queries"""\n        query = msg.data\n        self.latest_query = query\n        self.get_logger().info(f"Received identification query: {query}")\n\n        if self.latest_image is not None:\n            self.identify_objects_by_description(self.latest_image, query)\n\n    def identify_objects_by_description(self, image: np.ndarray, description: str):\n        """Identify objects based on a natural language description"""\n        try:\n            # First, detect all objects in the image\n            all_detections = self.object_detector.detect(image)\n\n            # Prepare CLIP text inputs based on the description\n            # Include the description and some negative examples\n            texts = [description, f"not {description}", "background", "unrelated object"]\n            text_tokens = clip.tokenize(texts).to(self.get_device())\n\n            # Preprocess the full image\n            pil_image = PILImage.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n            image_input = self.clip_preprocess(pil_image).unsqueeze(0).to(self.get_device())\n\n            # Get overall image match score\n            with torch.no_grad():\n                logits_per_image, _ = self.clip_model(image_input, text_tokens)\n                overall_probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n                overall_match_score = overall_probs[0]\n\n            # Now evaluate each detected object against the description\n            matched_objects = []\n            for label, conf, bbox in all_detections:\n                # Extract the object region from the image\n                x, y, w, h = bbox\n                obj_region = image[y:y+h, x:x+w]\n\n                # Convert to PIL and preprocess for CLIP\n                obj_pil = PILImage.fromarray(cv2.cvtColor(obj_region, cv2.COLOR_BGR2RGB))\n                obj_input = self.clip_preprocess(obj_pil).unsqueeze(0).to(self.get_device())\n\n                # Get CLIP scores for this specific object\n                with torch.no_grad():\n                    logits_per_image_obj, _ = self.clip_model(obj_input, text_tokens)\n                    obj_probs = logits_per_image_obj.softmax(dim=-1).cpu().numpy()[0]\n\n                # Calculate the match score for this object\n                obj_match_score = obj_probs[0]\n\n                # Combine the detection confidence with CLIP score\n                combined_score = (conf + obj_match_score) / 2.0\n\n                matched_objects.append({\n                    \'label\': label,\n                    \'bbox\': bbox,\n                    \'detection_confidence\': conf,\n                    \'clip_score\': float(obj_match_score),\n                    \'combined_score\': combined_score,\n                    \'description\': description\n                })\n\n            # Sort by combined score\n            matched_objects.sort(key=lambda x: x[\'combined_score\'], reverse=True)\n\n            # Filter results based on confidence threshold\n            confident_matches = [obj for obj in matched_objects if obj[\'combined_score\'] > 0.3]\n\n            # Publish results\n            self.publish_identification_results(confident_matches, description, overall_match_score)\n\n        except Exception as e:\n            self.get_logger().error(f"Error in object identification: {str(e)}")\n\n    def publish_identification_results(self, matches: List[Dict], description: str, overall_score: float):\n        """Publish object identification results"""\n        # Create detailed result message\n        result_msg = String()\n        result_data = {\n            \'query\': description,\n            \'overall_image_match_score\': float(overall_score),\n            \'matched_objects\': matches,\n            \'total_matches\': len(matches)\n        }\n        result_msg.data = json.dumps(result_data, indent=2)\n        self.identification_result_pub.publish(result_msg)\n\n        # Also publish as detections for visualization\n        detection_array = Detection2DArray()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = "camera_frame"\n\n        for match in matches:\n            from vision_msgs.msg import Detection2D, ObjectHypothesisWithPose\n\n            detection_msg = Detection2D()\n            detection_msg.header.stamp = detection_array.header.stamp\n            detection_msg.header.frame_id = detection_array.header.frame_id\n\n            # Set bounding box\n            x, y, w, h = match[\'bbox\']\n            detection_msg.bbox.center.x = x + w / 2\n            detection_msg.bbox.center.y = y + h / 2\n            detection_msg.bbox.size_x = w\n            detection_msg.bbox.size_y = h\n\n            # Set result\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = match[\'label\']\n            hypothesis.score = match[\'combined_score\']\n\n            detection_msg.results.append(hypothesis)\n            detection_array.detections.append(detection_msg)\n\n        self.detection_pub.publish(detection_array)\n\n        # Log results\n        self.get_logger().info(f"Found {len(matches)} objects matching \'{description}\'")\n        for i, match in enumerate(matches[:3]):  # Log top 3 matches\n            self.get_logger().info(\n                f"  {i+1}. {match[\'label\']} - Score: {match[\'combined_score\']:.3f}"\n            )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectIdentificationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Shutting down Object Identification Node")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-create-the-llm-integration-node",children:"Step 3: Create the LLM Integration Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a node that integrates visual perception with LLM decision making:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nimport clip\nfrom PIL import Image as PILImage\nfrom typing import List, Dict, Any\nimport json\nimport openai\n\nclass LLMVisionIntegrationNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_vision_integration_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Load CLIP model\n        self.get_logger().info("Loading CLIP model for LLM integration...")\n        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.get_device())\n        self.clip_model.eval()\n\n        # Create subscribers and publishers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.vision_query_sub = self.create_subscription(\n            String,\n            \'llm_vision_query\',\n            self.vision_query_callback,\n            10\n        )\n\n        self.llm_decision_pub = self.create_publisher(\n            String,\n            \'llm_decision\',\n            10\n        )\n\n        self.scene_description_pub = self.create_publisher(\n            String,\n            \'scene_description\',\n            10\n        )\n\n        # Store latest data\n        self.latest_image = None\n        self.latest_query = None\n\n        # Object detection model\n        self.object_detector = self._initialize_object_detector()\n\n        self.get_logger().info("LLM Vision Integration Node initialized")\n\n    def get_device(self):\n        """Get the appropriate device (GPU if available)"""\n        if torch.cuda.is_available():\n            device = "cuda"\n        else:\n            device = "cpu"\n        return device\n\n    def _initialize_object_detector(self):\n        """Initialize object detection model"""\n        class MockDetector:\n            def detect(self, image):\n                height, width = image.shape[:2]\n                return [\n                    [\'red cup\', 0.85, [width//4, height//4, width//6, height//6]],\n                    [\'blue book\', 0.78, [width//2, height//3, width//5, height//5]],\n                    [\'phone\', 0.92, [width//3, height//2, width//8, width//8]]\n                ]\n\n        return MockDetector()\n\n    def image_callback(self, msg: Image):\n        """Handle incoming camera images"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            self.latest_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f"Error converting image: {str(e)}")\n\n    def vision_query_callback(self, msg: String):\n        """Handle LLM vision queries"""\n        query = msg.data\n        self.latest_query = query\n        self.get_logger().info(f"Received LLM vision query: {query}")\n\n        if self.latest_image is not None:\n            self.process_llm_vision_query(self.latest_image, query)\n\n    def process_llm_vision_query(self, image: np.ndarray, query: str):\n        """Process a query using both visual perception and LLM"""\n        try:\n            # First, analyze the scene visually\n            scene_analysis = self.analyze_scene(image)\n\n            # Create a prompt for the LLM that includes visual information\n            llm_prompt = self.create_llm_prompt(query, scene_analysis)\n\n            # Get LLM response\n            llm_response = self.get_llm_response(llm_prompt)\n\n            # Publish the LLM decision\n            decision_msg = String()\n            decision_msg.data = json.dumps({\n                \'query\': query,\n                \'scene_analysis\': scene_analysis,\n                \'llm_response\': llm_response,\n                \'timestamp\': self.get_clock().now().nanoseconds\n            })\n            self.llm_decision_pub.publish(decision_msg)\n\n            # Publish scene description\n            scene_desc_msg = String()\n            scene_desc_msg.data = json.dumps(scene_analysis)\n            self.scene_description_pub.publish(scene_desc_msg)\n\n            self.get_logger().info(f"LLM decision published for query: {query}")\n\n        except Exception as e:\n            self.get_logger().error(f"Error in LLM vision processing: {str(e)}")\n\n    def analyze_scene(self, image: np.ndarray) -> Dict[str, Any]:\n        """Analyze the scene using visual perception"""\n        # Detect objects\n        detections = self.object_detector.detect(image)\n\n        # Use CLIP to get more detailed scene understanding\n        pil_image = PILImage.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image_input = self.clip_preprocess(pil_image).unsqueeze(0).to(self.get_device())\n\n        # Define common scene categories for CLIP to classify\n        scene_categories = [\n            "indoor scene", "outdoor scene", "kitchen", "living room", "office", "bedroom",\n            "cluttered", "organized", "bright", "dim", "empty", "occupied"\n        ]\n\n        text_tokens = clip.tokenize(scene_categories).to(self.get_device())\n\n        with torch.no_grad():\n            logits_per_image, _ = self.clip_model(image_input, text_tokens)\n            scene_probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n\n        # Get top scene categories\n        top_scene_indices = np.argsort(scene_probs)[-3:][::-1]  # Top 3\n        top_scenes = [(scene_categories[i], float(scene_probs[i])) for i in top_scene_indices]\n\n        # Format detections\n        formatted_detections = []\n        for label, conf, bbox in detections:\n            formatted_detections.append({\n                \'label\': label,\n                \'confidence\': float(conf),\n                \'bbox\': [int(x) for x in bbox]  # Convert to int for JSON serialization\n            })\n\n        return {\n            \'objects\': formatted_detections,\n            \'scene_categories\': top_scenes,\n            \'image_shape\': image.shape,\n            \'object_count\': len(detections)\n        }\n\n    def create_llm_prompt(self, query: str, scene_analysis: Dict[str, Any]) -> str:\n        """Create a prompt for the LLM with visual context"""\n        prompt = f"""\n        You are a robot assistant with visual perception capabilities. You can see the following scene:\n\n        Scene Analysis:\n        - Objects detected: {scene_analysis[\'objects\']}\n        - Scene categories: {[cat[0] for cat in scene_analysis[\'scene_categories\']]}\n        - Number of objects: {scene_analysis[\'object_count\']}\n\n        The user has asked: "{query}"\n\n        Please provide a helpful response based on what you can see in the scene. If the query is about objects that are visible, provide specific information about their location and appearance. If the query cannot be answered with the current scene, explain why and suggest what might be needed.\n\n        Response:\n        """\n\n        return prompt\n\n    def get_llm_response(self, prompt: str) -> str:\n        """Get response from LLM"""\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[\n                    {"role": "system", "content": "You are a helpful robot assistant with visual perception capabilities. Respond based on what you can see in the scene."},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.3,\n                max_tokens=300\n            )\n\n            return response.choices[0].message.content.strip()\n\n        except Exception as e:\n            self.get_logger().error(f"Error calling LLM: {str(e)}")\n            return f"Error getting LLM response: {str(e)}"\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMVisionIntegrationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Shutting down LLM Vision Integration Node")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-create-the-performance-monitor-node",children:"Step 4: Create the Performance Monitor Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a node that monitors and evaluates the performance of the vision-language system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nimport time\nfrom collections import deque\nfrom typing import Dict, List\nimport json\n\nclass PerformanceMonitorNode(Node):\n    def __init__(self):\n        super().__init__(\'performance_monitor_node\')\n\n        # Create subscribers\n        self.vision_result_sub = self.create_subscription(\n            String,\n            \'vision_language_result\',\n            self.vision_result_callback,\n            10\n        )\n\n        self.identification_result_sub = self.create_subscription(\n            String,\n            \'object_identification_result\',\n            self.identification_result_callback,\n            10\n        )\n\n        self.llm_decision_sub = self.create_subscription(\n            String,\n            \'llm_decision\',\n            self.llm_decision_callback,\n            10\n        )\n\n        self.image_sub = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Performance tracking\n        self.processing_times = deque(maxlen=100)  # Track last 100 processing times\n        self.query_count = 0\n        self.last_image_time = None\n        self.last_query_time = None\n\n        # Publishers for performance metrics\n        self.performance_pub = self.create_publisher(\n            String,\n            \'performance_metrics\',\n            10\n        )\n\n        # Timer for periodic performance reporting\n        self.performance_timer = self.create_timer(5.0, self.report_performance)\n\n        self.get_logger().info("Performance Monitor Node initialized")\n\n    def image_callback(self, msg: Image):\n        """Track image reception times"""\n        current_time = time.time()\n        if self.last_image_time:\n            # Calculate frame rate\n            frame_interval = current_time - self.last_image_time\n            fps = 1.0 / frame_interval if frame_interval > 0 else 0\n            self.get_logger().debug(f"Image FPS: {fps:.2f}")\n\n        self.last_image_time = current_time\n\n    def vision_result_callback(self, msg: String):\n        """Track vision-language processing times"""\n        try:\n            result_data = json.loads(msg.data)\n            if \'processing_time\' in result_data:\n                self.processing_times.append(result_data[\'processing_time\'])\n        except json.JSONDecodeError:\n            pass\n\n        self.query_count += 1\n        self.last_query_time = time.time()\n\n    def identification_result_callback(self, msg: String):\n        """Track object identification processing"""\n        self.query_count += 1\n\n    def llm_decision_callback(self, msg: String):\n        """Track LLM decision processing"""\n        self.query_count += 1\n\n    def report_performance(self):\n        """Report performance metrics"""\n        if not self.processing_times:\n            avg_processing_time = 0.0\n        else:\n            avg_processing_time = sum(self.processing_times) / len(self.processing_times)\n\n        metrics = {\n            \'average_processing_time\': avg_processing_time,\n            \'processing_time_samples\': len(self.processing_times),\n            \'query_count\': self.query_count,\n            \'timestamp\': time.time()\n        }\n\n        # Publish metrics\n        metrics_msg = String()\n        metrics_msg.data = json.dumps(metrics, indent=2)\n        self.performance_pub.publish(metrics_msg)\n\n        # Log metrics\n        self.get_logger().info(\n            f"Performance Metrics - Avg Processing Time: {avg_processing_time:.3f}s, "\n            f"Samples: {len(self.processing_times)}, Queries: {self.query_count}"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerformanceMonitorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Shutting down Performance Monitor Node")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-create-launch-file",children:"Step 5: Create Launch File"}),"\n",(0,s.jsx)(n.p,{children:"Create a launch file to start all vision-language perception nodes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Vision-Language Perception node\n        Node(\n            package='vision_language_perception',\n            executable='vision_language_perception_node',\n            name='vision_language_perception_node',\n            output='screen'\n        ),\n\n        # Object Identification node\n        Node(\n            package='vision_language_perception',\n            executable='object_identification_node',\n            name='object_identification_node',\n            output='screen'\n        ),\n\n        # LLM Integration node\n        Node(\n            package='vision_language_perception',\n            executable='llm_vision_integration_node',\n            name='llm_vision_integration_node',\n            output='screen'\n        ),\n\n        # Performance Monitor node\n        Node(\n            package='vision_language_perception',\n            executable='performance_monitor_node',\n            name='performance_monitor_node',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"testing-and-evaluation",children:"Testing and Evaluation"}),"\n",(0,s.jsx)(n.h3,{id:"1-basic-functionality-test",children:"1. Basic Functionality Test"}),"\n",(0,s.jsx)(n.p,{children:"Test the vision-language perception system with simple queries:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Find the red cup"'}),"\n",(0,s.jsx)(n.li,{children:'"Show me all the books"'}),"\n",(0,s.jsx)(n.li,{children:'"What objects are on the table?"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-complex-query-test",children:"2. Complex Query Test"}),"\n",(0,s.jsx)(n.p,{children:"Test with more complex natural language queries:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Find the largest object in the scene"'}),"\n",(0,s.jsx)(n.li,{children:'"Which object is closest to the center of the image?"'}),"\n",(0,s.jsx)(n.li,{children:'"Identify all electronic devices"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-performance-evaluation",children:"3. Performance Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Evaluate the system's performance metrics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Recognition accuracy for different object types"}),"\n",(0,s.jsx)(n.li,{children:"Response time for various query complexities"}),"\n",(0,s.jsx)(n.li,{children:"Memory usage during processing"}),"\n",(0,s.jsx)(n.li,{children:"Frame rate with continuous processing"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-accuracy-assessment",children:"4. Accuracy Assessment"}),"\n",(0,s.jsx)(n.p,{children:"Test under different conditions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Varying lighting conditions"}),"\n",(0,s.jsx)(n.li,{children:"Different object orientations"}),"\n",(0,s.jsx)(n.li,{children:"Partial occlusions"}),"\n",(0,s.jsx)(n.li,{children:"Multiple similar objects"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"optional-extensions",children:"Optional Extensions"}),"\n",(0,s.jsx)(n.h3,{id:"1-real-time-processing-optimization",children:"1. Real-time Processing Optimization"}),"\n",(0,s.jsx)(n.p,{children:"Implement optimizations for real-time performance:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Model quantization"}),"\n",(0,s.jsx)(n.li,{children:"Efficient preprocessing pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Multi-threading for parallel processing"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-advanced-scene-understanding",children:"2. Advanced Scene Understanding"}),"\n",(0,s.jsx)(n.p,{children:"Enhance scene understanding capabilities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"3D object localization"}),"\n",(0,s.jsx)(n.li,{children:"Spatial relationship understanding"}),"\n",(0,s.jsx)(n.li,{children:"Temporal consistency tracking"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-multimodal-fusion",children:"3. Multimodal Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Add additional sensory modalities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integration with depth sensors"}),"\n",(0,s.jsx)(n.li,{children:"Audio-visual fusion"}),"\n",(0,s.jsx)(n.li,{children:"Tactile feedback integration"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"How does the CLIP-based vision-language model compare to traditional object detection methods for specific queries?"}),"\n",(0,s.jsx)(n.li,{children:"What are the main computational bottlenecks in vision-language perception systems?"}),"\n",(0,s.jsx)(n.li,{children:"How could you improve the system's ability to handle ambiguous or complex natural language queries?"}),"\n",(0,s.jsx)(n.li,{children:"What are the trade-offs between accuracy and speed in real-time vision-language systems?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"what-you-learned",children:"What You Learned"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you implemented a comprehensive vision-language perception system that combines computer vision with natural language understanding. You learned how to integrate CLIP models with ROS 2, identify objects based on natural language descriptions, incorporate LLM decision-making with visual perception, and evaluate system performance. You also explored the challenges and optimization strategies for real-time vision-language systems in robotics applications."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);