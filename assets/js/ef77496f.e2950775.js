"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[853],{8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var s=t(6540);const a={},o=s.createContext(a);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(o.Provider,{value:n},e.children)}},8669(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid","title":"Lab 4: Capstone - The Autonomous Humanoid","description":"Overview","source":"@site/docs/module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid.md","sourceDirName":"module-4-vision-language-action/labs","slug":"/module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Lab 4: Capstone - The Autonomous Humanoid","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Lab 3: Vision-Language Perception Integration","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-3-vision-language-perception-integration"}}');var a=t(4848),o=t(8453);const i={title:"Lab 4: Capstone - The Autonomous Humanoid",sidebar_position:4},r="Lab 4: Capstone - The Autonomous Humanoid",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Lab Setup",id:"lab-setup",level:2},{value:"1. Install Additional Dependencies",id:"1-install-additional-dependencies",level:3},{value:"2. Create Capstone Package",id:"2-create-capstone-package",level:3},{value:"3. Set Up Simulation Environment (if using simulation)",id:"3-set-up-simulation-environment-if-using-simulation",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Create the Humanoid Orchestrator Node",id:"step-1-create-the-humanoid-orchestrator-node",level:3},{value:"Step 2: Create the Humanoid Navigation Node",id:"step-2-create-the-humanoid-navigation-node",level:3},{value:"Step 3: Create the Humanoid Manipulation Node",id:"step-3-create-the-humanoid-manipulation-node",level:3},{value:"Step 4: Create the Task Manager Node",id:"step-4-create-the-task-manager-node",level:3},{value:"Step 5: Create the System Monitor Node",id:"step-5-create-the-system-monitor-node",level:3},{value:"Step 6: Create Launch File",id:"step-6-create-launch-file",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"1. Basic Functionality Test",id:"1-basic-functionality-test",level:3},{value:"2. Complex Multi-Step Task Test",id:"2-complex-multi-step-task-test",level:3},{value:"3. System Integration Test",id:"3-system-integration-test",level:3},{value:"4. Performance Evaluation",id:"4-performance-evaluation",level:3},{value:"Optional Extensions",id:"optional-extensions",level:2},{value:"1. Advanced Humanoid Behaviors",id:"1-advanced-humanoid-behaviors",level:3},{value:"2. Enhanced Perception",id:"2-enhanced-perception",level:3},{value:"3. Improved Planning",id:"3-improved-planning",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"What You Learned",id:"what-you-learned",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lab-4-capstone---the-autonomous-humanoid",children:"Lab 4: Capstone - The Autonomous Humanoid"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This capstone lab integrates all components learned in Module 4 to create an autonomous humanoid robot system. The robot will receive voice commands, plan a path using cognitive planning, navigate obstacles, identify objects using vision-language perception, and manipulate them. This comprehensive project demonstrates the complete Vision-Language-Action (VLA) pipeline in a realistic humanoid robotics scenario."}),"\n",(0,a.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Design an end-to-end humanoid robot system"}),"\n",(0,a.jsx)(n.li,{children:"Integrate voice commands, cognitive planning, and perception"}),"\n",(0,a.jsx)(n.li,{children:"Implement complete task execution pipeline"}),"\n",(0,a.jsx)(n.li,{children:"Test system performance with complex multi-step commands"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Completed all previous labs in Module 4"}),"\n",(0,a.jsx)(n.li,{children:"Working knowledge of ROS 2, Nav2, and MoveIt2"}),"\n",(0,a.jsx)(n.li,{children:"Understanding of speech recognition, LLM integration, and computer vision"}),"\n",(0,a.jsx)(n.li,{children:"Access to simulation environment (Gazebo/Isaac Sim) or physical robot"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,a.jsx)(n.h3,{id:"1-install-additional-dependencies",children:"1. Install Additional Dependencies"}),"\n",(0,a.jsx)(n.p,{children:"Install packages required for the capstone project:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Navigation and manipulation dependencies\nsudo apt update\nsudo apt install ros-humble-navigation2 ros-humble-nav2-bringup\nsudo apt install ros-humble-moveit ros-humble-moveit-ros ros-humble-moveit-ros-planners\nsudo apt install ros-humble-moveit-ros-perception\n\n# Additional Python packages\npip install transforms3d\npip install pyquaternion\npip install control\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-create-capstone-package",children:"2. Create Capstone Package"}),"\n",(0,a.jsx)(n.p,{children:"Create a new ROS 2 package for the capstone project:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/voice_command_ws/src\nros2 pkg create --build-type ament_python humanoid_capstone\ncd humanoid_capstone\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-set-up-simulation-environment-if-using-simulation",children:"3. Set Up Simulation Environment (if using simulation)"}),"\n",(0,a.jsx)(n.p,{children:"For simulation, ensure you have Gazebo or Isaac Sim configured with a humanoid robot model."}),"\n",(0,a.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-create-the-humanoid-orchestrator-node",children:"Step 1: Create the Humanoid Orchestrator Node"}),"\n",(0,a.jsx)(n.p,{children:"Create the main orchestrator that coordinates all VLA components:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom builtin_interfaces.msg import Duration\nimport threading\nimport time\nimport json\nfrom enum import Enum\nfrom typing import Dict, Any, Optional\n\nclass RobotState(Enum):\n    IDLE = "idle"\n    LISTENING = "listening"\n    PROCESSING = "processing"\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    ERROR = "error"\n    COMPLETED = "completed"\n\nclass HumanoidOrchestratorNode(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_orchestrator_node\')\n\n        # Robot state management\n        self.current_state = RobotState.IDLE\n        self.current_task = None\n        self.robot_pose = None\n        self.navigation_active = False\n        self.manipulation_active = False\n\n        # Create subscribers\n        self.voice_command_sub = self.create_subscription(\n            String,\n            \'vl_command\',\n            self.voice_command_callback,\n            10\n        )\n\n        self.planning_result_sub = self.create_subscription(\n            String,\n            \'generated_plan\',\n            self.planning_result_callback,\n            10\n        )\n\n        self.execution_status_sub = self.create_subscription(\n            String,\n            \'executor_status\',\n            self.execution_status_callback,\n            10\n        )\n\n        self.odometry_sub = self.create_subscription(\n            Odometry,\n            \'odom\',\n            self.odometry_callback,\n            10\n        )\n\n        # Create publishers\n        self.state_pub = self.create_publisher(\n            String,\n            \'robot_state\',\n            10\n        )\n\n        self.plan_request_pub = self.create_publisher(\n            String,\n            \'natural_language_command\',\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            \'capstone_status\',\n            10\n        )\n\n        self.task_complete_pub = self.create_publisher(\n            Bool,\n            \'task_complete\',\n            10\n        )\n\n        # Task queue for handling multiple commands\n        self.task_queue = []\n        self.task_queue_lock = threading.Lock()\n\n        # Timer for state monitoring\n        self.state_timer = self.create_timer(0.1, self.state_monitor)\n\n        self.get_logger().info("Humanoid Orchestrator Node initialized")\n\n    def voice_command_callback(self, msg: String):\n        """Handle incoming voice commands"""\n        command = msg.data\n        self.get_logger().info(f"Received voice command: {command}")\n\n        # Add command to task queue\n        with self.task_queue_lock:\n            self.task_queue.append({\n                \'command\': command,\n                \'timestamp\': time.time(),\n                \'status\': \'pending\'\n            })\n\n        # Update state and trigger processing\n        self.current_state = RobotState.PROCESSING\n        self.publish_state()\n\n        # Request plan generation\n        plan_request = String()\n        plan_request.data = command\n        self.plan_request_pub.publish(plan_request)\n\n    def planning_result_callback(self, msg: String):\n        """Handle planning results"""\n        try:\n            plan_data = json.loads(msg.data)\n            self.get_logger().info(f"Received plan with {len(plan_data.get(\'steps\', []))} steps")\n\n            # Transition to execution state\n            self.current_state = RobotState.EXECUTING\n            self.publish_state()\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f"Starting execution of plan with {len(plan_data.get(\'steps\', []))} steps"\n            self.status_pub.publish(status_msg)\n\n        except json.JSONDecodeError:\n            self.get_logger().error("Invalid JSON in planning result")\n            self.current_state = RobotState.ERROR\n            self.publish_state()\n\n    def execution_status_callback(self, msg: String):\n        """Handle execution status updates"""\n        status_text = msg.data\n        self.get_logger().info(f"Execution status: {status_text}")\n\n        # Check if execution is complete\n        if "Plan execution completed" in status_text:\n            self.current_state = RobotState.COMPLETED\n            self.publish_state()\n\n            # Publish task completion\n            complete_msg = Bool()\n            complete_msg.data = True\n            self.task_complete_pub.publish(complete_msg)\n\n            # Return to idle after a short delay\n            self.create_timer(2.0, self.return_to_idle)\n\n    def odometry_callback(self, msg: Odometry):\n        """Update robot pose from odometry"""\n        self.robot_pose = msg.pose.pose\n\n    def state_monitor(self):\n        """Monitor and update robot state"""\n        # Publish current state\n        state_msg = String()\n        state_msg.data = self.current_state.value\n        self.state_pub.publish(state_msg)\n\n        # Process task queue if idle and tasks available\n        if self.current_state == RobotState.IDLE:\n            with self.task_queue_lock:\n                if self.task_queue:\n                    task = self.task_queue.pop(0)\n                    self.get_logger().info(f"Processing queued task: {task[\'command\']}")\n                    self.current_state = RobotState.PROCESSING\n                    self.publish_state()\n\n                    # Request plan for the queued task\n                    plan_request = String()\n                    plan_request.data = task[\'command\']\n                    self.plan_request_pub.publish(plan_request)\n\n    def return_to_idle(self):\n        """Return to idle state after task completion"""\n        self.current_state = RobotState.IDLE\n        self.publish_state()\n\n        # Publish completion status\n        status_msg = String()\n        status_msg.data = "Task completed, returned to idle state"\n        self.status_pub.publish(status_msg)\n\n        self.get_logger().info("Robot returned to idle state")\n\n    def publish_state(self):\n        """Publish current robot state"""\n        state_msg = String()\n        state_msg.data = self.current_state.value\n        self.state_pub.publish(state_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidOrchestratorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Shutting down Humanoid Orchestrator Node")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-create-the-humanoid-navigation-node",children:"Step 2: Create the Humanoid Navigation Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a specialized navigation node for humanoid-specific navigation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import PoseStamped, Point, Quaternion\nfrom nav2_msgs.action import NavigateToPose\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom std_msgs.msg import String\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport tf2_geometry_msgs\nimport numpy as np\nimport math\nfrom typing import List, Tuple\n\nclass HumanoidNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_navigation_node\')\n\n        # Create action client for navigation\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Create TF buffer and listener for coordinate transformations\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Create subscribers\n        self.navigation_goal_sub = self.create_subscription(\n            PoseStamped,\n            \'navigation_goal\',\n            self.navigation_goal_callback,\n            10\n        )\n\n        self.laser_scan_sub = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.laser_scan_callback,\n            10\n        )\n\n        # Create publishers\n        self.navigation_status_pub = self.create_publisher(\n            String,\n            \'navigation_status\',\n            10\n        )\n\n        # Navigation parameters for humanoid\n        self.humanoid_navigation_params = {\n            \'max_linear_speed\': 0.5,      # m/s\n            \'max_angular_speed\': 0.5,     # rad/s\n            \'min_distance_to_obstacle\': 0.5,  # m\n            \'inflation_radius\': 0.8,      # m\n            \'footprint_padding\': 0.3      # m\n        }\n\n        # Store current navigation state\n        self.current_goal = None\n        self.navigation_active = False\n        self.safe_to_navigate = True\n\n        self.get_logger().info("Humanoid Navigation Node initialized")\n\n    def navigation_goal_callback(self, msg: PoseStamped):\n        """Handle navigation goals for humanoid"""\n        self.get_logger().info(f"Received navigation goal: ({msg.pose.position.x}, {msg.pose.position.y})")\n\n        # Check if navigation server is available\n        if not self.nav_client.wait_for_server(timeout_sec=5.0):\n            self.get_logger().error("Navigation server not available")\n            status_msg = String()\n            status_msg.data = "Navigation server not available"\n            self.navigation_status_pub.publish(status_msg)\n            return\n\n        # Create navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = msg  # Use the received pose as the goal\n\n        # Add humanoid-specific navigation parameters\n        goal_msg.behavior_tree = "navigate_w_replanning_and_recovery"  # Use appropriate BT\n\n        # Send navigation goal\n        self.current_goal = goal_msg\n        self.navigation_active = True\n\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.navigation_done_callback)\n\n        status_msg = String()\n        status_msg.data = f"Navigating to ({msg.pose.position.x:.2f}, {msg.pose.position.y:.2f})"\n        self.navigation_status_pub.publish(status_msg)\n\n    def navigation_done_callback(self, future):\n        """Handle navigation completion"""\n        try:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                self.get_logger().info("Navigation goal accepted")\n                result_future = goal_handle.get_result_async()\n                result_future.add_done_callback(self.navigation_result_callback)\n            else:\n                self.get_logger().error("Navigation goal rejected")\n                self.navigation_active = False\n                status_msg = String()\n                status_msg.data = "Navigation goal rejected"\n                self.navigation_status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Navigation failed: {str(e)}")\n            self.navigation_active = False\n            status_msg = String()\n            status_msg.data = f"Navigation failed: {str(e)}"\n            self.navigation_status_pub.publish(status_msg)\n\n    def navigation_result_callback(self, future):\n        """Handle navigation result"""\n        try:\n            result = future.result().result\n            self.navigation_active = False\n\n            status_msg = String()\n            if result:\n                status_msg.data = "Navigation completed successfully"\n                self.get_logger().info("Navigation completed successfully")\n            else:\n                status_msg.data = "Navigation failed"\n                self.get_logger().error("Navigation failed")\n\n            self.navigation_status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Navigation result error: {str(e)}")\n            self.navigation_active = False\n            status_msg = String()\n            status_msg.data = f"Navigation result error: {str(e)}"\n            self.navigation_status_pub.publish(status_msg)\n\n    def laser_scan_callback(self, msg: LaserScan):\n        """Process laser scan data for obstacle detection"""\n        # Convert laser scan to obstacle distances\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        valid_ranges = [r for r in msg.ranges if msg.range_min < r < msg.range_max]\n\n        if not valid_ranges:\n            return\n\n        # Check for obstacles in humanoid\'s path\n        min_distance = min(valid_ranges) if valid_ranges else float(\'inf\')\n\n        if min_distance < self.humanoid_navigation_params[\'min_distance_to_obstacle\']:\n            self.safe_to_navigate = False\n            self.get_logger().warn(f"Obstacle detected at {min_distance:.2f}m, stopping navigation")\n\n            # If currently navigating, consider pausing or replanning\n            if self.navigation_active:\n                status_msg = String()\n                status_msg.data = f"Obstacle detected at {min_distance:.2f}m, replanning route"\n                self.navigation_status_pub.publish(status_msg)\n        else:\n            self.safe_to_navigate = True\n\n    def transform_pose(self, pose: PoseStamped, target_frame: str) -> Optional[PoseStamped]:\n        """Transform pose to target frame"""\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                target_frame,\n                pose.header.frame_id,\n                rclpy.time.Time(),\n                timeout=rclpy.duration.Duration(seconds=1.0)\n            )\n            transformed_pose = tf2_geometry_msgs.do_transform_pose(pose, transform)\n            transformed_pose.header.frame_id = target_frame\n            return transformed_pose\n        except TransformException as e:\n            self.get_logger().error(f"Transform failed: {str(e)}")\n            return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidNavigationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("Shutting down Humanoid Navigation Node")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-create-the-humanoid-manipulation-node",children:"Step 3: Create the Humanoid Manipulation Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a node for humanoid-specific manipulation tasks:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom sensor_msgs.msg import JointState\nfrom moveit_msgs.msg import MoveItErrorCodes\nfrom moveit_msgs.srv import GetPositionIK, GetPositionFK\nfrom moveit_msgs.action import MoveGroup\nfrom rclpy.action import ActionClient\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom control_msgs.action import FollowJointTrajectory\nimport numpy as np\nimport math\nfrom typing import List, Dict, Any\n\nclass HumanoidManipulationNode(Node):\n    def __init__(self):\n        super().__init__('humanoid_manipulation_node')\n\n        # Create action client for MoveIt\n        self.move_group_client = ActionClient(self, MoveGroup, 'move_group')\n        self.trajectory_client = ActionClient(self, FollowJointTrajectory, 'joint_trajectory_controller/follow_joint_trajectory')\n\n        # Create subscribers\n        self.manipulation_command_sub = self.create_subscription(\n            String,\n            'manipulation_command',\n            self.manipulation_command_callback,\n            10\n        )\n\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            'joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        # Create publishers\n        self.manipulation_status_pub = self.create_publisher(\n            String,\n            'manipulation_status',\n            10\n        )\n\n        # Service clients\n        self.ik_client = self.create_client(GetPositionIK, 'compute_ik')\n        self.fk_client = self.create_client(GetPositionFK, 'compute_fk')\n\n        # Store current joint states\n        self.current_joint_states = {}\n        self.manipulation_active = False\n\n        # Humanoid-specific parameters\n        self.humanoid_params = {\n            'arm_joints': ['left_shoulder_joint', 'left_elbow_joint', 'left_wrist_joint',\n                          'right_shoulder_joint', 'right_elbow_joint', 'right_wrist_joint'],\n            'gripper_joints': ['left_gripper_joint', 'right_gripper_joint'],\n            'arm_links': ['left_upper_arm', 'left_forearm', 'left_hand',\n                         'right_upper_arm', 'right_forearm', 'right_hand'],\n            'max_reach': 1.0,  # meters\n            'min_grasp_distance': 0.1,  # meters\n            'grasp_tolerance': 0.05  # meters\n        }\n\n        self.get_logger().info(\"Humanoid Manipulation Node initialized\")\n\n    def manipulation_command_callback(self, msg: String):\n        \"\"\"Handle manipulation commands\"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            command_type = command_data.get('command_type')\n\n            self.get_logger().info(f\"Received manipulation command: {command_type}\")\n\n            if command_type == 'grasp_object':\n                self.execute_grasp_command(command_data)\n            elif command_type == 'place_object':\n                self.execute_place_command(command_data)\n            elif command_type == 'move_arm':\n                self.execute_move_arm_command(command_data)\n            elif command_type == 'open_gripper':\n                self.execute_gripper_command(command_data, 'open')\n            elif command_type == 'close_gripper':\n                self.execute_gripper_command(command_data, 'close')\n            else:\n                self.get_logger().error(f\"Unknown manipulation command: {command_type}\")\n\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Invalid JSON in manipulation command\")\n\n    def joint_state_callback(self, msg: JointState):\n        \"\"\"Update current joint states\"\"\"\n        for i, name in enumerate(msg.name):\n            if name in self.humanoid_params['arm_joints'] or name in self.humanoid_params['gripper_joints']:\n                self.current_joint_states[name] = msg.position[i]\n\n    def execute_grasp_command(self, command_data: Dict[str, Any]):\n        \"\"\"Execute grasp object command\"\"\"\n        self.manipulation_active = True\n        object_pose = command_data.get('object_pose')\n        arm = command_data.get('arm', 'right')  # default to right arm\n\n        if not object_pose:\n            self.get_logger().error(\"No object pose provided for grasp command\")\n            self.manipulation_active = False\n            return\n\n        self.get_logger().info(f\"Attempting to grasp object at ({object_pose['x']}, {object_pose['y']}, {object_pose['z']}) with {arm} arm\")\n\n        # Plan approach trajectory\n        approach_pose = self.calculate_approach_pose(object_pose, arm)\n\n        # Move to approach pose\n        if self.move_to_pose(approach_pose, arm):\n            # Move to grasp pose\n            grasp_pose = self.calculate_grasp_pose(object_pose, arm)\n            if self.move_to_pose(grasp_pose, arm):\n                # Close gripper\n                self.close_gripper(arm)\n\n                # Lift object slightly\n                lift_pose = self.calculate_lift_pose(grasp_pose, arm)\n                self.move_to_pose(lift_pose, arm)\n\n                status_msg = String()\n                status_msg.data = f\"Successfully grasped object with {arm} arm\"\n                self.manipulation_status_pub.publish(status_msg)\n            else:\n                self.get_logger().error(\"Failed to reach grasp pose\")\n        else:\n            self.get_logger().error(\"Failed to reach approach pose\")\n\n        self.manipulation_active = False\n\n    def execute_place_command(self, command_data: Dict[str, Any]):\n        \"\"\"Execute place object command\"\"\"\n        self.manipulation_active = True\n        target_pose = command_data.get('target_pose')\n        arm = command_data.get('arm', 'right')\n\n        if not target_pose:\n            self.get_logger().error(\"No target pose provided for place command\")\n            self.manipulation_active = False\n            return\n\n        self.get_logger().info(f\"Attempting to place object at ({target_pose['x']}, {target_pose['y']}, {target_pose['z']}) with {arm} arm\")\n\n        # Plan approach trajectory to placement location\n        approach_pose = self.calculate_approach_pose(target_pose, arm)\n\n        if self.move_to_pose(approach_pose, arm):\n            # Move to placement pose\n            place_pose = self.calculate_place_pose(target_pose, arm)\n            if self.move_to_pose(place_pose, arm):\n                # Open gripper to release object\n                self.open_gripper(arm)\n\n                # Retract arm\n                retract_pose = self.calculate_retract_pose(place_pose, arm)\n                self.move_to_pose(retract_pose, arm)\n\n                status_msg = String()\n                status_msg.data = f\"Successfully placed object with {arm} arm\"\n                self.manipulation_status_pub.publish(status_msg)\n            else:\n                self.get_logger().error(\"Failed to reach place pose\")\n        else:\n            self.get_logger().error(\"Failed to reach approach pose\")\n\n        self.manipulation_active = False\n\n    def calculate_approach_pose(self, object_pose: Dict, arm: str) -> Pose:\n        \"\"\"Calculate approach pose for grasping\"\"\"\n        approach_offset = 0.15  # 15cm from object\n        approach_pose = Pose()\n\n        # Calculate approach direction based on object orientation and preferred direction\n        approach_pose.position.x = object_pose['x'] - approach_offset\n        approach_pose.position.y = object_pose['y']\n        approach_pose.position.z = object_pose['z'] + 0.05  # slightly above object\n\n        # Set orientation to face the object\n        approach_pose.orientation = self.calculate_orientation_to_object(object_pose, arm)\n\n        return approach_pose\n\n    def calculate_grasp_pose(self, object_pose: Dict, arm: str) -> Pose:\n        \"\"\"Calculate final grasp pose\"\"\"\n        grasp_pose = Pose()\n        grasp_pose.position.x = object_pose['x']\n        grasp_pose.position.y = object_pose['y']\n        grasp_pose.position.z = object_pose['z']\n\n        # Use same orientation as approach\n        grasp_pose.orientation = self.calculate_orientation_to_object(object_pose, arm)\n\n        return grasp_pose\n\n    def calculate_place_pose(self, target_pose: Dict, arm: str) -> Pose:\n        \"\"\"Calculate placement pose\"\"\"\n        place_pose = Pose()\n        place_pose.position.x = target_pose['x']\n        place_pose.position.y = target_pose['y']\n        place_pose.position.z = target_pose['z']\n\n        place_pose.orientation = self.calculate_orientation_for_placement(target_pose, arm)\n\n        return place_pose\n\n    def calculate_orientation_to_object(self, object_pose: Dict, arm: str) -> Quaternion:\n        \"\"\"Calculate orientation to face an object\"\"\"\n        # Simple implementation - in practice, this would be more sophisticated\n        # For now, return a default orientation\n        return Quaternion(x=0.0, y=0.0, z=0.0, w=1.0)\n\n    def calculate_orientation_for_placement(self, target_pose: Dict, arm: str) -> Quaternion:\n        \"\"\"Calculate orientation for object placement\"\"\"\n        # Simple implementation - in practice, this would consider the placement surface\n        return Quaternion(x=0.0, y=0.0, z=0.0, w=1.0)\n\n    def calculate_lift_pose(self, grasp_pose: Pose, arm: str) -> Pose:\n        \"\"\"Calculate pose after grasping to lift object\"\"\"\n        lift_pose = Pose()\n        lift_pose.position.x = grasp_pose.position.x\n        lift_pose.position.y = grasp_pose.position.y\n        lift_pose.position.z = grasp_pose.position.z + 0.1  # lift 10cm\n        lift_pose.orientation = grasp_pose.orientation\n        return lift_pose\n\n    def calculate_retract_pose(self, place_pose: Pose, arm: str) -> Pose:\n        \"\"\"Calculate retraction pose after placing\"\"\"\n        retract_pose = Pose()\n        retract_pose.position.x = place_pose.position.x - 0.1  # move back 10cm\n        retract_pose.position.y = place_pose.position.y\n        retract_pose.position.z = place_pose.position.z + 0.05  # lift slightly\n        retract_pose.orientation = place_pose.orientation\n        return retract_pose\n\n    def move_to_pose(self, target_pose: Pose, arm: str) -> bool:\n        \"\"\"Move arm to target pose using MoveIt\"\"\"\n        # In a real implementation, this would use MoveIt's planning and execution\n        # For this example, we'll simulate the movement\n        self.get_logger().info(f\"Moving {arm} arm to target pose\")\n\n        # Simulate movement time\n        import time\n        time.sleep(2.0)\n\n        return True  # Simulate success\n\n    def close_gripper(self, arm: str):\n        \"\"\"Close the gripper\"\"\"\n        self.get_logger().info(f\"Closing {arm} gripper\")\n        # In a real implementation, this would send commands to the gripper controller\n\n    def open_gripper(self, arm: str):\n        \"\"\"Open the gripper\"\"\"\n        self.get_logger().info(f\"Opening {arm} gripper\")\n        # In a real implementation, this would send commands to the gripper controller\n\n    def execute_move_arm_command(self, command_data: Dict[str, Any]):\n        \"\"\"Execute move arm command\"\"\"\n        arm = command_data.get('arm', 'right')\n        target_pose = command_data.get('target_pose')\n\n        if target_pose:\n            success = self.move_to_pose(self.dict_to_pose(target_pose), arm)\n            status_msg = String()\n            status_msg.data = f\"Move arm command {'succeeded' if success else 'failed'}\"\n            self.manipulation_status_pub.publish(status_msg)\n\n    def execute_gripper_command(self, command_data: Dict[str, Any], action: str):\n        \"\"\"Execute gripper command\"\"\"\n        arm = command_data.get('arm', 'right')\n\n        if action == 'close':\n            self.close_gripper(arm)\n        else:\n            self.open_gripper(arm)\n\n        status_msg = String()\n        status_msg.data = f\"Gripper command ({action}) executed for {arm} arm\"\n        self.manipulation_status_pub.publish(status_msg)\n\n    def dict_to_pose(self, pose_dict: Dict) -> Pose:\n        \"\"\"Convert dictionary to Pose message\"\"\"\n        pose = Pose()\n        pose.position.x = pose_dict.get('x', 0.0)\n        pose.position.y = pose_dict.get('y', 0.0)\n        pose.position.z = pose_dict.get('z', 0.0)\n        pose.orientation.x = pose_dict.get('qx', 0.0)\n        pose.orientation.y = pose_dict.get('qy', 0.0)\n        pose.orientation.z = pose_dict.get('qz', 0.0)\n        pose.orientation.w = pose_dict.get('qw', 1.0)\n        return pose\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidManipulationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\"Shutting down Humanoid Manipulation Node\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-4-create-the-task-manager-node",children:"Step 4: Create the Task Manager Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a node to manage complex multi-step tasks:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom cognitive_planning_interfaces.msg import Plan, PlanStep\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import PoseStamped\nfrom builtin_interfaces.msg import Time\nimport json\nimport time\nfrom typing import List, Dict, Any\nfrom enum import Enum\n\nclass TaskState(Enum):\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\nclass TaskManagerNode(Node):\n    def __init__(self):\n        super().__init__('task_manager_node')\n\n        # Create subscribers\n        self.task_request_sub = self.create_subscription(\n            String,\n            'task_request',\n            self.task_request_callback,\n            10\n        )\n\n        self.vision_detections_sub = self.create_subscription(\n            Detection2DArray,\n            'vision_language_detections',\n            self.vision_detections_callback,\n            10\n        )\n\n        self.plan_sub = self.create_subscription(\n            Plan,\n            'generated_plan',\n            self.plan_callback,\n            10\n        )\n\n        self.execution_status_sub = self.create_subscription(\n            String,\n            'executor_status',\n            self.execution_status_callback,\n            10\n        )\n\n        # Create publishers\n        self.task_status_pub = self.create_publisher(\n            String,\n            'task_status',\n            10\n        )\n\n        self.navigation_goal_pub = self.create_publisher(\n            PoseStamped,\n            'navigation_goal',\n            10\n        )\n\n        self.manipulation_command_pub = self.create_publisher(\n            String,\n            'manipulation_command',\n            10\n        )\n\n        self.task_complete_pub = self.create_publisher(\n            Bool,\n            'task_complete',\n            10\n        )\n\n        # Task management\n        self.active_tasks = {}\n        self.object_detections = {}\n        self.current_plan = None\n        self.current_plan_step = 0\n\n        self.get_logger().info(\"Task Manager Node initialized\")\n\n    def task_request_callback(self, msg: String):\n        \"\"\"Handle new task requests\"\"\"\n        try:\n            task_data = json.loads(msg.data)\n            task_id = task_data.get('task_id', str(int(time.time())))\n            task_description = task_data.get('description', '')\n\n            self.get_logger().info(f\"Received task request: {task_description} (ID: {task_id})\")\n\n            # Create new task\n            new_task = {\n                'id': task_id,\n                'description': task_description,\n                'state': TaskState.PENDING,\n                'created_time': self.get_clock().now().to_msg(),\n                'steps_completed': 0,\n                'total_steps': 0,\n                'requirements': task_data.get('requirements', {}),\n                'results': {}\n            }\n\n            self.active_tasks[task_id] = new_task\n            self.publish_task_status(task_id, f\"Task {task_id} created and pending\")\n\n            # Process the task\n            self.process_task(task_id, task_data)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Invalid JSON in task request\")\n\n    def vision_detections_callback(self, msg: Detection2DArray):\n        \"\"\"Handle vision detections\"\"\"\n        detections = []\n        for detection in msg.detections:\n            if detection.results:\n                # Get the best result\n                best_result = max(detection.results, key=lambda r: r.score)\n                detections.append({\n                    'label': best_result.id,\n                    'confidence': best_result.score,\n                    'bbox_center_x': detection.bbox.center.x,\n                    'bbox_center_y': detection.bbox.center.y,\n                    'bbox_size_x': detection.bbox.size_x,\n                    'bbox_size_y': detection.bbox.size_y\n                })\n\n        # Store detections for later use\n        self.object_detections = {\n            'timestamp': self.get_clock().now().to_msg(),\n            'detections': detections\n        }\n\n        self.get_logger().info(f\"Stored {len(detections)} object detections\")\n\n    def plan_callback(self, msg: Plan):\n        \"\"\"Handle received plans\"\"\"\n        self.current_plan = msg\n        self.current_plan_step = 0\n\n        self.get_logger().info(f\"Received plan with {len(msg.steps)} steps\")\n\n    def execution_status_callback(self, msg: String):\n        \"\"\"Handle execution status updates\"\"\"\n        status_text = msg.data\n        self.get_logger().info(f\"Execution status: {status_text}\")\n\n        # Check if current plan step is complete\n        if \"Executing step\" in status_text and \"completed\" in status_text:\n            self.current_plan_step += 1\n\n            if self.current_plan and self.current_plan_step >= len(self.current_plan.steps):\n                # Plan completed\n                self.get_logger().info(\"Task plan completed\")\n\n                # Publish task completion\n                complete_msg = Bool()\n                complete_msg.data = True\n                self.task_complete_pub.publish(complete_msg)\n\n    def process_task(self, task_id: str, task_data: Dict[str, Any]):\n        \"\"\"Process a task by generating and executing a plan\"\"\"\n        # Update task state\n        self.active_tasks[task_id]['state'] = TaskState.IN_PROGRESS\n        self.publish_task_status(task_id, f\"Processing task: {task_data.get('description', '')}\")\n\n        # Request plan generation\n        plan_request = String()\n        plan_request.data = task_data.get('description', '')\n\n        plan_request_publisher = self.create_publisher(String, 'natural_language_command', 10)\n        plan_request_publisher.publish(plan_request)\n\n        self.get_logger().info(f\"Requested plan generation for task {task_id}\")\n\n    def publish_task_status(self, task_id: str, status: str):\n        \"\"\"Publish task status\"\"\"\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'task_id': task_id,\n            'status': status,\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        self.task_status_pub.publish(status_msg)\n\n    def find_object_by_description(self, description: str) -> Dict[str, Any]:\n        \"\"\"Find an object in recent detections by description\"\"\"\n        for detection in self.object_detections.get('detections', []):\n            if description.lower() in detection['label'].lower():\n                return detection\n\n        return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TaskManagerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\"Shutting down Task Manager Node\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-5-create-the-system-monitor-node",children:"Step 5: Create the System Monitor Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a node to monitor the entire system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan\nfrom builtin_interfaces.msg import Time\nimport time\nfrom collections import deque\nimport json\n\nclass SystemMonitorNode(Node):\n    def __init__(self):\n        super().__init__('system_monitor_node')\n\n        # Create subscribers\n        self.state_sub = self.create_subscription(\n            String,\n            'robot_state',\n            self.state_callback,\n            10\n        )\n\n        self.status_sub = self.create_subscription(\n            String,\n            'capstone_status',\n            self.status_callback,\n            10\n        )\n\n        self.task_status_sub = self.create_subscription(\n            String,\n            'task_status',\n            self.task_status_callback,\n            10\n        )\n\n        self.performance_sub = self.create_subscription(\n            String,\n            'performance_metrics',\n            self.performance_callback,\n            10\n        )\n\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.laser_sub = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.laser_callback,\n            10\n        )\n\n        self.task_complete_sub = self.create_subscription(\n            Bool,\n            'task_complete',\n            self.task_complete_callback,\n            10\n        )\n\n        # Create publishers\n        self.system_status_pub = self.create_publisher(\n            String,\n            'system_status',\n            10\n        )\n\n        # System metrics\n        self.system_metrics = {\n            'robot_state': 'idle',\n            'last_task_completion': None,\n            'total_tasks_completed': 0,\n            'average_task_time': 0.0,\n            'task_times': deque(maxlen=100),\n            'last_image_time': None,\n            'last_laser_time': None,\n            'performance_metrics': {}\n        }\n\n        # Timer for system status updates\n        self.status_timer = self.create_timer(2.0, self.publish_system_status)\n\n        self.get_logger().info(\"System Monitor Node initialized\")\n\n    def state_callback(self, msg: String):\n        \"\"\"Update robot state\"\"\"\n        self.system_metrics['robot_state'] = msg.data\n\n    def status_callback(self, msg: String):\n        \"\"\"Handle status updates\"\"\"\n        self.get_logger().info(f\"Status update: {msg.data}\")\n\n    def task_status_callback(self, msg: String):\n        \"\"\"Handle task status updates\"\"\"\n        try:\n            status_data = json.loads(msg.data)\n            self.get_logger().info(f\"Task status: {status_data}\")\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Invalid JSON in task status\")\n\n    def performance_callback(self, msg: String):\n        \"\"\"Handle performance metrics\"\"\"\n        try:\n            perf_data = json.loads(msg.data)\n            self.system_metrics['performance_metrics'] = perf_data\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Invalid JSON in performance metrics\")\n\n    def image_callback(self, msg: Image):\n        \"\"\"Track image reception\"\"\"\n        current_time = time.time()\n        if self.system_metrics['last_image_time']:\n            # Calculate frame rate\n            frame_interval = current_time - self.system_metrics['last_image_time']\n            fps = 1.0 / frame_interval if frame_interval > 0 else 0\n            self.get_logger().debug(f\"Camera FPS: {fps:.2f}\")\n\n        self.system_metrics['last_image_time'] = current_time\n\n    def laser_callback(self, msg: LaserScan):\n        \"\"\"Track laser scan reception\"\"\"\n        self.system_metrics['last_laser_time'] = time.time()\n\n    def task_complete_callback(self, msg: Bool):\n        \"\"\"Handle task completion\"\"\"\n        if msg.data:\n            self.system_metrics['total_tasks_completed'] += 1\n            self.system_metrics['last_task_completion'] = time.time()\n            self.get_logger().info(f\"Task completed. Total completed: {self.system_metrics['total_tasks_completed']}\")\n\n    def publish_system_status(self):\n        \"\"\"Publish overall system status\"\"\"\n        system_status = {\n            'robot_state': self.system_metrics['robot_state'],\n            'total_tasks_completed': self.system_metrics['total_tasks_completed'],\n            'last_task_completion': self.system_metrics['last_task_completion'],\n            'performance': self.system_metrics['performance_metrics'],\n            'system_uptime': self.get_clock().now().nanoseconds,\n            'timestamp': time.time()\n        }\n\n        status_msg = String()\n        status_msg.data = json.dumps(system_status, indent=2)\n        self.system_status_pub.publish(status_msg)\n\n        # Log system summary\n        self.get_logger().info(\n            f\"System Status - State: {self.system_metrics['robot_state']}, \"\n            f\"Tasks completed: {self.system_metrics['total_tasks_completed']}\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SystemMonitorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\"Shutting down System Monitor Node\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-6-create-launch-file",children:"Step 6: Create Launch File"}),"\n",(0,a.jsx)(n.p,{children:"Create a comprehensive launch file for the capstone project:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nimport os\n\ndef generate_launch_description():\n    # Launch configuration\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n\n        # Humanoid Orchestrator Node\n        Node(\n            package='humanoid_capstone',\n            executable='humanoid_orchestrator_node',\n            name='humanoid_orchestrator_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Humanoid Navigation Node\n        Node(\n            package='humanoid_capstone',\n            executable='humanoid_navigation_node',\n            name='humanoid_navigation_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Humanoid Manipulation Node\n        Node(\n            package='humanoid_capstone',\n            executable='humanoid_manipulation_node',\n            name='humanoid_manipulation_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Task Manager Node\n        Node(\n            package='humanoid_capstone',\n            executable='task_manager_node',\n            name='task_manager_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # System Monitor Node\n        Node(\n            package='humanoid_capstone',\n            executable='system_monitor_node',\n            name='system_monitor_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Include other necessary nodes from previous labs\n        # Voice Command System\n        Node(\n            package='voice_command_system',\n            executable='audio_capture_node',\n            name='audio_capture_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        Node(\n            package='voice_command_system',\n            executable='whisper_node',\n            name='whisper_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        Node(\n            package='voice_command_system',\n            executable='voice_command_parser_node',\n            name='voice_command_parser_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Cognitive Planning System\n        Node(\n            package='cognitive_planning',\n            executable='llm_planner_node',\n            name='llm_planner_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        Node(\n            package='cognitive_planning',\n            executable='plan_executor_node',\n            name='plan_executor_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Vision-Language Perception System\n        Node(\n            package='vision_language_perception',\n            executable='vision_language_perception_node',\n            name='vision_language_perception_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        Node(\n            package='vision_language_perception',\n            executable='object_identification_node',\n            name='object_identification_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,a.jsx)(n.h3,{id:"1-basic-functionality-test",children:"1. Basic Functionality Test"}),"\n",(0,a.jsx)(n.p,{children:"Test the integrated system with simple commands:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'"Move to the kitchen"'}),"\n",(0,a.jsx)(n.li,{children:'"Pick up the red cup"'}),"\n",(0,a.jsx)(n.li,{children:'"Go to the table and place the cup there"'}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-complex-multi-step-task-test",children:"2. Complex Multi-Step Task Test"}),"\n",(0,a.jsx)(n.p,{children:"Test with complex multi-step commands:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'"Go to the kitchen, find the red cup, pick it up, then go to the living room and place it on the table"'}),"\n",(0,a.jsx)(n.li,{children:'"Find the keys in the bedroom, then navigate to the office and put them on the desk"'}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-system-integration-test",children:"3. System Integration Test"}),"\n",(0,a.jsx)(n.p,{children:"Test the complete VLA pipeline:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Voice command recognition and parsing"}),"\n",(0,a.jsx)(n.li,{children:"Cognitive planning with LLMs"}),"\n",(0,a.jsx)(n.li,{children:"Vision-language object identification"}),"\n",(0,a.jsx)(n.li,{children:"Navigation and manipulation execution"}),"\n",(0,a.jsx)(n.li,{children:"System monitoring and feedback"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-performance-evaluation",children:"4. Performance Evaluation"}),"\n",(0,a.jsx)(n.p,{children:"Evaluate the complete system:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"End-to-end task completion time"}),"\n",(0,a.jsx)(n.li,{children:"Success rate for different task types"}),"\n",(0,a.jsx)(n.li,{children:"System resource utilization"}),"\n",(0,a.jsx)(n.li,{children:"Robustness to environmental changes"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"optional-extensions",children:"Optional Extensions"}),"\n",(0,a.jsx)(n.h3,{id:"1-advanced-humanoid-behaviors",children:"1. Advanced Humanoid Behaviors"}),"\n",(0,a.jsx)(n.p,{children:"Implement more sophisticated humanoid behaviors:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Human-like motion planning"}),"\n",(0,a.jsx)(n.li,{children:"Social interaction capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Adaptive learning from user feedback"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-enhanced-perception",children:"2. Enhanced Perception"}),"\n",(0,a.jsx)(n.p,{children:"Add advanced perception capabilities:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"3D object reconstruction"}),"\n",(0,a.jsx)(n.li,{children:"Semantic scene understanding"}),"\n",(0,a.jsx)(n.li,{children:"Human activity recognition"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-improved-planning",children:"3. Improved Planning"}),"\n",(0,a.jsx)(n.p,{children:"Enhance planning with:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Dynamic replanning capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Multi-objective optimization"}),"\n",(0,a.jsx)(n.li,{children:"Risk assessment and mitigation"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"How well does the integrated VLA system handle ambiguous or complex natural language commands?"}),"\n",(0,a.jsx)(n.li,{children:"What are the main bottlenecks in the end-to-end autonomous humanoid system?"}),"\n",(0,a.jsx)(n.li,{children:"How could you improve the system's robustness to environmental uncertainties?"}),"\n",(0,a.jsx)(n.li,{children:"What safety considerations are critical for autonomous humanoid robots in human environments?"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"what-you-learned",children:"What You Learned"}),"\n",(0,a.jsx)(n.p,{children:"In this capstone lab, you implemented a complete autonomous humanoid robot system that integrates all components of the Vision-Language-Action pipeline. You learned how to orchestrate complex multi-step tasks, coordinate navigation and manipulation systems, integrate speech recognition with cognitive planning, and create a robust system architecture for humanoid robotics. This project demonstrates the practical application of VLA systems in creating truly autonomous humanoid robots capable of understanding and executing complex natural language commands in real-world environments."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}}}]);