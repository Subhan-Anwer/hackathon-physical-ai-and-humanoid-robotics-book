"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[1261],{5178(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4-vision-language-action/chapter-3","title":"Chapter 3: Cognitive Planning with LLMs","description":"Learning Objectives","source":"@site/docs/module-4-vision-language-action/chapter-3.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/chapter-3","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-3","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-3.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Chapter 3: Cognitive Planning with LLMs","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Voice Command Processing and Natural Language Understanding","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-2"},"next":{"title":"Chapter 4: Vision-Language Integration for Robot Perception","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-4"}}');var i=t(4848),s=t(8453);const o={title:"Chapter 3: Cognitive Planning with LLMs",sidebar_position:3},r="Chapter 3: Cognitive Planning with LLMs",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"LLM-Based Task Decomposition and Planning",id:"llm-based-task-decomposition-and-planning",level:2},{value:"The Role of LLMs in Robotic Planning",id:"the-role-of-llms-in-robotic-planning",level:3},{value:"Task Decomposition Strategies",id:"task-decomposition-strategies",level:3},{value:"Planning Architecture with LLM Integration",id:"planning-architecture-with-llm-integration",level:3},{value:"Natural Language to Action Mapping",id:"natural-language-to-action-mapping",level:2},{value:"Semantic Action Mapping",id:"semantic-action-mapping",level:3},{value:"Action Schema Framework",id:"action-schema-framework",level:3},{value:"Integration with ROS 2 Navigation and Manipulation Systems",id:"integration-with-ros-2-navigation-and-manipulation-systems",level:2},{value:"ROS 2 Action Interface for Planning",id:"ros-2-action-interface-for-planning",level:3},{value:"Plan Execution with ROS 2 Actions",id:"plan-execution-with-ros-2-actions",level:3},{value:"Planning Under Uncertainty and Dynamic Environments",id:"planning-under-uncertainty-and-dynamic-environments",level:2},{value:"Handling Uncertainty in LLM-Driven Planning",id:"handling-uncertainty-in-llm-driven-planning",level:3},{value:"Adaptive Planning Framework",id:"adaptive-planning-framework",level:3},{value:"Multi-Step Task Execution and Monitoring",id:"multi-step-task-execution-and-monitoring",level:2},{value:"Execution Monitoring System",id:"execution-monitoring-system",level:3},{value:"Task Execution Monitor",id:"task-execution-monitor",level:3},{value:"What You Learned",id:"what-you-learned",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-3-cognitive-planning-with-llms",children:"Chapter 3: Cognitive Planning with LLMs"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement LLM-based task decomposition and planning for robotic systems"}),"\n",(0,i.jsx)(n.li,{children:"Design natural language to action mapping systems using Large Language Models"}),"\n",(0,i.jsx)(n.li,{children:"Integrate LLM planning with ROS 2 navigation and manipulation systems"}),"\n",(0,i.jsx)(n.li,{children:"Apply planning strategies for uncertain and dynamic environments"}),"\n",(0,i.jsx)(n.li,{children:"Monitor and execute multi-step tasks with LLM-driven planning"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"llm-based-task-decomposition-and-planning",children:"LLM-Based Task Decomposition and Planning"}),"\n",(0,i.jsx)(n.h3,{id:"the-role-of-llms-in-robotic-planning",children:"The Role of LLMs in Robotic Planning"}),"\n",(0,i.jsx)(n.p,{children:"Large Language Models have revolutionized robotic planning by providing the ability to understand high-level, abstract commands and decompose them into executable action sequences. Unlike traditional planning systems that require explicit, low-level instructions, LLMs can leverage their vast knowledge base to infer appropriate action sequences from natural language descriptions."}),"\n",(0,i.jsx)(n.p,{children:"The planning process with LLMs involves several key steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Interpretation"}),": Understanding the high-level goal expressed in natural language"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition"}),": Breaking down complex goals into manageable subtasks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Sequencing"}),": Arranging subtasks in the correct order with appropriate preconditions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Integration"}),": Incorporating environmental and robot state information"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan Validation"}),": Ensuring the generated plan is feasible and safe"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"task-decomposition-strategies",children:"Task Decomposition Strategies"}),"\n",(0,i.jsx)(n.p,{children:'LLMs excel at decomposing complex tasks into simpler, executable components. Consider the command "Clean the room." A human would naturally break this down into subtasks like:'}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Identify dirty objects"}),"\n",(0,i.jsx)(n.li,{children:"Collect trash items"}),"\n",(0,i.jsx)(n.li,{children:"Organize misplaced items"}),"\n",(0,i.jsx)(n.li,{children:"Vacuum or mop the floor"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"An LLM-based planning system can learn to perform similar decompositions by training on examples or using in-context learning with appropriate prompts."}),"\n",(0,i.jsx)(n.h3,{id:"planning-architecture-with-llm-integration",children:"Planning Architecture with LLM Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import openai\nimport json\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass TaskType(Enum):\n    NAVIGATION = "navigation"\n    MANIPULATION = "manipulation"\n    PERCEPTION = "perception"\n    INTERACTION = "interaction"\n\n@dataclass\nclass PlanStep:\n    """Represents a single step in the execution plan"""\n    id: int\n    action: str\n    parameters: Dict[str, Any]\n    task_type: TaskType\n    preconditions: List[str]\n    postconditions: List[str]\n    description: str\n\nclass LLMPlanner:\n    """LLM-based task planner for robotic systems"""\n\n    def __init__(self, model_name: str = "gpt-3.5-turbo"):\n        self.model_name = model_name\n        self.planning_history = []\n\n    def decompose_task(self, natural_language_command: str, robot_capabilities: List[str],\n                      environment_state: Dict[str, Any]) -> List[PlanStep]:\n        """Decompose a natural language command into executable steps"""\n\n        # Create a structured prompt for the LLM\n        prompt = self._create_planning_prompt(natural_language_command, robot_capabilities, environment_state)\n\n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model_name,\n                messages=[\n                    {"role": "system", "content": self._get_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.3,\n                functions=[\n                    {\n                        "name": "create_plan",\n                        "description": "Create a step-by-step plan for the robot to execute",\n                        "parameters": {\n                            "type": "object",\n                            "properties": {\n                                "steps": {\n                                    "type": "array",\n                                    "items": {\n                                        "type": "object",\n                                        "properties": {\n                                            "id": {"type": "integer"},\n                                            "action": {"type": "string"},\n                                            "parameters": {"type": "object"},\n                                            "task_type": {"type": "string", "enum": ["navigation", "manipulation", "perception", "interaction"]},\n                                            "description": {"type": "string"}\n                                        },\n                                        "required": ["id", "action", "parameters", "task_type", "description"]\n                                    }\n                                }\n                            },\n                            "required": ["steps"]\n                        }\n                    }\n                ],\n                function_call={"name": "create_plan"}\n            )\n\n            # Parse the response\n            plan_data = json.loads(response.choices[0].message.function_call.arguments)\n            plan_steps = []\n\n            for step_data in plan_data[\'steps\']:\n                step = PlanStep(\n                    id=step_data[\'id\'],\n                    action=step_data[\'action\'],\n                    parameters=step_data[\'parameters\'],\n                    task_type=TaskType(step_data[\'task_type\']),\n                    preconditions=[],\n                    postconditions=[],\n                    description=step_data[\'description\']\n                )\n                plan_steps.append(step)\n\n            return plan_steps\n\n        except Exception as e:\n            print(f"Error in LLM planning: {e}")\n            return self._fallback_planning(natural_language_command)\n\n    def _create_planning_prompt(self, command: str, capabilities: List[str],\n                               environment: Dict[str, Any]) -> str:\n        """Create a structured prompt for the LLM planner"""\n        return f"""\n        Natural language command: {command}\n\n        Robot capabilities: {\', \'.join(capabilities)}\n\n        Current environment state: {json.dumps(environment, indent=2)}\n\n        Please decompose this command into specific, executable steps that the robot can perform.\n        Each step should be a discrete action with clear parameters.\n        Return the steps in a structured format with action type (navigation, manipulation, perception, interaction).\n        """\n\n    def _get_system_prompt(self) -> str:\n        """System prompt to guide the LLM\'s planning behavior"""\n        return """\n        You are an expert robotic task planner. Your role is to decompose high-level natural language commands\n        into specific, executable steps for a robot. Each step should be:\n        1. Feasible given the robot\'s capabilities\n        2. Specific enough to be executed by the robot\n        3. In the correct sequence to achieve the overall goal\n        4. Include necessary parameters for execution\n\n        Consider the environment state when creating the plan to ensure feasibility.\n        """\n\n    def _fallback_planning(self, command: str) -> List[PlanStep]:\n        """Fallback planning in case the LLM fails"""\n        # Simple fallback for common commands\n        if "move" in command.lower() or "go" in command.lower():\n            return [PlanStep(\n                id=1,\n                action="navigate_to",\n                parameters={"target_location": "default"},\n                task_type=TaskType.NAVIGATION,\n                preconditions=[],\n                postconditions=[],\n                description="Move to target location"\n            )]\n        else:\n            return [PlanStep(\n                id=1,\n                action="unknown_command",\n                parameters={},\n                task_type=TaskType.PERCEPTION,\n                preconditions=[],\n                postconditions=[],\n                description="Unable to parse command"\n            )]\n\n# Example usage\nplanner = LLMPlanner()\ncapabilities = ["navigate", "grasp", "place", "detect_objects", "open_gripper", "close_gripper"]\nenvironment = {"objects": ["cup", "book", "pen"], "locations": ["table", "kitchen", "desk"]}\n\nplan = planner.decompose_task("Pick up the red cup and place it on the table", capabilities, environment)\nfor step in plan:\n    print(f"Step {step.id}: {step.description} ({step.task_type.value})")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-to-action-mapping",children:"Natural Language to Action Mapping"}),"\n",(0,i.jsx)(n.h3,{id:"semantic-action-mapping",children:"Semantic Action Mapping"}),"\n",(0,i.jsx)(n.p,{children:"The conversion from natural language to robotic actions requires sophisticated semantic understanding and mapping. This process involves:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Action Grounding"}),": Mapping abstract language concepts to concrete physical actions the robot can perform."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parameter Extraction"}),": Identifying specific parameters needed for action execution (locations, objects, gripper positions, etc.)."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Constraint Handling"}),": Understanding constraints and conditions that must be satisfied before or during action execution."]}),"\n",(0,i.jsx)(n.h3,{id:"action-schema-framework",children:"Action Schema Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from typing import Dict, List, Optional, Callable\nimport inspect\n\nclass ActionSchema:\n    """Defines the structure and execution of robot actions"""\n\n    def __init__(self, name: str, description: str, parameters: Dict[str, Dict],\n                 executor: Callable, preconditions: List[str] = None):\n        self.name = name\n        self.description = description\n        self.parameters = parameters  # {param_name: {"type": type, "required": bool, "description": str}}\n        self.executor = executor\n        self.preconditions = preconditions or []\n\n    def validate_parameters(self, params: Dict) -> bool:\n        """Validate that required parameters are provided"""\n        for param_name, param_info in self.parameters.items():\n            if param_info.get("required", False):\n                if param_name not in params:\n                    return False\n        return True\n\n    def execute(self, **kwargs):\n        """Execute the action with provided parameters"""\n        if not self.validate_parameters(kwargs):\n            raise ValueError(f"Missing required parameters for action {self.name}")\n\n        return self.executor(**kwargs)\n\nclass ActionMapper:\n    """Maps natural language commands to executable actions"""\n\n    def __init__(self):\n        self.action_schemas = {}\n        self._register_default_actions()\n\n    def _register_default_actions(self):\n        """Register common robot actions"""\n        # Navigation action\n        def navigate_to(location: str, speed: float = 0.5):\n            """Navigate to a specific location"""\n            print(f"Navigating to {location} at speed {speed}")\n            # In a real system, this would interface with navigation stack\n            return {"status": "success", "location": location}\n\n        self.action_schemas["navigate_to"] = ActionSchema(\n            name="navigate_to",\n            description="Move the robot to a specified location",\n            parameters={\n                "location": {"type": str, "required": True, "description": "Target location"},\n                "speed": {"type": float, "required": False, "description": "Movement speed (0.0-1.0)", "default": 0.5}\n            },\n            executor=navigate_to\n        )\n\n        # Manipulation action\n        def grasp_object(object_name: str, location: str = None):\n            """Grasp an object"""\n            print(f"Grasping {object_name} at {location or \'current location\'}")\n            return {"status": "success", "object": object_name}\n\n        self.action_schemas["grasp_object"] = ActionSchema(\n            name="grasp_object",\n            description="Grasp a specific object",\n            parameters={\n                "object_name": {"type": str, "required": True, "description": "Name of object to grasp"},\n                "location": {"type": str, "required": False, "description": "Location of object"}\n            },\n            executor=grasp_object\n        )\n\n        # Object detection action\n        def detect_objects(target_object: str = None, location: str = None):\n            """Detect objects in the environment"""\n            print(f"Detecting objects{\' of type \' + target_object if target_object else \'\'} at {location or \'current location\'}")\n            # In a real system, this would interface with perception system\n            return {"status": "success", "objects": ["cup", "book"]}\n\n        self.action_schemas["detect_objects"] = ActionSchema(\n            name="detect_objects",\n            description="Detect objects in the environment",\n            parameters={\n                "target_object": {"type": str, "required": False, "description": "Specific object to detect"},\n                "location": {"type": str, "required": False, "description": "Location to search"}\n            },\n            executor=detect_objects\n        )\n\n    def map_command_to_action(self, parsed_command: Dict) -> Optional[ActionSchema]:\n        """Map a parsed command to an appropriate action schema"""\n        action_type = parsed_command.get(\'action_type\')\n        target_object = parsed_command.get(\'target_object\')\n        target_location = parsed_command.get(\'target_location\')\n\n        # Simple mapping logic - in practice, this would be more sophisticated\n        if action_type == \'navigation\':\n            return self.action_schemas.get(\'navigate_to\')\n        elif action_type == \'manipulation\':\n            if target_object:\n                return self.action_schemas.get(\'grasp_object\')\n        elif action_type == \'observation\':\n            return self.action_schemas.get(\'detect_objects\')\n\n        return None\n\n    def execute_plan_step(self, plan_step: PlanStep, context: Dict) -> Dict:\n        """Execute a single step of a plan"""\n        action_schema = self.action_schemas.get(plan_step.action)\n        if not action_schema:\n            return {"status": "error", "message": f"Unknown action: {plan_step.action}"}\n\n        # Prepare parameters based on plan step and context\n        params = plan_step.parameters.copy()\n\n        # Add context information if needed\n        if \'target_location\' in params and params[\'target_location\'] == \'default\':\n            params[\'target_location\'] = context.get(\'default_location\', \'unknown\')\n\n        try:\n            result = action_schema.execute(**params)\n            return result\n        except Exception as e:\n            return {"status": "error", "message": str(e)}\n\n# Example usage\nmapper = ActionMapper()\nstep = PlanStep(\n    id=1,\n    action="navigate_to",\n    parameters={"location": "kitchen", "speed": 0.7},\n    task_type=TaskType.NAVIGATION,\n    preconditions=[],\n    postconditions=[],\n    description="Move to kitchen"\n)\n\ncontext = {"default_location": "living_room"}\nresult = mapper.execute_plan_step(step, context)\nprint(f"Execution result: {result}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-ros-2-navigation-and-manipulation-systems",children:"Integration with ROS 2 Navigation and Manipulation Systems"}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-action-interface-for-planning",children:"ROS 2 Action Interface for Planning"}),"\n",(0,i.jsx)(n.p,{children:"Integrating LLM-based planning with ROS 2 requires careful coordination between the high-level planning system and the low-level control systems. The typical architecture involves:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Planner Node"}),": Generates high-level plans from natural language commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan Execution Node"}),": Translates plan steps into ROS 2 actions and monitors execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation System"}),": Executes navigation actions using Nav2"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation System"}),": Executes manipulation actions using MoveIt2 and controllers"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"plan-execution-with-ros-2-actions",children:"Plan Execution with ROS 2 Actions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport threading\nimport time\n\nclass PlanExecutorNode(Node):\n    """Executes plans generated by LLM planner using ROS 2 actions"""\n\n    def __init__(self):\n        super().__init__(\'plan_executor_node\')\n\n        # Create action clients for navigation\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Create subscribers\n        self.plan_sub = self.create_subscription(\n            String,\n            \'high_level_plan\',\n            self.plan_callback,\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            \'execution_status\',\n            10\n        )\n\n        # Plan execution state\n        self.current_plan = None\n        self.execution_thread = None\n        self.is_executing = False\n\n        self.get_logger().info("Plan Executor Node initialized")\n\n    def plan_callback(self, msg):\n        """Handle incoming plan from LLM planner"""\n        try:\n            plan_data = json.loads(msg.data)\n            self.current_plan = plan_data[\'steps\']\n            self.get_logger().info(f"Received plan with {len(self.current_plan)} steps")\n\n            # Start execution in a separate thread\n            if self.execution_thread is None or not self.execution_thread.is_alive():\n                self.execution_thread = threading.Thread(target=self.execute_plan)\n                self.execution_thread.start()\n            else:\n                self.get_logger().warn("Plan execution already in progress, skipping new plan")\n\n        except json.JSONDecodeError:\n            self.get_logger().error("Invalid JSON in plan message")\n        except Exception as e:\n            self.get_logger().error(f"Error processing plan: {str(e)}")\n\n    def execute_plan(self):\n        """Execute the current plan step by step"""\n        if not self.current_plan:\n            return\n\n        self.is_executing = True\n\n        for step_idx, step in enumerate(self.current_plan):\n            if not self.is_executing:\n                self.get_logger().info("Plan execution stopped by user")\n                break\n\n            self.get_logger().info(f"Executing step {step_idx + 1}: {step[\'description\']}")\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f"Executing step {step_idx + 1}: {step[\'description\']}"\n            self.status_pub.publish(status_msg)\n\n            # Execute the step based on its type\n            success = self.execute_step(step)\n\n            if not success:\n                self.get_logger().error(f"Failed to execute step {step_idx + 1}")\n                status_msg.data = f"Failed executing step {step_idx + 1}: {step[\'description\']}"\n                self.status_pub.publish(status_msg)\n                break\n\n        self.is_executing = False\n        self.get_logger().info("Plan execution completed")\n\n        # Publish completion status\n        status_msg = String()\n        status_msg.data = "Plan execution completed"\n        self.status_pub.publish(status_msg)\n\n    def execute_step(self, step: Dict) -> bool:\n        """Execute a single plan step"""\n        try:\n            if step[\'task_type\'] == \'navigation\':\n                return self.execute_navigation_step(step)\n            elif step[\'task_type\'] == \'manipulation\':\n                return self.execute_manipulation_step(step)\n            elif step[\'task_type\'] == \'perception\':\n                return self.execute_perception_step(step)\n            else:\n                self.get_logger().warn(f"Unknown step type: {step[\'task_type\']}")\n                return False\n        except Exception as e:\n            self.get_logger().error(f"Error executing step: {str(e)}")\n            return False\n\n    def execute_navigation_step(self, step: Dict) -> bool:\n        """Execute a navigation step using Nav2"""\n        # Wait for action server\n        if not self.nav_client.wait_for_server(timeout_sec=5.0):\n            self.get_logger().error("Navigation action server not available")\n            return False\n\n        # Create goal message\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n\n        # Set target pose (simplified - in practice, you\'d get this from step parameters)\n        target_location = step[\'parameters\'].get(\'location\', \'default\')\n\n        # This is a simplified example - in practice, you\'d look up coordinates\n        # for the named location from a map or knowledge base\n        if target_location == \'kitchen\':\n            goal_msg.pose.pose.position.x = 2.0\n            goal_msg.pose.pose.position.y = 1.0\n        elif target_location == \'living_room\':\n            goal_msg.pose.pose.position.x = 0.0\n            goal_msg.pose.pose.position.y = 0.0\n        else:\n            # Default coordinates\n            goal_msg.pose.pose.position.x = 1.0\n            goal_msg.pose.pose.position.y = 1.0\n\n        goal_msg.pose.pose.orientation.w = 1.0\n\n        # Send goal\n        future = self.nav_client.send_goal_async(goal_msg)\n\n        # Wait for result (with timeout)\n        rclpy.spin_until_future_complete(self, future, timeout_sec=30.0)\n\n        if future.result() is not None:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                result_future = goal_handle.get_result_async()\n                rclpy.spin_until_future_complete(self, result_future, timeout_sec=30.0)\n\n                if result_future.result() is not None:\n                    result = result_future.result().result\n                    self.get_logger().info(f"Navigation completed: {result}")\n                    return True\n                else:\n                    self.get_logger().error("Navigation result future was None")\n                    return False\n            else:\n                self.get_logger().error("Navigation goal was rejected")\n                return False\n        else:\n            self.get_logger().error("Navigation goal future was None")\n            return False\n\n    def execute_manipulation_step(self, step: Dict) -> bool:\n        """Execute a manipulation step"""\n        # This would interface with MoveIt2 or other manipulation systems\n        # For now, we\'ll simulate the execution\n        self.get_logger().info(f"Executing manipulation: {step[\'parameters\']}")\n        time.sleep(2.0)  # Simulate execution time\n        return True\n\n    def execute_perception_step(self, step: Dict) -> bool:\n        """Execute a perception step"""\n        # This would interface with perception systems\n        # For now, we\'ll simulate the execution\n        self.get_logger().info(f"Executing perception: {step[\'parameters\']}")\n        time.sleep(1.0)  # Simulate execution time\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    executor_node = PlanExecutorNode()\n\n    try:\n        rclpy.spin(executor_node)\n    except KeyboardInterrupt:\n        executor_node.get_logger().info("Shutting down plan executor")\n    finally:\n        executor_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"planning-under-uncertainty-and-dynamic-environments",children:"Planning Under Uncertainty and Dynamic Environments"}),"\n",(0,i.jsx)(n.h3,{id:"handling-uncertainty-in-llm-driven-planning",children:"Handling Uncertainty in LLM-Driven Planning"}),"\n",(0,i.jsx)(n.p,{children:"Robotic environments are inherently uncertain and dynamic, which presents challenges for LLM-driven planning. Key considerations include:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Perception Uncertainty"}),": The robot's understanding of the environment may be incomplete or incorrect."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Action Execution Uncertainty"}),": Actions may not execute as expected due to environmental factors or robot limitations."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Temporal Uncertainty"}),": Environmental conditions may change between planning and execution."]}),"\n",(0,i.jsx)(n.h3,{id:"adaptive-planning-framework",children:"Adaptive Planning Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from typing import Dict, List, Tuple, Optional\nimport random\n\nclass AdaptivePlanner:\n    """Handles planning in uncertain and dynamic environments"""\n\n    def __init__(self, llm_planner: LLMPlanner):\n        self.llm_planner = llm_planner\n        self.execution_history = []\n        self.uncertainty_model = UncertaintyModel()\n\n    def create_adaptive_plan(self, command: str, robot_capabilities: List[str],\n                           environment_state: Dict[str, Any]) -> List[PlanStep]:\n        """Create a plan that accounts for uncertainty"""\n        # Generate initial plan\n        initial_plan = self.llm_planner.decompose_task(command, robot_capabilities, environment_state)\n\n        # Add uncertainty handling to the plan\n        adaptive_plan = self._add_uncertainty_handling(initial_plan, environment_state)\n\n        return adaptive_plan\n\n    def _add_uncertainty_handling(self, plan: List[PlanStep], env_state: Dict[str, Any]) -> List[PlanStep]:\n        """Add uncertainty handling steps to the plan"""\n        adaptive_plan = []\n\n        for step in plan:\n            # Add perception steps before critical actions\n            if step.task_type in [TaskType.NAVIGATION, TaskType.MANIPULATION]:\n                # Add environment verification before critical steps\n                verification_step = PlanStep(\n                    id=len(adaptive_plan) + 1,\n                    action="verify_environment",\n                    parameters={"target": self._get_verification_target(step)},\n                    task_type=TaskType.PERCEPTION,\n                    preconditions=[],\n                    postconditions=[],\n                    description=f"Verify environment for {step.description}"\n                )\n                adaptive_plan.append(verification_step)\n\n            # Add the original step\n            step.id = len(adaptive_plan) + 1\n            adaptive_plan.append(step)\n\n            # Add post-execution verification\n            verification_step = PlanStep(\n                id=len(adaptive_plan) + 1,\n                action="verify_execution",\n                parameters={"expected_result": self._get_expected_result(step)},\n                task_type=TaskType.PERCEPTION,\n                preconditions=[],\n                postconditions=[],\n                description=f"Verify successful execution of {step.description}"\n            )\n            adaptive_plan.append(verification_step)\n\n        return adaptive_plan\n\n    def _get_verification_target(self, step: PlanStep) -> str:\n        """Get the target for environment verification"""\n        if step.task_type == TaskType.NAVIGATION:\n            return step.parameters.get(\'location\', \'unknown\')\n        elif step.task_type == TaskType.MANIPULATION:\n            return step.parameters.get(\'object_name\', \'unknown\')\n        return \'environment\'\n\n    def _get_expected_result(self, step: PlanStep) -> Dict[str, Any]:\n        """Get the expected result of executing a step"""\n        return {\n            \'action\': step.action,\n            \'parameters\': step.parameters,\n            \'completed\': True\n        }\n\n    def handle_execution_failure(self, failed_step: PlanStep, error: Exception,\n                               current_env: Dict[str, Any]) -> List[PlanStep]:\n        """Handle failure of a plan step and generate recovery actions"""\n        self.get_logger().warn(f"Step failed: {failed_step.description}, error: {str(error)}")\n\n        # Generate recovery plan based on the type of failure\n        if "navigation" in failed_step.action.lower():\n            return self._handle_navigation_failure(failed_step, current_env)\n        elif "manipulation" in failed_step.action.lower():\n            return self._handle_manipulation_failure(failed_step, current_env)\n        else:\n            return self._handle_general_failure(failed_step, current_env)\n\n    def _handle_navigation_failure(self, step: PlanStep, env: Dict[str, Any]) -> List[PlanStep]:\n        """Handle navigation failure"""\n        # Try alternative route or ask for help\n        recovery_steps = [\n            PlanStep(\n                id=1,\n                action="detect_obstacles",\n                parameters={"location": step.parameters.get(\'location\')},\n                task_type=TaskType.PERCEPTION,\n                preconditions=[],\n                postconditions=[],\n                description="Detect obstacles in path"\n            ),\n            PlanStep(\n                id=2,\n                action="request_assistance",\n                parameters={"reason": "navigation_failed", "location": step.parameters.get(\'location\')},\n                task_type=TaskType.INTERACTION,\n                preconditions=[],\n                postconditions=[],\n                description="Request human assistance for navigation"\n            )\n        ]\n        return recovery_steps\n\n    def _handle_manipulation_failure(self, step: PlanStep, env: Dict[str, Any]) -> List[PlanStep]:\n        """Handle manipulation failure"""\n        recovery_steps = [\n            PlanStep(\n                id=1,\n                action="relocate_object",\n                parameters={"object": step.parameters.get(\'object_name\')},\n                task_type=TaskType.NAVIGATION,\n                preconditions=[],\n                postconditions=[],\n                description="Move closer to object for better manipulation"\n            ),\n            PlanStep(\n                id=2,\n                action="verify_object_properties",\n                parameters={"object": step.parameters.get(\'object_name\')},\n                task_type=TaskType.PERCEPTION,\n                preconditions=[],\n                postconditions=[],\n                description="Verify object properties for manipulation"\n            )\n        ]\n        return recovery_steps\n\n    def _handle_general_failure(self, step: PlanStep, env: Dict[str, Any]) -> List[PlanStep]:\n        """Handle general failure"""\n        recovery_steps = [\n            PlanStep(\n                id=1,\n                action="request_clarification",\n                parameters={"step": step.description},\n                task_type=TaskType.INTERACTION,\n                preconditions=[],\n                postconditions=[],\n                description="Request clarification about the task"\n            )\n        ]\n        return recovery_steps\n\nclass UncertaintyModel:\n    """Models uncertainty in the planning process"""\n\n    def __init__(self):\n        self.uncertainty_factors = {\n            \'navigation\': 0.1,  # 10% failure rate for navigation\n            \'manipulation\': 0.15,  # 15% failure rate for manipulation\n            \'perception\': 0.05,  # 5% failure rate for perception\n            \'interaction\': 0.08   # 8% failure rate for interaction\n        }\n\n    def calculate_success_probability(self, step: PlanStep, env_state: Dict[str, Any]) -> float:\n        """Calculate the probability of success for a step"""\n        base_prob = 1.0 - self.uncertainty_factors.get(step.task_type.value, 0.1)\n\n        # Adjust based on environmental factors\n        if env_state.get(\'obstacles\', 0) > 5 and step.task_type == TaskType.NAVIGATION:\n            base_prob *= 0.7  # Reduce success probability in cluttered environment\n\n        if env_state.get(\'lighting\', \'good\') == \'poor\' and step.task_type == TaskType.MANIPULATION:\n            base_prob *= 0.8  # Reduce success probability in poor lighting\n\n        return max(0.0, min(1.0, base_prob))\n'})}),"\n",(0,i.jsx)(n.h2,{id:"multi-step-task-execution-and-monitoring",children:"Multi-Step Task Execution and Monitoring"}),"\n",(0,i.jsx)(n.h3,{id:"execution-monitoring-system",children:"Execution Monitoring System"}),"\n",(0,i.jsx)(n.p,{children:"Executing multi-step tasks requires continuous monitoring and the ability to adapt to changing conditions. The monitoring system tracks:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Step Completion"}),": Whether each step has been successfully completed"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan Deviation"}),": Detection of deviations from the expected plan"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recovery Actions"}),": Execution of recovery steps when failures occur"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Progress Tracking"}),": Monitoring overall progress toward the goal"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"task-execution-monitor",children:"Task Execution Monitor"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from datetime import datetime, timedelta\nfrom enum import Enum\n\nclass ExecutionStatus(Enum):\n    PENDING = "pending"\n    EXECUTING = "executing"\n    SUCCESS = "success"\n    FAILED = "failed"\n    RECOVERING = "recovering"\n\nclass TaskExecutionMonitor:\n    """Monitors execution of multi-step tasks and handles deviations"""\n\n    def __init__(self, adaptive_planner: AdaptivePlanner):\n        self.adaptive_planner = adaptive_planner\n        self.current_task = None\n        self.current_plan = []\n        self.step_status = {}  # {step_id: ExecutionStatus}\n        self.execution_start_time = None\n        self.last_update_time = None\n        self.max_execution_time = 300  # 5 minutes max execution time\n\n    def start_task_execution(self, plan: List[PlanStep], task_description: str):\n        """Start execution of a multi-step task"""\n        self.current_task = task_description\n        self.current_plan = plan\n        self.step_status = {step.id: ExecutionStatus.PENDING for step in plan}\n        self.execution_start_time = datetime.now()\n        self.last_update_time = datetime.now()\n\n        self.get_logger().info(f"Starting execution of task: {task_description}")\n        self.get_logger().info(f"Plan contains {len(plan)} steps")\n\n    def update_step_status(self, step_id: int, status: ExecutionStatus, details: str = ""):\n        """Update the status of a specific step"""\n        if step_id in self.step_status:\n            self.step_status[step_id] = status\n            self.last_update_time = datetime.now()\n\n            if status == ExecutionStatus.SUCCESS:\n                self.get_logger().info(f"Step {step_id} completed successfully")\n            elif status == ExecutionStatus.FAILED:\n                self.get_logger().error(f"Step {step_id} failed: {details}")\n            elif status == ExecutionStatus.EXECUTING:\n                self.get_logger().info(f"Step {step_id} started execution")\n\n    def check_execution_timeout(self) -> bool:\n        """Check if the current execution has timed out"""\n        if self.execution_start_time:\n            elapsed = datetime.now() - self.execution_start_time\n            return elapsed.total_seconds() > self.max_execution_time\n        return False\n\n    def get_overall_progress(self) -> Dict[str, Any]:\n        """Get overall progress of the current task"""\n        if not self.current_plan:\n            return {"status": "no_task", "progress": 0.0}\n\n        completed_steps = sum(1 for status in self.step_status.values()\n                             if status in [ExecutionStatus.SUCCESS, ExecutionStatus.FAILED])\n        total_steps = len(self.current_plan)\n        progress = completed_steps / total_steps if total_steps > 0 else 0.0\n\n        # Determine overall status\n        if any(status == ExecutionStatus.FAILED for status in self.step_status.values()):\n            overall_status = "failed"\n        elif all(status == ExecutionStatus.SUCCESS for status in self.step_status.values()):\n            overall_status = "completed"\n        elif any(status == ExecutionStatus.EXECUTING for status in self.step_status.values()):\n            overall_status = "executing"\n        else:\n            overall_status = "pending"\n\n        return {\n            "status": overall_status,\n            "progress": progress,\n            "completed_steps": completed_steps,\n            "total_steps": total_steps,\n            "elapsed_time": (datetime.now() - self.execution_start_time).total_seconds() if self.execution_start_time else 0\n        }\n\n    def handle_step_failure(self, failed_step: PlanStep, error: Exception,\n                          current_env: Dict[str, Any]) -> List[PlanStep]:\n        """Handle failure of a step and return recovery plan"""\n        self.update_step_status(failed_step.id, ExecutionStatus.FAILED, str(error))\n\n        # Generate recovery plan\n        recovery_plan = self.adaptive_planner.handle_execution_failure(\n            failed_step, error, current_env\n        )\n\n        # Update status to recovering\n        for recovery_step in recovery_plan:\n            self.step_status[recovery_step.id] = ExecutionStatus.PENDING\n\n        return recovery_plan\n\n    def is_task_complete(self) -> bool:\n        """Check if the current task is complete"""\n        if not self.current_plan:\n            return True\n\n        return all(status == ExecutionStatus.SUCCESS for status in self.step_status.values())\n\n    def get_logger(self):\n        """Simple logger for the monitor"""\n        class SimpleLogger:\n            def info(self, msg):\n                print(f"INFO: {msg}")\n            def error(self, msg):\n                print(f"ERROR: {msg}")\n            def warn(self, msg):\n                print(f"WARN: {msg}")\n        return SimpleLogger()\n\n# Example usage\ndef example_task_execution():\n    # Create components\n    llm_planner = LLMPlanner()\n    adaptive_planner = AdaptivePlanner(llm_planner)\n    monitor = TaskExecutionMonitor(adaptive_planner)\n\n    # Create a sample plan\n    sample_plan = [\n        PlanStep(1, "navigate_to", {"location": "kitchen"}, TaskType.NAVIGATION, [], [], "Go to kitchen"),\n        PlanStep(2, "detect_objects", {"target_object": "cup"}, TaskType.PERCEPTION, [], [], "Find the cup"),\n        PlanStep(3, "grasp_object", {"object_name": "cup"}, TaskType.MANIPULATION, [], [], "Pick up the cup"),\n        PlanStep(4, "navigate_to", {"location": "table"}, TaskType.NAVIGATION, [], [], "Go to table"),\n        PlanStep(5, "place_object", {"location": "table"}, TaskType.MANIPULATION, [], [], "Place cup on table")\n    ]\n\n    # Start execution\n    monitor.start_task_execution(sample_plan, "Bring cup from kitchen to table")\n\n    # Simulate execution progress\n    for i, step in enumerate(sample_plan):\n        print(f"Executing step {i+1}: {step.description}")\n        monitor.update_step_status(step.id, ExecutionStatus.EXECUTING)\n\n        # Simulate some processing time\n        import time\n        time.sleep(0.5)\n\n        # Simulate success for most steps, failure for step 3\n        if step.id == 3:  # The grasp step\n            monitor.update_step_status(step.id, ExecutionStatus.FAILED, "Object not graspable")\n\n            # Handle the failure\n            recovery_plan = monitor.handle_step_failure(\n                step, Exception("Object not graspable"), {"objects": ["cup"]}\n            )\n            print(f"Generated recovery plan with {len(recovery_plan)} steps")\n        else:\n            monitor.update_step_status(step.id, ExecutionStatus.SUCCESS)\n\n    # Get final status\n    progress = monitor.get_overall_progress()\n    print(f"Final progress: {progress}")\n\nif __name__ == "__main__":\n    example_task_execution()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"what-you-learned",children:"What You Learned"}),"\n",(0,i.jsx)(n.p,{children:"In this chapter, you've learned how to implement sophisticated cognitive planning systems using Large Language Models for robotics applications. You now understand how to decompose complex tasks into executable steps, map natural language commands to robotic actions, integrate planning with ROS 2 navigation and manipulation systems, handle uncertainty and dynamic environments, and monitor multi-step task execution. These capabilities form the cognitive core of VLA systems, enabling robots to understand high-level commands and execute them through complex, adaptive behaviors."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);