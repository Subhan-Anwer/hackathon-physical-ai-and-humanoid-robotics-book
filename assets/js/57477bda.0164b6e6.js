"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[782],{5169(e,n,a){a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer","title":"Lab 4: Sim-to-Real Transfer","description":"Objective","source":"@site/docs/module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer.md","sourceDirName":"module-3-ai-robot-brain/labs","slug":"/module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Lab 4: Sim-to-Real Transfer","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Lab 3: Navigation and VSLAM Integration","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/category/module-4-vision-language-action-vla"}}');var s=a(4848),i=a(8453);const r={title:"Lab 4: Sim-to-Real Transfer",sidebar_position:4},o="Lab 4: Sim-to-Real Transfer",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Apply Domain Randomization Techniques",id:"step-1-apply-domain-randomization-techniques",level:3},{value:"Step 2: Train Robot Behaviors in Simulation",id:"step-2-train-robot-behaviors-in-simulation",level:3},{value:"Step 3: Transfer Learned Behaviors to Real Hardware",id:"step-3-transfer-learned-behaviors-to-real-hardware",level:3},{value:"Step 4: Evaluate Performance and Adaptation Strategies",id:"step-4-evaluate-performance-and-adaptation-strategies",level:3},{value:"Expected Outcome",id:"expected-outcome",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Optional Extension Tasks",id:"optional-extension-tasks",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lab-4-sim-to-real-transfer",children:"Lab 4: Sim-to-Real Transfer"})}),"\n",(0,s.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you will apply domain randomization techniques to improve sim-to-real transfer, train robot behaviors in simulation, transfer learned behaviors to real hardware, and evaluate performance and adaptation strategies. This lab demonstrates the complete pipeline from simulation-based training to real-world deployment and validation."}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed all previous labs (Lab 3.1 through Lab 3.3)"}),"\n",(0,s.jsx)(n.li,{children:"Access to a physical robot platform (TurtleBot3, Husky, or similar)"}),"\n",(0,s.jsx)(n.li,{children:"Simulation environment with trained behaviors from previous labs"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of reinforcement learning concepts"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of robot calibration procedures"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-apply-domain-randomization-techniques",children:"Step 1: Apply Domain Randomization Techniques"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Configure Domain Randomization in Isaac Sim"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# domain_randomization_config.py\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.stage import get_current_stage\nimport carb\nimport numpy as np\nimport random\n\nclass DomainRandomization:\n    def __init__(self):\n        self.stage = get_current_stage()\n        self.randomization_params = {\n            'lighting': {\n                'intensity_range': (0.5, 2.0),\n                'color_temperature_range': (3000, 8000),\n                'direction_range': (0, 360)\n            },\n            'textures': {\n                'roughness_range': (0.0, 1.0),\n                'metallic_range': (0.0, 1.0),\n                'albedo_range': (0.0, 1.0)\n            },\n            'physics': {\n                'friction_range': (0.1, 1.0),\n                'restitution_range': (0.0, 0.5),\n                'mass_variance': 0.1\n            },\n            'sensor_noise': {\n                'camera_noise': 0.01,\n                'lidar_noise': 0.02,\n                'imu_drift': 0.001\n            }\n        }\n\n    def randomize_lighting(self):\n        \"\"\"Randomize lighting conditions in the scene\"\"\"\n        lights = self.get_all_lights()\n        for light in lights:\n            # Randomize intensity\n            intensity = random.uniform(\n                self.randomization_params['lighting']['intensity_range'][0],\n                self.randomization_params['lighting']['intensity_range'][1]\n            )\n            light.GetAttribute(\"intensity\").Set(intensity)\n\n            # Randomize color temperature\n            color_temp = random.uniform(\n                self.randomization_params['lighting']['color_temperature_range'][0],\n                self.randomization_params['lighting']['color_temperature_range'][1]\n            )\n            # Convert color temperature to RGB (simplified)\n            rgb = self.color_temperature_to_rgb(color_temp)\n            light.GetAttribute(\"color\").Set(carb.Float3(rgb[0], rgb[1], rgb[2]))\n\n    def randomize_textures(self):\n        \"\"\"Randomize surface textures and materials\"\"\"\n        # Get all objects in the scene\n        objects = self.get_all_objects()\n        for obj in objects:\n            # Randomize material properties\n            roughness = random.uniform(\n                self.randomization_params['textures']['roughness_range'][0],\n                self.randomization_params['textures']['roughness_range'][1]\n            )\n            metallic = random.uniform(\n                self.randomization_params['textures']['metallic_range'][0],\n                self.randomization_params['textures']['metallic_range'][1]\n            )\n            albedo = random.uniform(\n                self.randomization_params['textures']['albedo_range'][0],\n                self.randomization_params['textures']['albedo_range'][1]\n            )\n\n            # Apply to material\n            self.apply_material_properties(obj, roughness, metallic, albedo)\n\n    def randomize_physics(self):\n        \"\"\"Randomize physics properties\"\"\"\n        objects = self.get_all_objects()\n        for obj in objects:\n            # Randomize friction\n            friction = random.uniform(\n                self.randomization_params['physics']['friction_range'][0],\n                self.randomization_params['physics']['friction_range'][1]\n            )\n            obj.GetAttribute(\"physics:staticFriction\").Set(friction)\n            obj.GetAttribute(\"physics:dynamicFriction\").Set(friction)\n\n            # Randomize mass with variance\n            base_mass = obj.GetAttribute(\"physics:mass\").Get()\n            variance = self.randomization_params['physics']['mass_variance']\n            new_mass = base_mass * random.uniform(1-variance, 1+variance)\n            obj.GetAttribute(\"physics:mass\").Set(new_mass)\n\n    def add_sensor_noise(self, sensor_data):\n        \"\"\"Add realistic noise to sensor data\"\"\"\n        # Add noise to camera data\n        if 'camera' in sensor_data:\n            noise = np.random.normal(0, self.randomization_params['sensor_noise']['camera_noise'], sensor_data['camera'].shape)\n            sensor_data['camera'] = sensor_data['camera'] + noise\n            sensor_data['camera'] = np.clip(sensor_data['camera'], 0, 255)\n\n        # Add noise to LIDAR data\n        if 'lidar' in sensor_data:\n            noise = np.random.normal(0, self.randomization_params['sensor_noise']['lidar_noise'], len(sensor_data['lidar']))\n            sensor_data['lidar'] = sensor_data['lidar'] + noise\n            sensor_data['lidar'] = np.maximum(sensor_data['lidar'], 0)\n\n        # Simulate IMU drift\n        if 'imu' in sensor_data:\n            drift = np.random.normal(0, self.randomization_params['sensor_noise']['imu_drift'], 6)\n            sensor_data['imu'] = sensor_data['imu'] + drift\n\n        return sensor_data\n\n    def color_temperature_to_rgb(self, color_temp):\n        \"\"\"Convert color temperature to RGB values (simplified approximation)\"\"\"\n        temp = color_temp / 100\n        if temp <= 66:\n            red = 255\n            green = temp\n            green = 99.4708025861 * math.log(green) - 161.1195681661\n        else:\n            red = temp - 60\n            red = 329.698727446 * (red ** -0.1332047592)\n            green = temp - 60\n            green = 288.1221695283 * (green ** -0.0755148492)\n\n        if temp >= 66:\n            blue = 255\n        elif temp <= 19:\n            blue = 0\n        else:\n            blue = temp - 10\n            blue = 138.5177312231 * math.log(blue) - 305.0447927307\n\n        return [max(0, min(255, red))/255, max(0, min(255, green))/255, max(0, min(255, blue))/255]\n\n    def get_all_lights(self):\n        \"\"\"Get all light prims in the scene\"\"\"\n        from pxr import UsdLux\n        lights = []\n        for prim in self.stage.TraverseAll():\n            if prim.IsA(UsdLux.DistantLight) or prim.IsA(UsdLux.DiskLight) or prim.IsA(UsdLux.SphereLight):\n                lights.append(prim)\n        return lights\n\n    def get_all_objects(self):\n        \"\"\"Get all object prims in the scene\"\"\"\n        objects = []\n        for prim in self.stage.TraverseAll():\n            if prim.GetTypeName() in ['Cube', 'Sphere', 'Cylinder', 'Mesh']:\n                objects.append(prim)\n        return objects\n\n    def apply_material_properties(self, obj, roughness, metallic, albedo):\n        \"\"\"Apply material properties to an object\"\"\"\n        # This is a simplified implementation\n        # In practice, you'd use Isaac Sim's material system\n        pass\n\n# Usage in simulation training loop\ndr = DomainRandomization()\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implement Randomization During Training"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# training_with_randomization.py\nimport rclpy\nfrom rclpy.node import Node\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass TrainingWithRandomization(Node):\n    def __init__(self):\n        super().__init__(\'training_with_randomization\')\n\n        # Initialize domain randomization\n        self.domain_randomizer = DomainRandomization()\n\n        # Neural network for policy\n        self.policy_network = self.create_policy_network()\n        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.001)\n\n        # Training parameters\n        self.episode_count = 0\n        self.max_episodes = 1000\n\n        # Apply randomization every N steps\n        self.randomization_frequency = 10\n\n    def train_episode(self):\n        """Train one episode with domain randomization"""\n        # Apply domain randomization\n        if self.episode_count % self.randomization_frequency == 0:\n            self.domain_randomizer.randomize_lighting()\n            self.domain_randomizer.randomize_textures()\n            self.domain_randomizer.randomize_physics()\n            self.get_logger().info(f"Applied domain randomization at episode {self.episode_count}")\n\n        # Reset environment\n        obs = self.reset_environment()\n        done = False\n        episode_reward = 0\n\n        while not done:\n            # Get action from policy\n            action = self.get_action(obs)\n\n            # Execute action\n            next_obs, reward, done = self.step_environment(action)\n\n            # Add sensor noise\n            next_obs = self.domain_randomizer.add_sensor_noise(next_obs)\n\n            # Store transition\n            self.store_transition(obs, action, reward, next_obs, done)\n\n            # Update policy\n            self.update_policy()\n\n            obs = next_obs\n            episode_reward += reward\n\n        self.episode_count += 1\n        self.get_logger().info(f"Episode {self.episode_count}, Reward: {episode_reward}")\n\n    def create_policy_network(self):\n        """Create neural network for policy"""\n        class PolicyNetwork(nn.Module):\n            def __init__(self, input_size, output_size):\n                super(PolicyNetwork, self).__init__()\n                self.fc1 = nn.Linear(input_size, 256)\n                self.fc2 = nn.Linear(256, 256)\n                self.fc3 = nn.Linear(256, output_size)\n                self.relu = nn.ReLU()\n                self.softmax = nn.Softmax(dim=-1)\n\n            def forward(self, x):\n                x = self.relu(self.fc1(x))\n                x = self.relu(self.fc2(x))\n                x = self.fc3(x)\n                return self.softmax(x)\n\n        return PolicyNetwork(input_size=24, output_size=4)  # Adjust sizes as needed\n\n    def get_action(self, obs):\n        """Get action from policy network"""\n        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n        action_probs = self.policy_network(obs_tensor)\n        action = torch.multinomial(action_probs, 1).item()\n        return action\n\n    def update_policy(self):\n        """Update policy using stored transitions"""\n        # Implement policy update (e.g., PPO, DQN, etc.)\n        pass\n\n    def store_transition(self, obs, action, reward, next_obs, done):\n        """Store transition in replay buffer"""\n        pass\n\n    def reset_environment(self):\n        """Reset simulation environment"""\n        pass\n\n    def step_environment(self, action):\n        """Execute action in environment"""\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    trainer = TrainingWithRandomization()\n\n    # Run training episodes\n    while trainer.episode_count < trainer.max_episodes:\n        trainer.train_episode()\n\n    # Save trained model\n    torch.save(trainer.policy_network.state_dict(), "trained_policy.pth")\n\n    trainer.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-2-train-robot-behaviors-in-simulation",children:"Step 2: Train Robot Behaviors in Simulation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Set Up Reinforcement Learning Environment"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# rl_environment.py\nimport gym\nfrom gym import spaces\nimport numpy as np\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image\nfrom geometry_msgs.msg import Twist, Pose\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\n\nclass RobotRLEnvironment(gym.Env):\n    def __init__(self):\n        super(RobotRLEnvironment, self).__init__()\n\n        # Define action and observation spaces\n        self.action_space = spaces.Discrete(4)  # Forward, backward, left, right\n        self.observation_space = spaces.Box(\n            low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32\n        )  # 20 LIDAR readings + 3 pose values + 1 velocity\n\n        # ROS setup\n        self.node = rclpy.create_node(\'rl_environment\')\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.cmd_vel_pub = self.node.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.laser_sub = self.node.create_subscription(\n            LaserScan, \'/scan\', self.laser_callback, 10)\n        self.odom_sub = self.node.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10)\n\n        # State variables\n        self.laser_data = np.zeros(20)\n        self.pose = np.zeros(3)  # x, y, theta\n        self.velocity = 0.0\n        self.collision = False\n\n        # Episode parameters\n        self.max_steps = 1000\n        self.current_step = 0\n        self.goal_position = np.array([5.0, 5.0])\n        self.start_position = np.array([0.0, 0.0])\n\n    def laser_callback(self, msg):\n        """Handle laser scan data"""\n        # Take 20 readings evenly spaced around 360 degrees\n        num_readings = 20\n        step = len(msg.ranges) // num_readings\n        self.laser_data = np.array([msg.ranges[i*step] for i in range(num_readings)])\n        self.laser_data = np.nan_to_num(self.laser_data, nan=10.0, posinf=10.0, neginf=0.0)\n\n    def odom_callback(self, msg):\n        """Handle odometry data"""\n        self.pose[0] = msg.pose.pose.position.x\n        self.pose[1] = msg.pose.pose.position.y\n\n        # Convert quaternion to euler\n        orientation = msg.pose.pose.orientation\n        self.pose[2] = self.quaternion_to_euler(orientation)\n\n        self.velocity = np.sqrt(\n            msg.twist.twist.linear.x**2 +\n            msg.twist.twist.linear.y**2 +\n            msg.twist.twist.linear.z**2\n        )\n\n    def quaternion_to_euler(self, quat):\n        """Convert quaternion to euler angle (simplified for z-axis rotation)"""\n        siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)\n        cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)\n        return np.arctan2(siny_cosp, cosy_cosp)\n\n    def step(self, action):\n        """Execute one step in the environment"""\n        # Convert action to velocity command\n        cmd_vel = Twist()\n        if action == 0:  # Forward\n            cmd_vel.linear.x = 0.5\n            cmd_vel.angular.z = 0.0\n        elif action == 1:  # Backward\n            cmd_vel.linear.x = -0.5\n            cmd_vel.angular.z = 0.0\n        elif action == 2:  # Turn left\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.5\n        elif action == 3:  # Turn right\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = -0.5\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Wait for next observation\n        rclpy.spin_once(self.node, timeout_sec=0.1)\n\n        # Check for collision\n        min_distance = np.min(self.laser_data)\n        self.collision = min_distance < 0.3\n\n        # Calculate reward\n        reward = self.calculate_reward()\n\n        # Check if episode is done\n        current_pos = np.array([self.pose[0], self.pose[1]])\n        distance_to_goal = np.linalg.norm(current_pos - self.goal_position)\n        done = (self.collision or\n                distance_to_goal < 0.5 or\n                self.current_step >= self.max_steps)\n\n        # Prepare observation\n        obs = np.concatenate([\n            self.laser_data,\n            self.pose,\n            [self.velocity]\n        ]).astype(np.float32)\n\n        self.current_step += 1\n\n        return obs, reward, done, {}\n\n    def calculate_reward(self):\n        """Calculate reward based on current state"""\n        current_pos = np.array([self.pose[0], self.pose[1]])\n        distance_to_goal = np.linalg.norm(current_pos - self.goal_position)\n\n        # Reward for getting closer to goal\n        reward = -distance_to_goal * 0.1\n\n        # Bonus for reaching goal\n        if distance_to_goal < 0.5:\n            reward += 100\n\n        # Penalty for collision\n        if self.collision:\n            reward -= 100\n\n        # Small penalty for each step to encourage efficiency\n        reward -= 0.1\n\n        return reward\n\n    def reset(self):\n        """Reset the environment"""\n        self.current_step = 0\n\n        # Reset robot position (in simulation this would reset the robot)\n        # For this example, we\'ll just reset our tracking variables\n        self.laser_data = np.zeros(20)\n        self.pose = np.zeros(3)\n        self.velocity = 0.0\n        self.collision = False\n\n        # In a real setup, you would reset the robot in the simulator\n        # For now, return a random initial observation\n        obs = np.concatenate([\n            np.random.uniform(1, 10, 20),  # Simulated laser data\n            np.random.uniform(-1, 1, 3),   # Simulated pose\n            [0.0]                          # Simulated velocity\n        ]).astype(np.float32)\n\n        return obs\n\n    def close(self):\n        """Clean up resources"""\n        self.node.destroy_node()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Train Navigation Policy"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# train_navigation_policy.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import deque\nimport random\n\nclass DQNetwork(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size=128):\n        super(DQNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, hidden_size)\n        self.fc4 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size, lr=0.001):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=10000)\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = lr\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.q_network = DQNetwork(state_size, action_size).to(self.device)\n        self.target_network = DQNetwork(state_size, action_size).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n        # Update target network\n        self.update_target_network()\n\n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        if np.random.random() <= self.epsilon:\n            return random.randrange(self.action_size)\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n\n    def replay(self, batch_size=32):\n        if len(self.memory) < batch_size:\n            return\n\n        batch = random.sample(self.memory, batch_size)\n        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)\n        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)\n        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)\n        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)\n        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)\n\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (0.99 * next_q_values * ~dones)\n\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def load(self, name):\n        self.q_network.load_state_dict(torch.load(name))\n\n    def save(self, name):\n        torch.save(self.q_network.state_dict(), name)\n\ndef train_agent():\n    env = RobotRLEnvironment()\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.n\n    agent = DQNAgent(state_size, action_size)\n\n    episodes = 1000\n    batch_size = 32\n    scores = deque(maxlen=100)\n\n    for e in range(episodes):\n        state = env.reset()\n        total_reward = 0\n\n        for time in range(1000):\n            action = agent.act(state)\n            next_state, reward, done, _ = env.step(action)\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n\n            if done:\n                break\n\n            if len(agent.memory) > batch_size:\n                agent.replay(batch_size)\n\n        scores.append(total_reward)\n        avg_score = np.mean(scores)\n\n        print(f"Episode: {e+1}/{episodes}, Score: {total_reward}, Average Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.2f}")\n\n        # Update target network every 100 episodes\n        if e % 100 == 0:\n            agent.update_target_network()\n\n    # Save the trained model\n    agent.save("navigation_policy.pth")\n    print("Model saved as navigation_policy.pth")\n\nif __name__ == "__main__":\n    train_agent()\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-3-transfer-learned-behaviors-to-real-hardware",children:"Step 3: Transfer Learned Behaviors to Real Hardware"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Prepare for Real Robot Deployment"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# real_robot_interface.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image\nfrom geometry_msgs.msg import Twist, Pose\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import String\nimport torch\nimport numpy as np\nfrom cv_bridge import CvBridge\n\nclass RealRobotInterface(Node):\n    def __init__(self):\n        super().__init__(\'real_robot_interface\')\n        self.bridge = CvBridge()\n\n        # Load trained policy\n        self.policy_network = self.load_policy_network()\n\n        # Robot interface\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.laser_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10)\n\n        # State variables\n        self.laser_data = np.zeros(20)\n        self.pose = np.zeros(3)\n        self.velocity = 0.0\n        self.is_running = False\n\n        # Parameters\n        self.action_frequency = 10  # Hz\n        self.timer = self.create_timer(1.0/self.action_frequency, self.control_loop)\n\n        # Goal position\n        self.goal_position = np.array([5.0, 5.0])\n\n        self.get_logger().info("Real Robot Interface initialized")\n\n    def load_policy_network(self):\n        """Load the trained policy network"""\n        # Define the same network architecture as during training\n        class PolicyNetwork(torch.nn.Module):\n            def __init__(self, input_size, output_size, hidden_size=128):\n                super(PolicyNetwork, self).__init__()\n                self.fc1 = torch.nn.Linear(input_size, hidden_size)\n                self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n                self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n                self.fc4 = torch.nn.Linear(hidden_size, output_size)\n                self.relu = torch.nn.ReLU()\n\n            def forward(self, x):\n                x = self.relu(self.fc1(x))\n                x = self.relu(self.fc2(x))\n                x = self.relu(self.fc3(x))\n                x = self.fc4(x)\n                return x\n\n        policy = PolicyNetwork(input_size=24, output_size=4)\n\n        try:\n            policy.load_state_dict(torch.load("navigation_policy.pth", map_location=torch.device(\'cpu\')))\n            policy.eval()\n            self.get_logger().info("Policy model loaded successfully")\n            return policy\n        except Exception as e:\n            self.get_logger().error(f"Failed to load policy: {e}")\n            return None\n\n    def laser_callback(self, msg):\n        """Handle laser scan data from real robot"""\n        # Process laser data similar to simulation\n        num_readings = 20\n        step = len(msg.ranges) // num_readings\n        self.laser_data = np.array([msg.ranges[i*step] for i in range(num_readings)])\n        self.laser_data = np.nan_to_num(self.laser_data, nan=10.0, posinf=10.0, neginf=0.0)\n\n    def odom_callback(self, msg):\n        """Handle odometry data from real robot"""\n        self.pose[0] = msg.pose.pose.position.x\n        self.pose[1] = msg.pose.pose.position.y\n\n        # Convert quaternion to euler\n        orientation = msg.pose.pose.orientation\n        self.pose[2] = self.quaternion_to_euler(orientation)\n\n        self.velocity = np.sqrt(\n            msg.twist.twist.linear.x**2 +\n            msg.twist.twist.linear.y**2 +\n            msg.twist.twist.linear.z**2\n        )\n\n    def quaternion_to_euler(self, quat):\n        """Convert quaternion to euler angle"""\n        siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)\n        cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)\n        return np.arctan2(siny_cosp, cosy_cosp)\n\n    def control_loop(self):\n        """Main control loop - get action from policy and execute"""\n        if not self.is_running or self.policy_network is None:\n            return\n\n        # Prepare observation\n        obs = np.concatenate([\n            self.laser_data,\n            self.pose,\n            [self.velocity]\n        ]).astype(np.float32)\n\n        # Get action from policy\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n            action_probs = self.policy_network(obs_tensor)\n            action = torch.argmax(action_probs, dim=1).item()\n\n        # Convert action to velocity command\n        cmd_vel = Twist()\n        if action == 0:  # Forward\n            cmd_vel.linear.x = 0.3\n            cmd_vel.angular.z = 0.0\n        elif action == 1:  # Backward\n            cmd_vel.linear.x = -0.2\n            cmd_vel.angular.z = 0.0\n        elif action == 2:  # Turn left\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.4\n        elif action == 3:  # Turn right\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = -0.4\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def start_navigation(self):\n        """Start the navigation process"""\n        self.is_running = True\n        self.get_logger().info("Navigation started")\n\n    def stop_navigation(self):\n        """Stop the navigation process"""\n        self.is_running = False\n        # Stop the robot\n        cmd_vel = Twist()\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info("Navigation stopped")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    robot_interface = RealRobotInterface()\n\n    # Start navigation\n    robot_interface.start_navigation()\n\n    try:\n        rclpy.spin(robot_interface)\n    except KeyboardInterrupt:\n        robot_interface.stop_navigation()\n\n    robot_interface.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implement Sim-to-Real Adaptation"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# adaptation_module.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom collections import deque\nimport statistics\n\nclass AdaptationModule:\n    def __init__(self, policy_network, adaptation_rate=0.01):\n        self.policy_network = policy_network\n        self.adaptation_rate = adaptation_rate\n\n        # Track performance metrics\n        self.performance_history = deque(maxlen=100)\n        self.laser_stats = {\n            \'mean\': deque(maxlen=50),\n            \'std\': deque(maxlen=50)\n        }\n\n        # Adaptation parameters\n        self.sim_laser_mean = 5.0  # Average from simulation\n        self.sim_laser_std = 2.0   # Std from simulation\n\n        # Real robot calibration\n        self.real_laser_mean = 5.0\n        self.real_laser_std = 2.0\n\n        # Initialize normalization parameters\n        self.update_calibration()\n\n    def update_calibration(self):\n        """Update calibration based on recent real-world data"""\n        if len(self.laser_stats[\'mean\']) > 10:\n            self.real_laser_mean = statistics.mean(self.laser_stats[\'mean\'])\n            self.real_laser_std = statistics.mean(self.laser_stats[\'std\'])\n\n    def normalize_laser_data(self, laser_data):\n        """Normalize laser data from real robot to match simulation distribution"""\n        # Calculate current real-world statistics\n        current_mean = np.mean(laser_data)\n        current_std = np.std(laser_data)\n\n        # Update statistics\n        self.laser_stats[\'mean\'].append(current_mean)\n        self.laser_stats[\'std\'].append(current_std)\n\n        # Normalize to simulation distribution\n        normalized = (laser_data - current_mean) / (current_std + 1e-8)\n        normalized = normalized * self.sim_laser_std + self.sim_laser_mean\n\n        # Clip to reasonable values\n        normalized = np.clip(normalized, 0.1, 10.0)\n\n        return normalized\n\n    def adapt_policy(self, state, action, reward, next_state):\n        """Adapt policy based on real-world experience"""\n        # Add small adaptation based on performance\n        performance = reward  # Simplified performance metric\n        self.performance_history.append(performance)\n\n        # If performance is consistently low, adjust exploration\n        if len(self.performance_history) >= 10:\n            avg_performance = statistics.mean(list(self.performance_history)[-10:])\n            if avg_performance < -5:  # Threshold for poor performance\n                # Increase exploration temporarily\n                self.adaptation_rate *= 1.1\n            else:\n                # Decrease adaptation rate back to normal\n                self.adaptation_rate = max(self.adaptation_rate * 0.99, 0.01)\n\n    def process_observation(self, obs):\n        """Process observation from real robot before policy evaluation"""\n        # Extract laser data (first 20 elements)\n        laser_data = obs[:20]\n        other_data = obs[20:]\n\n        # Normalize laser data\n        normalized_laser = self.normalize_laser_data(laser_data)\n\n        # Combine normalized data\n        processed_obs = np.concatenate([normalized_laser, other_data])\n\n        return processed_obs\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-4-evaluate-performance-and-adaptation-strategies",children:"Step 4: Evaluate Performance and Adaptation Strategies"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Create Evaluation Framework"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# evaluation_framework.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Path\nfrom std_msgs.msg import Float32\nimport numpy as np\nimport time\nfrom collections import deque\n\nclass EvaluationFramework(Node):\n    def __init__(self):\n        super().__init__(\'evaluation_framework\')\n\n        # Publishers for metrics\n        self.success_rate_pub = self.create_publisher(Float32, \'/eval/success_rate\', 10)\n        self.path_efficiency_pub = self.create_publisher(Float32, \'/eval/path_efficiency\', 10)\n        self.collision_rate_pub = self.create_publisher(Float32, \'/eval/collision_rate\', 10)\n        self.time_to_goal_pub = self.create_publisher(Float32, \'/eval/time_to_goal\', 10)\n\n        # Subscribers\n        self.pose_sub = self.create_subscription(\n            PoseStamped, \'/robot_pose\', self.pose_callback, 10)\n        self.path_sub = self.create_subscription(\n            Path, \'/robot_path\', self.path_callback, 10)\n\n        # Evaluation parameters\n        self.goal_position = np.array([5.0, 5.0])\n        self.start_time = None\n        self.current_pose = np.array([0.0, 0.0])\n        self.path_history = deque(maxlen=1000)\n        self.evaluation_trials = 0\n        self.successful_trials = 0\n        self.collision_trials = 0\n        self.total_path_length = 0.0\n\n        # Evaluation timer\n        self.eval_timer = self.create_timer(1.0, self.evaluate_performance)\n\n    def pose_callback(self, msg):\n        """Update current pose"""\n        self.current_pose = np.array([msg.pose.position.x, msg.pose.position.y])\n\n        # Record path if we have a start time\n        if self.start_time is not None:\n            self.path_history.append(self.current_pose.copy())\n\n    def path_callback(self, msg):\n        """Record path data"""\n        pass  # Path is recorded via pose updates\n\n    def start_evaluation_trial(self):\n        """Start a new evaluation trial"""\n        self.start_time = time.time()\n        self.path_history.clear()\n        self.current_pose = np.array([0.0, 0.0])  # Reset to start position\n        self.evaluation_trials += 1\n        self.get_logger().info(f"Started evaluation trial {self.evaluation_trials}")\n\n    def end_evaluation_trial(self, success, collision=False):\n        """End current evaluation trial"""\n        if self.start_time is None:\n            return\n\n        trial_time = time.time() - self.start_time\n        self.start_time = None\n\n        if success:\n            self.successful_trials += 1\n            self.total_path_length += self.calculate_path_length()\n\n            # Calculate metrics\n            success_rate = self.successful_trials / self.evaluation_trials\n            path_efficiency = self.calculate_path_efficiency()\n\n            # Publish metrics\n            self.publish_metrics(success_rate, path_efficiency, trial_time, collision)\n\n            self.get_logger().info(f"Trial completed - Success: {success}, Time: {trial_time:.2f}s")\n        else:\n            if collision:\n                self.collision_trials += 1\n            self.get_logger().info(f"Trial failed - Success: {success}")\n\n    def calculate_path_length(self):\n        """Calculate total path length"""\n        if len(self.path_history) < 2:\n            return 0.0\n\n        total_length = 0.0\n        for i in range(1, len(self.path_history)):\n            dist = np.linalg.norm(self.path_history[i] - self.path_history[i-1])\n            total_length += dist\n\n        return total_length\n\n    def calculate_path_efficiency(self):\n        """Calculate path efficiency as optimal path length / actual path length"""\n        if len(self.path_history) == 0:\n            return 0.0\n\n        actual_path_length = self.calculate_path_length()\n        optimal_distance = np.linalg.norm(self.goal_position - np.array([0.0, 0.0]))\n\n        if actual_path_length == 0:\n            return 0.0\n\n        efficiency = optimal_distance / actual_path_length\n        return min(efficiency, 1.0)  # Cap at 1.0\n\n    def evaluate_performance(self):\n        """Periodically evaluate performance"""\n        if self.start_time is not None:\n            # Check if reached goal\n            distance_to_goal = np.linalg.norm(self.current_pose - self.goal_position)\n            if distance_to_goal < 0.5:  # Within goal tolerance\n                self.end_evaluation_trial(success=True)\n\n            # Check if timed out\n            elif time.time() - self.start_time > 120:  # 2 minutes timeout\n                self.end_evaluation_trial(success=False)\n\n    def publish_metrics(self, success_rate, path_efficiency, time_to_goal, collision):\n        """Publish evaluation metrics"""\n        success_msg = Float32()\n        success_msg.data = success_rate\n        self.success_rate_pub.publish(success_msg)\n\n        efficiency_msg = Float32()\n        efficiency_msg.data = path_efficiency\n        self.path_efficiency_pub.publish(efficiency_msg)\n\n        collision_msg = Float32()\n        collision_msg.data = 1.0 if collision else 0.0\n        self.collision_rate_pub.publish(collision_msg)\n\n        time_msg = Float32()\n        time_msg.data = time_to_goal\n        self.time_to_goal_pub.publish(time_msg)\n\n    def get_current_metrics(self):\n        """Get current evaluation metrics"""\n        success_rate = self.successful_trials / max(self.evaluation_trials, 1)\n        collision_rate = self.collision_trials / max(self.evaluation_trials, 1)\n\n        return {\n            \'success_rate\': success_rate,\n            \'collision_rate\': collision_rate,\n            \'trials_completed\': self.evaluation_trials,\n            \'successful_trials\': self.successful_trials\n        }\n\ndef run_evaluation():\n    """Run evaluation for specified number of trials"""\n    rclpy.init()\n    evaluator = EvaluationFramework()\n\n    # Run multiple trials\n    num_trials = 20\n    for trial in range(num_trials):\n        evaluator.get_logger().info(f"Starting trial {trial + 1}/{num_trials}")\n        evaluator.start_evaluation_trial()\n\n        # Wait for trial to complete (in practice, this would be event-driven)\n        import time\n        time.sleep(120)  # Max time per trial\n\n        # Force end if still running\n        if evaluator.start_time is not None:\n            evaluator.end_evaluation_trial(success=False)\n\n    # Print final metrics\n    metrics = evaluator.get_current_metrics()\n    evaluator.get_logger().info(f"Final metrics: {metrics}")\n\n    evaluator.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    run_evaluation()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implement Performance Comparison Tools"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# performance_comparison.py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nclass PerformanceComparison:\n    def __init__(self):\n        self.simulation_data = []\n        self.real_world_data = []\n        self.metrics = ['success_rate', 'path_efficiency', 'collision_rate', 'time_to_goal']\n\n    def add_simulation_data(self, data_point):\n        \"\"\"Add simulation performance data\"\"\"\n        self.simulation_data.append(data_point)\n\n    def add_real_world_data(self, data_point):\n        \"\"\"Add real world performance data\"\"\"\n        self.real_world_data.append(data_point)\n\n    def plot_comparison(self):\n        \"\"\"Plot comparison between simulation and real world\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        fig.suptitle('Sim-to-Real Performance Comparison', fontsize=16)\n\n        for i, metric in enumerate(self.metrics):\n            ax = axes[i//2, i%2]\n\n            # Extract data for the metric\n            sim_values = [d[metric] for d in self.simulation_data if metric in d]\n            real_values = [d[metric] for d in self.real_world_data if metric in d]\n\n            # Create comparison plot\n            x = np.arange(len(sim_values))\n            width = 0.35\n\n            ax.bar(x - width/2, sim_values, width, label='Simulation', alpha=0.8)\n            ax.bar(x + width/2, real_values, width, label='Real World', alpha=0.8)\n\n            ax.set_xlabel('Trial')\n            ax.set_ylabel(metric.replace('_', ' ').title())\n            ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.savefig('sim_to_real_comparison.png', dpi=300, bbox_inches='tight')\n        plt.show()\n\n    def calculate_gap_metrics(self):\n        \"\"\"Calculate sim-to-real gap metrics\"\"\"\n        gaps = {}\n\n        for metric in self.metrics:\n            sim_values = [d[metric] for d in self.simulation_data if metric in d]\n            real_values = [d[metric] for d in self.real_world_data if metric in d]\n\n            if sim_values and real_values:\n                sim_mean = np.mean(sim_values)\n                real_mean = np.mean(real_values)\n                gap = abs(sim_mean - real_mean)\n                relative_gap = gap / max(abs(sim_mean), abs(real_mean), 1e-8)\n\n                gaps[metric] = {\n                    'absolute_gap': gap,\n                    'relative_gap': relative_gap,\n                    'sim_mean': sim_mean,\n                    'real_mean': real_mean\n                }\n\n        return gaps\n\n    def print_gap_analysis(self):\n        \"\"\"Print detailed gap analysis\"\"\"\n        gaps = self.calculate_gap_metrics()\n\n        print(\"Sim-to-Real Gap Analysis:\")\n        print(\"=\" * 50)\n\n        for metric, values in gaps.items():\n            print(f\"\\n{metric.replace('_', ' ').title()}:\")\n            print(f\"  Simulation Mean: {values['sim_mean']:.3f}\")\n            print(f\"  Real World Mean: {values['real_mean']:.3f}\")\n            print(f\"  Absolute Gap: {values['absolute_gap']:.3f}\")\n            print(f\"  Relative Gap: {values['relative_gap']:.1%}\")\n\n            if values['relative_gap'] > 0.2:\n                print(f\"  \u26a0\ufe0f  Significant gap detected - may require domain randomization adjustment\")\n            else:\n                print(f\"  \u2705 Gap within acceptable range\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create sample data\n    comparison = PerformanceComparison()\n\n    # Add sample simulation data\n    for i in range(20):\n        comparison.add_simulation_data({\n            'success_rate': np.random.normal(0.9, 0.05),\n            'path_efficiency': np.random.normal(0.8, 0.1),\n            'collision_rate': np.random.normal(0.05, 0.02),\n            'time_to_goal': np.random.normal(30, 5)\n        })\n\n    # Add sample real world data\n    for i in range(20):\n        comparison.add_real_world_data({\n            'success_rate': np.random.normal(0.7, 0.1),\n            'path_efficiency': np.random.normal(0.6, 0.15),\n            'collision_rate': np.random.normal(0.15, 0.05),\n            'time_to_goal': np.random.normal(45, 10)\n        })\n\n    # Plot comparison\n    comparison.plot_comparison()\n\n    # Print gap analysis\n    comparison.print_gap_analysis()\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,s.jsx)(n.p,{children:"Upon completion of this lab, you should have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Successfully applied domain randomization techniques to improve sim-to-real transfer"}),"\n",(0,s.jsx)(n.li,{children:"Trained robot behaviors in simulation that can operate effectively in the real world"}),"\n",(0,s.jsx)(n.li,{children:"Implemented a complete pipeline for transferring learned behaviors to physical hardware"}),"\n",(0,s.jsx)(n.li,{children:"Evaluated performance metrics comparing simulation and real-world results"}),"\n",(0,s.jsx)(n.li,{children:"Developed adaptation strategies to handle sim-to-real discrepancies"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The system should demonstrate:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Successful navigation behaviors transferred from simulation to real robot"}),"\n",(0,s.jsx)(n.li,{children:"Quantified performance gap between simulation and reality"}),"\n",(0,s.jsx)(n.li,{children:"Adaptation mechanisms that improve real-world performance"}),"\n",(0,s.jsx)(n.li,{children:"Evaluation framework for ongoing performance assessment"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Poor real-world performance"}),": Increase domain randomization range and retrain"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor data mismatch"}),": Implement better sensor calibration and normalization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Control instability"}),": Reduce control gains and add filtering to sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety concerns"}),": Implement safety boundaries and emergency stops"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"optional-extension-tasks",children:"Optional Extension Tasks"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Online Adaptation"}),": Implement continuous adaptation during real-world operation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-Task Transfer"}),": Transfer multiple behaviors simultaneously (navigation, manipulation, etc.)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cross-Robot Transfer"}),": Transfer behaviors between different robot platforms."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Systematic Evaluation"}),": Create a comprehensive evaluation suite with various environmental conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This lab completed the full sim-to-real transfer pipeline, demonstrating how to apply domain randomization, train behaviors in simulation, transfer them to real hardware, and evaluate performance. You've learned to quantify and address the sim-to-real gap, implement adaptation strategies, and create evaluation frameworks for ongoing performance assessment. These skills are essential for deploying simulation-trained robotic systems in real-world applications."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,a){a.d(n,{R:()=>r,x:()=>o});var t=a(6540);const s={},i=t.createContext(s);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);