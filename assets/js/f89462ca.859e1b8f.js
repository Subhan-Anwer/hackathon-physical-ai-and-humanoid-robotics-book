"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[406],{8129(n,e,a){a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>_});const o=JSON.parse('{"id":"module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration","title":"Lab 3: Navigation and VSLAM Integration","description":"Objective","source":"@site/docs/module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration.md","sourceDirName":"module-3-ai-robot-brain/labs","slug":"/module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Lab 3: Navigation and VSLAM Integration","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Lab 2: AI-Based Perception Pipeline","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline"},"next":{"title":"Lab 4: Sim-to-Real Transfer","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer"}}');var t=a(4848),i=a(8453);const s={title:"Lab 3: Navigation and VSLAM Integration",sidebar_position:3},r="Lab 3: Navigation and VSLAM Integration",l={},_=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Configure Isaac ROS Navigation Stack with Nav2",id:"step-1-configure-isaac-ros-navigation-stack-with-nav2",level:3},{value:"Step 2: Implement VSLAM for Robot Localization",id:"step-2-implement-vslam-for-robot-localization",level:3},{value:"Step 3: Create Autonomous Navigation Behaviors",id:"step-3-create-autonomous-navigation-behaviors",level:3},{value:"Step 4: Test Navigation in Complex Environments",id:"step-4-test-navigation-in-complex-environments",level:3},{value:"Expected Outcome",id:"expected-outcome",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Optional Extension Tasks",id:"optional-extension-tasks",level:2},{value:"Summary",id:"summary",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"lab-3-navigation-and-vslam-integration",children:"Lab 3: Navigation and VSLAM Integration"})}),"\n",(0,t.jsx)(e.h2,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(e.p,{children:"In this lab, you will configure the Isaac ROS navigation stack with Navigation2, implement VSLAM for robot localization, create autonomous navigation behaviors, and test navigation in complex environments. This lab demonstrates the integration of visual SLAM with navigation systems to enable robust localization and path planning in unknown environments."}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Completed Lab 3.1: Isaac Sim Environment Setup"}),"\n",(0,t.jsx)(e.li,{children:"Completed Lab 3.2: AI-Based Perception Pipeline"}),"\n",(0,t.jsx)(e.li,{children:"Isaac ROS navigation packages installed"}),"\n",(0,t.jsx)(e.li,{children:"Understanding of ROS 2 navigation concepts"}),"\n",(0,t.jsx)(e.li,{children:"Basic knowledge of SLAM algorithms"}),"\n",(0,t.jsx)(e.li,{children:"NVIDIA Isaac Sim with robot model and sensors configured"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,t.jsx)(e.h3,{id:"step-1-configure-isaac-ros-navigation-stack-with-nav2",children:"Step 1: Configure Isaac ROS Navigation Stack with Nav2"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Install Navigation2 and Isaac ROS Navigation Packages"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"sudo apt update\nsudo apt install ros-humble-navigation2\nsudo apt install ros-humble-nav2-bringup\nsudo apt install ros-humble-isaac-ros-nav2-bridge\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Create Navigation Configuration Files"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:'# nav2_params.yaml\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_link"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.5\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n    scan_topic: scan\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    default_nav_to_pose_bt_xml: ""\n    default_nav_through_poses_bt_xml: ""\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_compute_path_through_poses_action_bt_node\n    - nav2_smooth_path_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n    - nav2_assisted_teleop_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_drive_on_heading_bt_node\n    - nav2_clear_costmap_service_bt_node\n    - nav2_is_stuck_condition_bt_node\n    - nav2_goal_reached_condition_bt_node\n    - nav2_goal_updated_condition_bt_node\n    - nav2_globally_updated_goal_condition_bt_node\n    - nav2_is_path_valid_condition_bt_node\n    - nav2_initial_pose_received_condition_bt_node\n    - nav2_reinitialize_global_localization_service_bt_node\n    - nav2_rate_controller_bt_node\n    - nav2_distance_controller_bt_node\n    - nav2_speed_controller_bt_node\n    - nav2_truncate_path_action_bt_node\n    - nav2_truncate_path_local_action_bt_node\n    - nav2_goal_updater_node_bt_node\n    - nav2_recovery_node_bt_node\n    - nav2_pipeline_sequence_bt_node\n    - nav2_round_robin_node_bt_node\n    - nav2_transform_available_condition_bt_node\n    - nav2_time_expired_condition_bt_node\n    - nav2_path_expiring_timer_condition\n    - nav2_distance_traveled_condition_bt_node\n    - nav2_single_trigger_bt_node\n    - nav2_is_battery_low_condition_bt_node\n    - nav2_navigate_through_poses_action_bt_node\n    - nav2_navigate_to_pose_action_bt_node\n    - nav2_remove_passed_goals_action_bt_node\n    - nav2_planner_selector_bt_node\n    - nav2_controller_selector_bt_node\n    - nav2_goal_checker_selector_bt_node\n    - nav2_controller_cancel_bt_node\n    - nav2_path_longer_on_approach_bt_node\n    - nav2_wait_cancel_bt_node\n    - nav2_spin_cancel_bt_node\n    - nav2_back_up_cancel_bt_node\n    - nav2_assisted_teleop_cancel_bt_node\n    - nav2_drive_on_heading_cancel_bt_node\n'})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Set Up Costmap Configuration"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:"# costmap_common_params.yaml\nobstacle_range: 3.0\nraytrace_range: 3.5\nfootprint: [[-0.325, -0.325], [-0.325, 0.325], [0.325, 0.325], [0.325, -0.325]]\ninflation_radius: 0.55\ncost_scaling_factor: 3.0\nmap_type: costmap\nobstacle_layer:\n  enabled: true\n  obstacle_range: 3.0\n  raytrace_range: 3.5\n  observation_sources: scan\n  scan:\n    topic: /scan\n    max_obstacle_height: 2.0\n    clearing: true\n    marking: true\n    data_type: LaserScan\nvoxel_layer:\n  enabled: true\n  publish_voxel_map: false\n  origin_z: 0.0\n  z_resolution: 0.2\n  z_voxels: 10\n  max_obstacle_height: 2.0\n  mark_threshold: 0\n  observation_sources: pointcloud\n  pointcloud:\n    topic: /points\n    max_obstacle_height: 2.0\n    min_obstacle_height: 0.0\n    clearing: true\n    marking: true\n    data_type: PointCloud2\ninflation_layer:\n  enabled: true\n  cost_scaling_factor: 3.0\n  inflation_radius: 0.55\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"step-2-implement-vslam-for-robot-localization",children:"Step 2: Implement VSLAM for Robot Localization"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Install Isaac ROS VSLAM Packages"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"sudo apt install ros-humble-isaac-ros-visual-slam\nsudo apt install ros-humble-isaac-ros-isaac-sim-bridge\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Create VSLAM Configuration"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:'# vslam_params.yaml\nvisual_slam_node:\n  ros__parameters:\n    use_sim_time: True\n    enable_imu: true\n    enable_rectification: true\n    rectified_images: false\n    map_frame: "map"\n    odom_frame: "odom"\n    base_frame: "base_link"\n    publish_odom_tf: true\n    publish_map_tf: true\n    mode: "localization"\n    max_num_features: 1000\n    initial_map_size: 100\n    min_num_images_to_match: 3\n    min_tracked_features_ratio: 0.5\n    min_matches_to_track: 10\n    max_reproj_error: 3.0\n    max_pose_covariance: 0.1\n    max_linear_velocity: 1.0\n    max_angular_velocity: 1.0\n'})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Create VSLAM Launch File"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# vslam_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Path to parameter files\n    config_visual_slam = os.path.join(\n        get_package_share_directory('your_package'),\n        'config',\n        'vslam_params.yaml'\n    )\n\n    # Visual SLAM node\n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='visual_slam_node',\n        parameters=[config_visual_slam],\n        remappings=[\n            ('/visual_slam/image', '/camera/image_rect'),\n            ('/visual_slam/camera_info', '/camera/camera_info'),\n            ('/visual_slam/imu', '/imu/data')\n        ]\n    )\n\n    return LaunchDescription([\n        visual_slam_node\n    ])\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Implement Visual-Inertial Odometry (VIO)"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# vio_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped, TransformStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport open3d as o3d\n\nclass VisualInertialOdometryNode(Node):\n    def __init__(self):\n        super().__init__('vio_node')\n        self.bridge = CvBridge()\n\n        # Subscribe to camera and IMU data\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n\n        # Publisher for odometry\n        self.odom_pub = self.create_publisher(\n            Odometry, '/visual_odom', 10)\n\n        # Initialize VIO parameters\n        self.prev_image = None\n        self.current_pose = np.eye(4)\n        self.feature_detector = cv2.ORB_create(nfeatures=1000)\n        self.feature_matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n\n        # IMU integration\n        self.imu_orientation = np.array([0, 0, 0, 1])  # quaternion\n        self.imu_angular_velocity = np.zeros(3)\n        self.imu_linear_acceleration = np.zeros(3)\n\n    def image_callback(self, msg):\n        current_image = self.bridge.imgmsg_to_cv2(msg, \"mono8\")\n\n        if self.prev_image is not None:\n            # Extract and match features\n            matches = self.match_features(self.prev_image, current_image)\n\n            if len(matches) >= 10:\n                # Estimate motion using matched features\n                rotation, translation = self.estimate_motion(matches)\n\n                # Integrate IMU data for orientation\n                orientation = self.integrate_imu_orientation()\n\n                # Update pose\n                self.update_pose(rotation, translation, orientation)\n\n                # Publish odometry\n                self.publish_odometry(msg.header.stamp)\n\n        self.prev_image = current_image\n\n    def match_features(self, prev_img, curr_img):\n        # Detect features\n        prev_kp = self.feature_detector.detect(prev_img, None)\n        curr_kp = self.feature_detector.detect(curr_img, None)\n\n        # Compute descriptors\n        prev_kp, prev_desc = self.feature_detector.compute(prev_img, prev_kp)\n        curr_kp, curr_desc = self.feature_detector.compute(curr_img, curr_kp)\n\n        # Match features\n        matches = self.feature_matcher.match(prev_desc, curr_desc)\n        matches = sorted(matches, key=lambda x: x.distance)\n\n        return matches[:50]  # Return top 50 matches\n\n    def estimate_motion(self, matches):\n        # Extract matched points\n        prev_points = np.float32([prev_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        curr_points = np.float32([curr_kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        # Estimate essential matrix\n        E, mask = cv2.findEssentialMat(prev_points, curr_points,\n                                      cameraMatrix=self.camera_matrix,\n                                      method=cv2.RANSAC,\n                                      threshold=1.0)\n\n        # Recover pose\n        _, rotation, translation, _ = cv2.recoverPose(E, prev_points, curr_points,\n                                                     cameraMatrix=self.camera_matrix)\n\n        return rotation, translation\n\n    def integrate_imu_orientation(self):\n        # Integrate angular velocity to get orientation\n        # This is a simplified approach - in practice, you'd use a proper IMU integration method\n        dt = 0.01  # Assuming 100Hz IMU rate\n        angular_vel = self.imu_angular_velocity\n\n        # Convert to axis-angle representation\n        angle = np.linalg.norm(angular_vel) * dt\n        if angle > 0:\n            axis = angular_vel / np.linalg.norm(angular_vel)\n            # Convert to quaternion\n            w = np.cos(angle/2)\n            x = axis[0] * np.sin(angle/2)\n            y = axis[1] * np.sin(angle/2)\n            z = axis[2] * np.sin(angle/2)\n\n            # Update orientation\n            q_imu = np.array([w, x, y, z])\n            self.imu_orientation = self.quaternion_multiply(self.imu_orientation, q_imu)\n\n        return self.imu_orientation\n\n    def update_pose(self, rotation, translation, orientation):\n        # Create transformation matrix from rotation and translation\n        T = np.eye(4)\n        T[:3, :3] = rotation\n        T[:3, 3] = translation.flatten()\n\n        # Update current pose\n        self.current_pose = self.current_pose @ T\n\n    def publish_odometry(self, stamp):\n        odom_msg = Odometry()\n        odom_msg.header.stamp = stamp\n        odom_msg.header.frame_id = 'odom'\n        odom_msg.child_frame_id = 'base_link'\n\n        # Set position\n        odom_msg.pose.pose.position.x = self.current_pose[0, 3]\n        odom_msg.pose.pose.position.y = self.current_pose[1, 3]\n        odom_msg.pose.pose.position.z = self.current_pose[2, 3]\n\n        # Set orientation from IMU\n        odom_msg.pose.pose.orientation.w = self.imu_orientation[0]\n        odom_msg.pose.pose.orientation.x = self.imu_orientation[1]\n        odom_msg.pose.pose.orientation.y = self.imu_orientation[2]\n        odom_msg.pose.pose.orientation.z = self.imu_orientation[3]\n\n        # Publish odometry\n        self.odom_pub.publish(odom_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisualInertialOdometryNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"step-3-create-autonomous-navigation-behaviors",children:"Step 3: Create Autonomous Navigation Behaviors"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Implement Path Planning Node"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# path_planning_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, PoseWithCovarianceStamped\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom visualization_msgs.msg import MarkerArray\nimport numpy as np\nimport heapq\n\nclass PathPlanningNode(Node):\n    def __init__(self):\n        super().__init__('path_planning_node')\n\n        # Subscribe to map and current pose\n        self.map_sub = self.create_subscription(\n            OccupancyGrid, '/map', self.map_callback, 10)\n        self.pose_sub = self.create_subscription(\n            PoseWithCovarianceStamped, '/initialpose', self.pose_callback, 10)\n\n        # Publisher for path\n        self.path_pub = self.create_publisher(\n            Path, '/global_plan', 10)\n\n        # Service for navigation goals\n        self.nav_goal_service = self.create_service(\n            PoseStamped, '/set_nav_goal', self.nav_goal_callback)\n\n        self.map_data = None\n        self.current_pose = None\n\n    def map_callback(self, msg):\n        self.map_data = msg\n        self.map_width = msg.info.width\n        self.map_height = msg.info.height\n        self.map_resolution = msg.info.resolution\n        self.map_origin = msg.info.origin\n\n    def pose_callback(self, msg):\n        self.current_pose = msg.pose.pose\n\n    def nav_goal_callback(self, request, response):\n        if self.map_data is None or self.current_pose is None:\n            self.get_logger().warn(\"Map or pose not available\")\n            return response\n\n        # Convert goal to map coordinates\n        goal_map_x = int((request.pose.position.x - self.map_origin.position.x) / self.map_resolution)\n        goal_map_y = int((request.pose.position.y - self.map_origin.position.y) / self.map_resolution)\n\n        # Convert current pose to map coordinates\n        start_map_x = int((self.current_pose.position.x - self.map_origin.position.x) / self.map_resolution)\n        start_map_y = int((self.current_pose.position.y - self.map_origin.position.y) / self.map_resolution)\n\n        # Plan path using A* algorithm\n        path = self.a_star_plan(start_map_x, start_map_y, goal_map_x, goal_map_y)\n\n        if path:\n            # Convert path back to world coordinates\n            world_path = self.convert_path_to_world(path)\n            self.publish_path(world_path)\n        else:\n            self.get_logger().warn(\"No path found\")\n\n        return response\n\n    def a_star_plan(self, start_x, start_y, goal_x, goal_y):\n        # Implement A* path planning algorithm\n        def heuristic(a, b):\n            return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n        def get_neighbors(x, y):\n            neighbors = []\n            for dx, dy in [(0, 1), (1, 0), (0, -1), (-1, 0), (1, 1), (1, -1), (-1, 1), (-1, -1)]:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < self.map_width and 0 <= ny < self.map_height:\n                    # Check if cell is free (value < 50 means it's not occupied)\n                    cell_idx = ny * self.map_width + nx\n                    if self.map_data.data[cell_idx] < 50:\n                        neighbors.append((nx, ny))\n            return neighbors\n\n        open_set = [(0, (start_x, start_y))]\n        came_from = {}\n        g_score = {(start_x, start_y): 0}\n        f_score = {(start_x, start_y): heuristic((start_x, start_y), (goal_x, goal_y))}\n\n        while open_set:\n            current = heapq.heappop(open_set)[1]\n\n            if current == (goal_x, goal_y):\n                # Reconstruct path\n                path = []\n                while current in came_from:\n                    path.append(current)\n                    current = came_from[current]\n                path.append((start_x, start_y))\n                return path[::-1]\n\n            for neighbor in get_neighbors(*current):\n                tentative_g_score = g_score[current] + heuristic(current, neighbor)\n\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score[neighbor] = tentative_g_score + heuristic(neighbor, (goal_x, goal_y))\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n\n        return None  # No path found\n\n    def convert_path_to_world(self, path):\n        world_path = Path()\n        world_path.header.frame_id = 'map'\n\n        for map_x, map_y in path:\n            pose = PoseStamped()\n            pose.header.frame_id = 'map'\n            pose.pose.position.x = map_x * self.map_resolution + self.map_origin.position.x\n            pose.pose.position.y = map_y * self.map_resolution + self.map_origin.position.y\n            pose.pose.position.z = 0.0\n            pose.pose.orientation.w = 1.0\n\n            world_path.poses.append(pose)\n\n        return world_path\n\n    def publish_path(self, path):\n        self.path_pub.publish(path)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PathPlanningNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Create Navigation Behavior Tree"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'\x3c!-- navigation_behavior_tree.xml --\x3e\n<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <PipelineSequence name="NavigateWithRecovery">\n            <RecoveryNode number_of_retries="6" name="NavigateRecovery">\n                <PipelineSequence name="Navigate">\n                    <ComputePathToPose goal="{goal}" path="{path}" planner_id="GridBased"/>\n                    <SmoothPath input_path="{path}" output_path="{smoothed_path}" smoother_id="SimpleSmoother"/>\n                    <FollowPath path="{smoothed_path}" controller_id="FollowPath"/>\n                </PipelineSequence>\n                <ReactiveFallback name="RecoveryFallback">\n                    <GoalUpdated/>\n                    <ClearEntireCostmap name="ClearLocalCostmap" service_name="local_costmap/clear_entirely_local_costmap"/>\n                    <ClearEntireCostmap name="ClearGlobalCostmap" service_name="global_costmap/clear_entirely_global_costmap"/>\n                    <Spin duration="2.0"/>\n                </ReactiveFallback>\n            </RecoveryNode>\n        </PipelineSequence>\n    </BehaviorTree>\n</root>\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"step-4-test-navigation-in-complex-environments",children:"Step 4: Test Navigation in Complex Environments"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Create Complex Simulation Environment"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# complex_environment.py\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_prim\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.carb import set_carb_setting\nimport numpy as np\n\n# Create a complex indoor environment\ndef create_complex_environment():\n    # Create walls\n    create_prim("/World/Wall1", "Cuboid",\n                position=[0, 5, 1],\n                size=[10, 0.2, 2])\n    create_prim("/World/Wall2", "Cuboid",\n                position=[5, 0, 1],\n                size=[0.2, 10, 2])\n    create_prim("/World/Wall3", "Cuboid",\n                position=[0, -5, 1],\n                size=[10, 0.2, 2])\n    create_prim("/World/Wall4", "Cuboid",\n                position=[-5, 0, 1],\n                size=[0.2, 10, 2])\n\n    # Create obstacles\n    for i in range(5):\n        for j in range(5):\n            if (i + j) % 2 == 0:  # Create obstacles in checkerboard pattern\n                create_prim(f"/World/Obstacle_{i}_{j}", "Cylinder",\n                            position=[i-2, j-2, 0.5],\n                            radius=0.3, height=1.0)\n\n    # Create furniture\n    create_prim("/World/Table1", "Cuboid",\n                position=[3, 2, 0.4],\n                size=[1.5, 0.8, 0.8])\n    create_prim("/World/Chair1", "Cylinder",\n                position=[3.8, 2.5, 0.25],\n                radius=0.2, height=0.5)\n\n# Run this in Isaac Sim to create the environment\ncreate_complex_environment()\n'})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Implement Navigation Testing Script"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# navigation_tester.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport time\nimport math\n\nclass NavigationTester(Node):\n    def __init__(self):\n        super().__init__('navigation_tester')\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        # Define test waypoints\n        self.waypoints = [\n            (2.0, 0.0, 0.0),    # Waypoint 1\n            (0.0, 2.0, 1.57),   # Waypoint 2\n            (-2.0, 0.0, 3.14),  # Waypoint 3\n            (0.0, -2.0, -1.57), # Waypoint 4\n        ]\n\n        # Start testing\n        self.test_navigation()\n\n    def test_navigation(self):\n        for i, (x, y, theta) in enumerate(self.waypoints):\n            self.get_logger().info(f\"Testing navigation to waypoint {i+1}: ({x}, {y})\")\n\n            # Send navigation goal\n            goal_msg = NavigateToPose.Goal()\n            goal_msg.pose.header.frame_id = 'map'\n            goal_msg.pose.pose.position.x = x\n            goal_msg.pose.pose.position.y = y\n            goal_msg.pose.pose.orientation.z = math.sin(theta / 2.0)\n            goal_msg.pose.pose.orientation.w = math.cos(theta / 2.0)\n\n            # Wait for server\n            self.nav_client.wait_for_server()\n\n            # Send goal\n            future = self.nav_client.send_goal_async(goal_msg)\n            rclpy.spin_until_future_complete(self, future)\n\n            # Wait for result\n            time.sleep(5)  # Allow time for navigation to complete\n\n    def send_goal(self, x, y, theta):\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.pose.position.x = x\n        goal_msg.pose.pose.position.y = y\n        goal_msg.pose.pose.orientation.z = math.sin(theta / 2.0)\n        goal_msg.pose.pose.orientation.w = math.cos(theta / 2.0)\n\n        self.nav_client.wait_for_server()\n        return self.nav_client.send_goal_async(goal_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    tester = NavigationTester()\n    rclpy.spin(tester)\n    tester.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Launch Complete Navigation System"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# navigation_system_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Launch Nav2\n    nav2_bringup_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n            os.path.join(\n                get_package_share_directory('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            )\n        ),\n        launch_arguments={\n            'use_sim_time': 'true',\n            'params_file': os.path.join(\n                get_package_share_directory('your_package'),\n                'config',\n                'nav2_params.yaml'\n            )\n        }.items()\n    )\n\n    # Launch VSLAM\n    vslam_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n            os.path.join(\n                get_package_share_directory('your_package'),\n                'launch',\n                'vslam_launch.py'\n            )\n        )\n    )\n\n    # Launch custom path planning\n    path_planning_node = Node(\n        package='your_package',\n        executable='path_planning_node',\n        name='path_planning_node',\n        parameters=[\n            os.path.join(\n                get_package_share_directory('your_package'),\n                'config',\n                'nav2_params.yaml'\n            )\n        ]\n    )\n\n    return LaunchDescription([\n        nav2_bringup_launch,\n        vslam_launch,\n        path_planning_node\n    ])\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,t.jsx)(e.p,{children:"Upon completion of this lab, you should have:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"A fully configured Isaac ROS navigation stack integrated with Navigation2"}),"\n",(0,t.jsx)(e.li,{children:"Working VSLAM system providing robust robot localization"}),"\n",(0,t.jsx)(e.li,{children:"Autonomous navigation behaviors with obstacle avoidance"}),"\n",(0,t.jsx)(e.li,{children:"Tested navigation performance in complex simulation environments"}),"\n",(0,t.jsx)(e.li,{children:"Understanding of how visual SLAM enhances navigation capabilities"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"The navigation system should be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Localize the robot using visual and inertial data"}),"\n",(0,t.jsx)(e.li,{children:"Plan and execute paths in complex environments"}),"\n",(0,t.jsx)(e.li,{children:"Avoid obstacles and recover from navigation failures"}),"\n",(0,t.jsx)(e.li,{children:"Maintain accurate position estimates during navigation"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"VSLAM not providing accurate localization"}),": Check camera calibration and IMU synchronization"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Navigation failing in complex environments"}),": Adjust costmap parameters and inflation radius"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robot getting stuck frequently"}),": Fine-tune local planner parameters and obstacle detection"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"High computational load"}),": Reduce feature detection parameters or processing frequency"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"optional-extension-tasks",children:"Optional Extension Tasks"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Multi-Goal Navigation"}),": Implement navigation to multiple waypoints in sequence."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Obstacle Avoidance"}),": Add detection and avoidance of moving obstacles in the environment."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Human-Aware Navigation"}),": Implement navigation behaviors that consider human presence and social conventions."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Exploration Behavior"}),": Create autonomous exploration of unknown environments using frontier-based exploration."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This lab demonstrated the integration of visual SLAM with navigation systems to create a robust autonomous navigation solution. You've learned to configure the Isaac ROS navigation stack, implement VSLAM for localization, create autonomous navigation behaviors, and test the system in complex environments. The combination of visual and inertial sensing with Navigation2 provides a powerful foundation for mobile robot navigation in real-world scenarios."})]})}function p(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453(n,e,a){a.d(e,{R:()=>s,x:()=>r});var o=a(6540);const t={},i=o.createContext(t);function s(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);