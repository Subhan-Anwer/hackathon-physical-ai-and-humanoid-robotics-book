"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[84],{5841(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline","title":"Lab 2: AI-Based Perception Pipeline","description":"Objective","source":"@site/docs/module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline.md","sourceDirName":"module-3-ai-robot-brain/labs","slug":"/module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Lab 2: AI-Based Perception Pipeline","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Lab 1: Isaac Sim Environment Setup","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-1-isaac-sim-environment-setup"},"next":{"title":"Lab 3: Navigation and VSLAM Integration","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration"}}');var t=i(4848),o=i(8453);const r={title:"Lab 2: AI-Based Perception Pipeline",sidebar_position:2},a="Lab 2: AI-Based Perception Pipeline",c={},l=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Set Up Isaac ROS Perception Nodes",id:"step-1-set-up-isaac-ros-perception-nodes",level:3},{value:"Step 2: Configure Camera and Sensor Data Pipeline",id:"step-2-configure-camera-and-sensor-data-pipeline",level:3},{value:"Step 3: Implement Object Detection Pipeline",id:"step-3-implement-object-detection-pipeline",level:3},{value:"Step 4: Implement Semantic Segmentation",id:"step-4-implement-semantic-segmentation",level:3},{value:"Step 5: Process Sensor Data Using GPU Acceleration",id:"step-5-process-sensor-data-using-gpu-acceleration",level:3},{value:"Step 6: Validate Perception Accuracy",id:"step-6-validate-perception-accuracy",level:3},{value:"Expected Outcome",id:"expected-outcome",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Optional Extension Tasks",id:"optional-extension-tasks",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lab-2-ai-based-perception-pipeline",children:"Lab 2: AI-Based Perception Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"In this lab, you will set up Isaac ROS perception nodes, implement object detection and semantic segmentation using GPU-accelerated AI models, process sensor data using GPU acceleration, and validate perception accuracy in simulation. This lab demonstrates how to build an end-to-end AI perception pipeline that leverages NVIDIA's hardware acceleration capabilities."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Lab 3.1: Isaac Sim Environment Setup"}),"\n",(0,t.jsx)(n.li,{children:"NVIDIA GPU with CUDA support (RTX 3080 or higher recommended)"}),"\n",(0,t.jsx)(n.li,{children:"Isaac ROS packages installed"}),"\n",(0,t.jsx)(n.li,{children:"Basic knowledge of deep learning frameworks (PyTorch/TensorFlow)"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of computer vision concepts"}),"\n",(0,t.jsx)(n.li,{children:"Docker and NVIDIA Container Toolkit"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-set-up-isaac-ros-perception-nodes",children:"Step 1: Set Up Isaac ROS Perception Nodes"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Install Isaac ROS Perception Packages"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install ros-humble-isaac-ros-perception\nsudo apt install ros-humble-isaac-ros-visual-slam\nsudo apt install ros-humble-isaac-ros-bitmask-publisher\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Verify Installation"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check available Isaac ROS packages\nros2 pkg list | grep isaac_ros\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Launch Perception Stack"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Create a launch file for perception nodes\nmkdir -p ~/isaac_ws/src/perception_launch\ncd ~/isaac_ws/src/perception_launch\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-2-configure-camera-and-sensor-data-pipeline",children:"Step 2: Configure Camera and Sensor Data Pipeline"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Set Up Camera Configuration"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# camera_config.yaml\ncamera:\n  ros__parameters:\n    width: 640\n    height: 480\n    fps: 30\n    fov: 1.047\n    frame_id: "camera_link"\n    rectified: true\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Create Camera Launch File"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# camera_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config = os.path.join(\n        get_package_share_directory('perception_launch'),\n        'config',\n        'camera_config.yaml'\n    )\n\n    camera_node = Node(\n        package='isaac_ros_stereo_image_proc',\n        executable='disparity_node',\n        name='disparity_node',\n        parameters=[config]\n    )\n\n    return LaunchDescription([camera_node])\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Connect Camera to Isaac Sim"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Configure Isaac Sim to publish camera data to ROS topics"}),"\n",(0,t.jsx)(n.li,{children:"Verify camera data is being published at the expected rate"}),"\n",(0,t.jsx)(n.li,{children:"Test camera calibration and rectification"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-3-implement-object-detection-pipeline",children:"Step 3: Implement Object Detection Pipeline"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Set Up TensorRT-based Object Detection"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# object_detection_pipeline.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport tensorrt as trt\nimport pycuda.driver as cuda\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n        self.bridge = CvBridge()\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n\n        # Publisher for detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/detections', 10)\n\n        # Initialize TensorRT engine\n        self.initialize_tensorrt()\n\n    def initialize_tensorrt(self):\n        # Load TensorRT engine for object detection\n        self.trt_logger = trt.Logger(trt.Logger.WARNING)\n        with open(\"yolov8.engine\", \"rb\") as f:\n            self.engine = trt.Runtime(self.trt_logger).deserialize_cuda_engine(f.read())\n        self.context = self.engine.create_execution_context()\n\n    def image_callback(self, msg):\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n        # Run object detection\n        detections = self.run_inference(cv_image)\n\n        # Publish results\n        self.publish_detections(detections)\n\n    def run_inference(self, image):\n        # Preprocess image\n        input_tensor = self.preprocess(image)\n\n        # Run inference on GPU\n        output = self.inference(input_tensor)\n\n        # Postprocess results\n        detections = self.postprocess(output, image.shape)\n\n        return detections\n\n    def preprocess(self, image):\n        # Resize and normalize image for model input\n        resized = cv2.resize(image, (640, 640))\n        normalized = resized.astype(np.float32) / 255.0\n        return np.transpose(normalized, (2, 0, 1))\n\n    def inference(self, input_tensor):\n        # GPU inference implementation\n        inputs, outputs, bindings, stream = self.allocate_buffers()\n        np.copyto(inputs[0].host, input_tensor.ravel())\n\n        # Run inference\n        [output] = self.do_inference_v2(\n            context=self.context,\n            bindings=bindings,\n            inputs=inputs,\n            outputs=outputs,\n            stream=stream\n        )\n        return output\n\n    def postprocess(self, output, image_shape):\n        # Convert raw output to detection format\n        # Implementation depends on specific model output format\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetectionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Configure Object Detection Parameters"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# detection_config.yaml\nobject_detection:\n  ros__parameters:\n    model_path: "/path/to/yolov8.engine"\n    confidence_threshold: 0.5\n    nms_threshold: 0.4\n    input_width: 640\n    input_height: 640\n    max_batch_size: 1\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-4-implement-semantic-segmentation",children:"Step 4: Implement Semantic Segmentation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Set Up Semantic Segmentation Node"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# semantic_segmentation_pipeline.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Int32MultiArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport tensorrt as trt\n\nclass SemanticSegmentationNode(Node):\n    def __init__(self):\n        super().__init__('semantic_segmentation_node')\n        self.bridge = CvBridge()\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n\n        # Publisher for segmentation mask\n        self.mask_pub = self.create_publisher(\n            Int32MultiArray, '/segmentation_mask', 10)\n\n        # Initialize segmentation model\n        self.initialize_segmentation_model()\n\n    def initialize_segmentation_model(self):\n        # Load TensorRT segmentation model\n        self.trt_logger = trt.Logger(trt.Logger.WARNING)\n        with open(\"deeplabv3.engine\", \"rb\") as f:\n            self.engine = trt.Runtime(self.trt_logger).deserialize_cuda_engine(f.read())\n        self.context = self.engine.create_execution_context()\n\n    def image_callback(self, msg):\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n        # Run semantic segmentation\n        segmentation_mask = self.run_segmentation(cv_image)\n\n        # Publish results\n        self.publish_segmentation(segmentation_mask)\n\n    def run_segmentation(self, image):\n        # Preprocess image for segmentation\n        input_tensor = self.preprocess_segmentation(image)\n\n        # Run segmentation inference\n        output = self.segmentation_inference(input_tensor)\n\n        # Convert to class predictions\n        mask = np.argmax(output, axis=0)\n        return mask\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SemanticSegmentationNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Create Segmentation Launch Configuration"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# segmentation_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    segmentation_node = Node(\n        package='your_package',\n        executable='semantic_segmentation_pipeline',\n        name='semantic_segmentation',\n        parameters=[\n            {'model_path': 'deeplabv3.engine'},\n            {'input_width': 513},\n            {'input_height': 513}\n        ]\n    )\n\n    return LaunchDescription([segmentation_node])\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-5-process-sensor-data-using-gpu-acceleration",children:"Step 5: Process Sensor Data Using GPU Acceleration"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Set Up GPU-Accelerated Point Cloud Processing"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# pointcloud_processing.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2, PointField\nfrom std_msgs.msg import Header\nimport numpy as np\nimport open3d as o3d\nfrom numba import cuda\nimport cupy as cp\n\nclass PointCloudProcessingNode(Node):\n    def __init__(self):\n        super().__init__('pointcloud_processing')\n\n        # Subscribe to LIDAR point cloud\n        self.pc_sub = self.create_subscription(\n            PointCloud2, '/scan_cloud', self.pc_callback, 10)\n\n        # Publisher for processed point cloud\n        self.processed_pc_pub = self.create_publisher(\n            PointCloud2, '/processed_scan_cloud', 10)\n\n    @cuda.jit\n    def remove_ground_kernel(self, points, ground_threshold, output):\n        # CUDA kernel for ground plane removal\n        idx = cuda.grid(1)\n        if idx < points.shape[0]:\n            if points[idx, 2] > ground_threshold:\n                output[idx] = 1\n            else:\n                output[idx] = 0\n\n    def pc_callback(self, msg):\n        # Convert ROS PointCloud2 to numpy array\n        points = self.pointcloud2_to_array(msg)\n\n        # Process on GPU\n        gpu_points = cp.asarray(points)\n        ground_removed = self.remove_ground_gpu(gpu_points)\n\n        # Convert back to ROS message\n        processed_msg = self.array_to_pointcloud2(ground_removed, msg.header)\n        self.processed_pc_pub.publish(processed_msg)\n\n    def remove_ground_gpu(self, points):\n        # Use GPU to remove ground plane\n        ground_threshold = -0.1  # Adjust based on robot height\n        mask = cp.zeros(points.shape[0], dtype=cp.int32)\n\n        threads_per_block = 256\n        blocks_per_grid = (points.shape[0] + threads_per_block - 1) // threads_per_block\n\n        self.remove_ground_kernel[blocks_per_grid, threads_per_block](\n            points, ground_threshold, mask)\n\n        return points[mask == 1]\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PointCloudProcessingNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Implement Multi-Sensor Fusion"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# sensor_fusion.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import PoseStamped\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n\n        # Subscribe to multiple sensors\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.pc_sub = self.create_subscription(\n            PointCloud2, '/scan_cloud', self.pc_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n\n        # Publisher for fused data\n        self.fused_data_pub = self.create_publisher(\n            ObjectHypothesisArray, '/fused_objects', 10)\n\n        # Initialize fusion parameters\n        self.camera_intrinsics = np.array([\n            [554.25, 0, 320],\n            [0, 415.69, 240],\n            [0, 0, 1]\n        ])\n        self.extrinsics = np.eye(4)  # Camera to LIDAR transform\n\n    def fuse_camera_lidar(self, image_detections, pointcloud):\n        # Project 3D points to 2D image space\n        projected_points = self.project_points_to_image(pointcloud)\n\n        # Associate detections with point cloud regions\n        fused_objects = self.associate_detections(projected_points, image_detections)\n\n        return fused_objects\n\n    def project_points_to_image(self, pointcloud):\n        # Transform 3D points to camera frame and project to 2D\n        points_3d = self.pointcloud_to_array(pointcloud)\n        points_4d = np.column_stack([points_3d, np.ones(len(points_3d))])\n\n        # Apply extrinsic transform\n        points_cam = (self.extrinsics @ points_4d.T).T[:, :3]\n\n        # Project to image plane\n        points_2d = (self.camera_intrinsics @ points_cam.T).T\n        points_2d = points_2d[:, :2] / points_2d[:, 2:3]\n\n        return points_2d\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-6-validate-perception-accuracy",children:"Step 6: Validate Perception Accuracy"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Create Ground Truth Comparison Node"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# validation_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom vision_msgs.msg import Detection2DArray\nfrom builtin_interfaces.msg import Time\nimport numpy as np\n\nclass PerceptionValidationNode(Node):\n    def __init__(self):\n        super().__init__('perception_validation')\n\n        # Subscribe to detections and ground truth\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.detection_callback, 10)\n        self.ground_truth_sub = self.create_subscription(\n            Detection2DArray, '/ground_truth', self.ground_truth_callback, 10)\n\n        # Publisher for validation metrics\n        self.metrics_pub = self.create_publisher(\n            String, '/perception_metrics', 10)\n\n        self.detection_buffer = {}\n        self.ground_truth_buffer = {}\n\n    def detection_callback(self, msg):\n        # Store detections with timestamp\n        timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n        self.detection_buffer[timestamp] = msg\n\n    def ground_truth_callback(self, msg):\n        # Store ground truth with timestamp\n        timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n        self.ground_truth_buffer[timestamp] = msg\n\n    def calculate_metrics(self, detections, ground_truth):\n        # Calculate precision, recall, mAP\n        ious = self.calculate_ious(detections, ground_truth)\n\n        # True positives, false positives, false negatives\n        tp = np.sum(ious > 0.5)  # IoU threshold\n        fp = len(detections) - tp\n        fn = len(ground_truth) - tp\n\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n\n        return {\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1_score,\n            'mAP': self.calculate_map(detections, ground_truth, ious)\n        }\n\n    def calculate_ious(self, detections, ground_truth):\n        # Calculate Intersection over Union for each detection-ground_truth pair\n        ious = np.zeros((len(detections), len(ground_truth)))\n\n        for i, det in enumerate(detections):\n            for j, gt in enumerate(ground_truth):\n                iou = self.bbox_iou(det.bbox, gt.bbox)\n                ious[i, j] = iou\n\n        return ious\n\n    def bbox_iou(self, box1, box2):\n        # Calculate IoU between two bounding boxes\n        x1_inter = max(box1.center.x - box1.size_x/2, box2.center.x - box2.size_x/2)\n        y1_inter = max(box1.center.y - box1.size_y/2, box2.center.y - box2.size_y/2)\n        x2_inter = min(box1.center.x + box1.size_x/2, box2.center.x + box2.size_x/2)\n        y2_inter = min(box1.center.y + box1.size_y/2, box2.center.y + box2.size_y/2)\n\n        if x2_inter <= x1_inter or y2_inter <= y1_inter:\n            return 0\n\n        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n        box1_area = box1.size_x * box1.size_y\n        box2_area = box2.size_x * box2.size_y\n        union_area = box1_area + box2_area - inter_area\n\n        return inter_area / union_area if union_area > 0 else 0\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Run Validation Tests"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Launch validation test\nros2 launch your_package validation_test.launch.py\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,t.jsx)(n.p,{children:"Upon completion of this lab, you should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A complete AI-based perception pipeline with GPU acceleration"}),"\n",(0,t.jsx)(n.li,{children:"Working object detection using TensorRT-optimized models"}),"\n",(0,t.jsx)(n.li,{children:"Semantic segmentation capabilities with real-time performance"}),"\n",(0,t.jsx)(n.li,{children:"Multi-sensor fusion combining camera and LIDAR data"}),"\n",(0,t.jsx)(n.li,{children:"Validation system comparing perception results to ground truth"}),"\n",(0,t.jsx)(n.li,{children:"Performance metrics showing the effectiveness of your perception system"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The perception pipeline should be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Detect and classify objects in real-time at 30+ FPS"}),"\n",(0,t.jsx)(n.li,{children:"Generate accurate semantic segmentation masks"}),"\n",(0,t.jsx)(n.li,{children:"Fuse data from multiple sensors effectively"}),"\n",(0,t.jsx)(n.li,{children:"Achieve high accuracy compared to ground truth data"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TensorRT engine not loading"}),": Verify model conversion and compatibility"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU memory errors"}),": Reduce batch size or input resolution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Low inference speed"}),": Check CUDA installation and GPU utilization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Detection accuracy issues"}),": Verify model calibration and preprocessing steps"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"optional-extension-tasks",children:"Optional Extension Tasks"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Instance Segmentation"}),": Implement instance segmentation to distinguish between different objects of the same class."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"3D Object Detection"}),": Extend the pipeline to detect 3D objects from stereo or RGB-D data."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tracking Pipeline"}),": Add object tracking capabilities to maintain object identities across frames."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Adversarial Testing"}),": Test the perception system with adversarial examples to evaluate robustness."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This lab demonstrated the implementation of a comprehensive AI-based perception pipeline using Isaac ROS and GPU acceleration. You've learned to set up object detection, semantic segmentation, and multi-sensor fusion systems that can process sensor data in real-time. The validation framework provides metrics to evaluate perception accuracy, which is crucial for building reliable robotic systems."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);