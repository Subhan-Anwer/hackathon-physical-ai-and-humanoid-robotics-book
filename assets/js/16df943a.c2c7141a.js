"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[243],{5550(n,e,a){a.r(e),a.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vision-language-action/labs/lab-1-voice-command-recognition-system","title":"Lab 1: Voice Command Recognition System","description":"Overview","source":"@site/docs/module-4-vision-language-action/labs/lab-1-voice-command-recognition-system.md","sourceDirName":"module-4-vision-language-action/labs","slug":"/module-4-vision-language-action/labs/lab-1-voice-command-recognition-system","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-1-voice-command-recognition-system","draft":false,"unlisted":false,"editUrl":"https://github.com/subhan-anwer/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-1-voice-command-recognition-system.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Lab 1: Voice Command Recognition System","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Hands On Labs","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/category/hands-on-labs"},"next":{"title":"Lab 2: Cognitive Planning Pipeline","permalink":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline"}}');var t=a(4848),o=a(8453);const s={title:"Lab 1: Voice Command Recognition System",sidebar_position:1},r="Lab 1: Voice Command Recognition System",c={},d=[{value:"Overview",id:"overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Lab Setup",id:"lab-setup",level:2},{value:"1. Install Required Dependencies",id:"1-install-required-dependencies",level:3},{value:"2. Set Up OpenAI API (if using Whisper API)",id:"2-set-up-openai-api-if-using-whisper-api",level:3},{value:"3. Alternative: Install Local Whisper Model",id:"3-alternative-install-local-whisper-model",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Create the Audio Capture Node",id:"step-1-create-the-audio-capture-node",level:3},{value:"Step 2: Create the Whisper Processing Node",id:"step-2-create-the-whisper-processing-node",level:3},{value:"Step 3: Create the Voice Command Parser Node",id:"step-3-create-the-voice-command-parser-node",level:3},{value:"Step 4: Create the Command Validation and Feedback Node",id:"step-4-create-the-command-validation-and-feedback-node",level:3},{value:"Step 5: Create Launch File",id:"step-5-create-launch-file",level:3},{value:"Testing and Evaluation",id:"testing-and-evaluation",level:2},{value:"1. Basic Functionality Test",id:"1-basic-functionality-test",level:3},{value:"2. Performance Evaluation",id:"2-performance-evaluation",level:3},{value:"3. Accuracy Assessment",id:"3-accuracy-assessment",level:3},{value:"4. Troubleshooting Common Issues",id:"4-troubleshooting-common-issues",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"What You Learned",id:"what-you-learned",level:2}];function l(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"lab-1-voice-command-recognition-system",children:"Lab 1: Voice Command Recognition System"})}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"This lab introduces the fundamental concepts of voice command recognition for robotics applications. You will implement a complete voice command recognition system using OpenAI Whisper, integrate it with ROS 2, and test its performance in various acoustic conditions."}),"\n",(0,t.jsx)(e.h2,{id:"objectives",children:"Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Set up OpenAI Whisper for real-time speech recognition"}),"\n",(0,t.jsx)(e.li,{children:"Implement voice command parsing and classification"}),"\n",(0,t.jsx)(e.li,{children:"Integrate voice commands with ROS 2 message passing"}),"\n",(0,t.jsx)(e.li,{children:"Test and evaluate recognition accuracy in various conditions"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Basic understanding of ROS 2 concepts"}),"\n",(0,t.jsx)(e.li,{children:"Python programming experience"}),"\n",(0,t.jsx)(e.li,{children:"OpenAI API account (for Whisper API) or local Whisper model"}),"\n",(0,t.jsx)(e.li,{children:"Microphone for audio input"}),"\n",(0,t.jsx)(e.li,{children:"Basic knowledge of audio processing concepts"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,t.jsx)(e.h3,{id:"1-install-required-dependencies",children:"1. Install Required Dependencies"}),"\n",(0,t.jsx)(e.p,{children:"First, create a new ROS 2 workspace for the voice command project:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"mkdir -p ~/voice_command_ws/src\ncd ~/voice_command_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,t.jsx)(e.p,{children:"Install the required Python packages:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"pip install openai\npip install whisper\npip install torch torchaudio\npip install pyaudio\npip install speechrecognition\npip install transformers\n"})}),"\n",(0,t.jsx)(e.h3,{id:"2-set-up-openai-api-if-using-whisper-api",children:"2. Set Up OpenAI API (if using Whisper API)"}),"\n",(0,t.jsx)(e.p,{children:"If you plan to use OpenAI's Whisper API:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:'export OPENAI_API_KEY="your-api-key-here"\n'})}),"\n",(0,t.jsx)(e.h3,{id:"3-alternative-install-local-whisper-model",children:"3. Alternative: Install Local Whisper Model"}),"\n",(0,t.jsx)(e.p,{children:"For local processing without API dependency:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"pip install git+https://github.com/openai/whisper.git\n"})}),"\n",(0,t.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,t.jsx)(e.h3,{id:"step-1-create-the-audio-capture-node",children:"Step 1: Create the Audio Capture Node"}),"\n",(0,t.jsx)(e.p,{children:"Create a ROS 2 node that captures audio from the microphone and publishes it as messages:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import AudioData\nimport pyaudio\nimport numpy as np\nimport threading\nimport queue\n\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__(\'audio_capture_node\')\n\n        # Audio parameters\n        self.rate = 16000  # Sample rate\n        self.chunk = 1024  # Buffer size\n        self.format = pyaudio.paInt16\n        self.channels = 1\n\n        # Create publisher for audio data\n        self.audio_pub = self.create_publisher(AudioData, \'audio_input\', 10)\n\n        # Audio queue for processing\n        self.audio_queue = queue.Queue()\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n\n        # Start audio capture thread\n        self.capture_thread = threading.Thread(target=self.capture_audio, daemon=True)\n        self.capture_thread.start()\n\n        self.get_logger().info("Audio Capture Node initialized")\n\n    def capture_audio(self):\n        """Capture audio from microphone and publish to ROS topic"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        self.get_logger().info("Audio capture started")\n\n        try:\n            while rclpy.ok():\n                # Read audio data\n                data = stream.read(self.chunk, exception_on_overflow=False)\n\n                # Create and publish AudioData message\n                audio_msg = AudioData()\n                audio_msg.data = data\n                self.audio_pub.publish(audio_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Audio capture error: {str(e)}")\n        finally:\n            stream.stop_stream()\n            stream.close()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AudioCaptureNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.audio.terminate()\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"step-2-create-the-whisper-processing-node",children:"Step 2: Create the Whisper Processing Node"}),"\n",(0,t.jsx)(e.p,{children:"Create a node that processes audio using Whisper and converts speech to text:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import AudioData\nfrom std_msgs.msg import String\nimport whisper\nimport numpy as np\nimport threading\nimport queue\nimport io\nfrom scipy.io import wavfile\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n\n        # Load Whisper model (choose appropriate size for your hardware)\n        self.get_logger().info("Loading Whisper model...")\n        self.model = whisper.load_model("base")  # Options: tiny, base, small, medium, large\n\n        # Create subscribers and publishers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'audio_input\',\n            self.audio_callback,\n            10\n        )\n\n        self.transcript_pub = self.create_publisher(\n            String,\n            \'transcript\',\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            \'whisper_status\',\n            10\n        )\n\n        # Audio processing queue\n        self.audio_queue = queue.Queue()\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio, daemon=True)\n        self.processing_thread.start()\n\n        self.get_logger().info("Whisper Node initialized with model loaded")\n\n    def audio_callback(self, msg):\n        """Callback function to handle incoming audio data"""\n        try:\n            # Convert AudioData to numpy array\n            audio_array = np.frombuffer(msg.data, dtype=np.int16)\n            audio_array = audio_array.astype(np.float32) / 32768.0  # Normalize\n\n            # Add to processing queue\n            self.audio_queue.put(audio_array)\n        except Exception as e:\n            self.get_logger().error(f"Error processing audio callback: {str(e)}")\n\n    def process_audio(self):\n        """Process audio data from queue using Whisper"""\n        accumulated_audio = np.array([])\n\n        while rclpy.ok():\n            try:\n                # Get audio data from queue\n                audio_chunk = self.audio_queue.get(timeout=1.0)\n\n                # Accumulate audio for better recognition\n                accumulated_audio = np.concatenate([accumulated_audio, audio_chunk])\n\n                # Process accumulated audio when we have enough (0.5 seconds worth)\n                if len(accumulated_audio) >= 8000:  # 0.5 seconds at 16kHz\n                    # Transcribe the accumulated audio\n                    result = self.model.transcribe(accumulated_audio)\n                    transcription = result[\'text\'].strip()\n\n                    if transcription:  # Only publish non-empty transcriptions\n                        self.get_logger().info(f"Transcribed: {transcription}")\n\n                        # Publish transcription\n                        transcript_msg = String()\n                        transcript_msg.data = transcription\n                        self.transcript_pub.publish(transcript_msg)\n\n                        # Publish status\n                        status_msg = String()\n                        status_msg.data = f"Transcribed: {transcription}"\n                        self.status_pub.publish(status_msg)\n\n                    # Clear accumulated audio (keep some overlap for continuity)\n                    accumulated_audio = accumulated_audio[-4000:]  # Keep last 0.25 seconds\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f"Error in audio processing: {str(e)}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"step-3-create-the-voice-command-parser-node",children:"Step 3: Create the Voice Command Parser Node"}),"\n",(0,t.jsx)(e.p,{children:"Create a node that parses transcribed text into structured commands:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom std_msgs.msg import Bool\nimport json\nimport re\n\nclass VoiceCommandParserNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_parser_node')\n\n        # Create subscribers and publishers\n        self.transcript_sub = self.create_subscription(\n            String,\n            'transcript',\n            self.transcript_callback,\n            10\n        )\n\n        self.command_pub = self.create_publisher(\n            String,\n            'parsed_commands',\n            10\n        )\n\n        self.validity_pub = self.create_publisher(\n            Bool,\n            'command_validity',\n            10\n        )\n\n        # Define command patterns\n        self.command_patterns = [\n            {\n                'pattern': r'go to (.+)',\n                'intent': 'navigation',\n                'action': 'navigate_to'\n            },\n            {\n                'pattern': r'move to (.+)',\n                'intent': 'navigation',\n                'action': 'navigate_to'\n            },\n            {\n                'pattern': r'navigate to (.+)',\n                'intent': 'navigation',\n                'action': 'navigate_to'\n            },\n            {\n                'pattern': r'pick up (.+)',\n                'intent': 'manipulation',\n                'action': 'grasp_object'\n            },\n            {\n                'pattern': r'grasp (.+)',\n                'intent': 'manipulation',\n                'action': 'grasp_object'\n            },\n            {\n                'pattern': r'take (.+)',\n                'intent': 'manipulation',\n                'action': 'grasp_object'\n            },\n            {\n                'pattern': r'find (.+)',\n                'intent': 'perception',\n                'action': 'detect_object'\n            },\n            {\n                'pattern': r'locate (.+)',\n                'intent': 'perception',\n                'action': 'detect_object'\n            },\n            {\n                'pattern': r'look for (.+)',\n                'intent': 'perception',\n                'action': 'detect_object'\n            },\n            {\n                'pattern': r'stop',\n                'intent': 'control',\n                'action': 'stop_robot'\n            },\n            {\n                'pattern': r'pause',\n                'intent': 'control',\n                'action': 'pause_robot'\n            }\n        ]\n\n        self.get_logger().info(\"Voice Command Parser Node initialized\")\n\n    def transcript_callback(self, msg):\n        \"\"\"Parse incoming transcript into structured commands\"\"\"\n        transcript = msg.data.lower().strip()\n\n        if not transcript:\n            return\n\n        self.get_logger().info(f\"Parsing transcript: {transcript}\")\n\n        # Try to match command patterns\n        for pattern_info in self.command_patterns:\n            match = re.search(pattern_info['pattern'], transcript)\n            if match:\n                # Extract the target (object, location, etc.)\n                target = match.group(1).strip() if len(match.groups()) > 0 else \"\"\n\n                # Create structured command\n                command = {\n                    'intent': pattern_info['intent'],\n                    'action': pattern_info['action'],\n                    'target': target,\n                    'original_text': transcript,\n                    'confidence': 0.9  # For now, assume high confidence\n                }\n\n                # Publish parsed command\n                command_msg = String()\n                command_msg.data = json.dumps(command)\n                self.command_pub.publish(command_msg)\n\n                # Publish validity\n                validity_msg = Bool()\n                validity_msg.data = True\n                self.validity_pub.publish(validity_msg)\n\n                self.get_logger().info(f\"Parsed command: {command}\")\n                return\n\n        # If no pattern matched, command is invalid\n        self.get_logger().info(f\"No command pattern matched: {transcript}\")\n        validity_msg = Bool()\n        validity_msg.data = False\n        self.validity_pub.publish(validity_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandParserNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"step-4-create-the-command-validation-and-feedback-node",children:"Step 4: Create the Command Validation and Feedback Node"}),"\n",(0,t.jsx)(e.p,{children:"Create a node that validates commands and provides feedback:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom std_msgs.msg import Empty\nimport json\nimport time\n\nclass CommandValidationNode(Node):\n    def __init__(self):\n        super().__init__('command_validation_node')\n\n        # Create subscribers and publishers\n        self.command_sub = self.create_subscription(\n            String,\n            'parsed_commands',\n            self.command_callback,\n            10\n        )\n\n        self.validity_sub = self.create_subscription(\n            Bool,\n            'command_validity',\n            self.validity_callback,\n            10\n        )\n\n        self.feedback_pub = self.create_publisher(\n            String,\n            'command_feedback',\n            10\n        )\n\n        self.ack_pub = self.create_publisher(\n            Empty,\n            'command_acknowledged',\n            10\n        )\n\n        # Store recent commands for validation\n        self.recent_commands = []\n        self.last_validity = False\n\n        self.get_logger().info(\"Command Validation Node initialized\")\n\n    def command_callback(self, msg):\n        \"\"\"Handle incoming parsed commands\"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            self.get_logger().info(f\"Received command: {command_data}\")\n\n            # Validate command\n            is_valid = self.validate_command(command_data)\n\n            if is_valid:\n                # Publish feedback\n                feedback_msg = String()\n                feedback_msg.data = f\"Command acknowledged: {command_data['action']} {command_data.get('target', '')}\"\n                self.feedback_pub.publish(feedback_msg)\n\n                # Publish acknowledgment\n                ack_msg = Empty()\n                self.ack_pub.publish(ack_msg)\n\n                self.get_logger().info(f\"Command validated and acknowledged: {command_data['action']}\")\n            else:\n                feedback_msg = String()\n                feedback_msg.data = f\"Invalid command: {command_data}\"\n                self.feedback_pub.publish(feedback_msg)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f\"Invalid JSON in command: {msg.data}\")\n        except Exception as e:\n            self.get_logger().error(f\"Error processing command: {str(e)}\")\n\n    def validity_callback(self, msg):\n        \"\"\"Update validity status\"\"\"\n        self.last_validity = msg.data\n\n    def validate_command(self, command_data):\n        \"\"\"Validate command based on various criteria\"\"\"\n        # Check if action is recognized\n        valid_actions = [\n            'navigate_to', 'grasp_object', 'detect_object',\n            'stop_robot', 'pause_robot'\n        ]\n\n        if command_data['action'] not in valid_actions:\n            return False\n\n        # Check if target is appropriate for the action\n        if command_data['action'] in ['navigate_to', 'detect_object'] and not command_data.get('target'):\n            return False\n\n        # Check if command is not a duplicate of recent commands\n        current_time = time.time()\n        for recent_cmd in self.recent_commands:\n            if (current_time - recent_cmd['timestamp'] < 2.0 and  # Within 2 seconds\n                recent_cmd['command']['action'] == command_data['action'] and\n                recent_cmd['command'].get('target') == command_data.get('target')):\n                return False  # Duplicate command\n\n        # Add to recent commands\n        self.recent_commands.append({\n            'command': command_data,\n            'timestamp': current_time\n        })\n\n        # Clean up old commands (keep last 10 seconds)\n        self.recent_commands = [\n            cmd for cmd in self.recent_commands\n            if current_time - cmd['timestamp'] < 10.0\n        ]\n\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandValidationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"step-5-create-launch-file",children:"Step 5: Create Launch File"}),"\n",(0,t.jsx)(e.p,{children:"Create a launch file to start all nodes together:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Audio capture node\n        Node(\n            package='voice_command_system',\n            executable='audio_capture_node',\n            name='audio_capture_node',\n            output='screen'\n        ),\n\n        # Whisper processing node\n        Node(\n            package='voice_command_system',\n            executable='whisper_node',\n            name='whisper_node',\n            output='screen'\n        ),\n\n        # Voice command parser node\n        Node(\n            package='voice_command_system',\n            executable='voice_command_parser_node',\n            name='voice_command_parser_node',\n            output='screen'\n        ),\n\n        # Command validation node\n        Node(\n            package='voice_command_system',\n            executable='command_validation_node',\n            name='command_validation_node',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,t.jsx)(e.h2,{id:"testing-and-evaluation",children:"Testing and Evaluation"}),"\n",(0,t.jsx)(e.h3,{id:"1-basic-functionality-test",children:"1. Basic Functionality Test"}),"\n",(0,t.jsx)(e.p,{children:"Test the system with simple commands:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:'"Go to the kitchen"'}),"\n",(0,t.jsx)(e.li,{children:'"Find the red cup"'}),"\n",(0,t.jsx)(e.li,{children:'"Stop"'}),"\n",(0,t.jsx)(e.li,{children:'"Pick up the book"'}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"2-performance-evaluation",children:"2. Performance Evaluation"}),"\n",(0,t.jsx)(e.p,{children:"Test the system under different conditions:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Quiet environment"}),"\n",(0,t.jsx)(e.li,{children:"Noisy environment"}),"\n",(0,t.jsx)(e.li,{children:"Different speaking volumes"}),"\n",(0,t.jsx)(e.li,{children:"Various accents (if possible)"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"3-accuracy-assessment",children:"3. Accuracy Assessment"}),"\n",(0,t.jsx)(e.p,{children:"Record the following metrics:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Recognition accuracy (correctly transcribed commands)"}),"\n",(0,t.jsx)(e.li,{children:"Command parsing accuracy (correctly identified intents)"}),"\n",(0,t.jsx)(e.li,{children:"Response time (from speech to command execution)"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"4-troubleshooting-common-issues",children:"4. Troubleshooting Common Issues"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"No audio input"}),": Check microphone permissions and connections"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Poor recognition"}),": Ensure proper microphone placement and audio quality"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Command not recognized"}),": Verify command patterns in the parser node"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"How does the Whisper model's performance vary with different model sizes (tiny, base, small)?"}),"\n",(0,t.jsx)(e.li,{children:"What are the main challenges in voice command recognition for robotics applications?"}),"\n",(0,t.jsx)(e.li,{children:"How could you improve the command parsing accuracy for ambiguous commands?"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"what-you-learned",children:"What You Learned"}),"\n",(0,t.jsx)(e.p,{children:"In this lab, you implemented a complete voice command recognition system using OpenAI Whisper and integrated it with ROS 2. You learned how to capture audio, process speech to text, parse commands, and validate inputs. You also evaluated the system's performance under various conditions and identified potential improvements for real-world applications."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(l,{...n})}):l(n)}},8453(n,e,a){a.d(e,{R:()=>s,x:()=>r});var i=a(6540);const t={},o=i.createContext(t);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);