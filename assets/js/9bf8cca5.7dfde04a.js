"use strict";(globalThis.webpackChunksite=globalThis.webpackChunksite||[]).push([[756],{8391(a){a.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/intro","label":"Introduction to Physical AI and Humanoid Robotics","docId":"intro","unlisted":false},{"type":"category","label":"Module 1 - Robotic Nervous System","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-1-ros2/chapter-1","label":"Chapter 1: ROS 2 Fundamentals","docId":"module-1-ros2/chapter-1","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-1-ros2/chapter-2","label":"Chapter 2: ROS 2 Ecosystem and Tools","docId":"module-1-ros2/chapter-2","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-1-ros2/chapter-3","label":"Chapter 3: Advanced ROS 2 Concepts","docId":"module-1-ros2/chapter-3","unlisted":false},{"type":"category","label":"Hands On Labs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-1-ros2/labs/lab-1-installation-and-pubsub","label":"Lab 1: Installation and Basic Publisher/Subscriber","docId":"module-1-ros2/labs/lab-1-installation-and-pubsub","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-1-ros2/labs/lab-2-custom-messages-and-services","label":"Lab 2: Custom Messages and Services","docId":"module-1-ros2/labs/lab-2-custom-messages-and-services","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-1-ros2/labs/lab-3-multi-node-robot-system","label":"Lab 3: Multi-Node Robot System","docId":"module-1-ros2/labs/lab-3-multi-node-robot-system","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-1-ros2/labs/lab-4-actions-and-navigation","label":"Lab 4: Actions and Navigation","docId":"module-1-ros2/labs/lab-4-actions-and-navigation","unlisted":false}]}]},{"type":"category","label":"Module 2 - Digital Twin","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-1","label":"Chapter 1: What is a Digital Twin?","docId":"module-2-digital-twin/chapter-1","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-3","label":"Chapter 3: Robot Modeling with URDF & Sensors","docId":"module-2-digital-twin/chapter-3","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-4","label":"Chapter 4: Unity for High-Fidelity Interaction","docId":"module-2-digital-twin/chapter-4","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/chapter-2","label":"Chapter 2: Gazebo for Robotics Simulation","docId":"module-2-digital-twin/chapter-2","unlisted":false},{"type":"category","label":"Hands On Labs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/labs/lab-1-digital-twin-fundamentals","label":"Lab 1: Digital Twin Fundamentals","docId":"module-2-digital-twin/labs/lab-1-digital-twin-fundamentals","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/labs/lab-2-gazebo-environment-setup","label":"Lab 2: Gazebo Environment Setup","docId":"module-2-digital-twin/labs/lab-2-gazebo-environment-setup","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/labs/lab-3-robot-modeling-with-urdf","label":"Lab 3: Robot Modeling with URDF","docId":"module-2-digital-twin/labs/lab-3-robot-modeling-with-urdf","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-2-digital-twin/labs/lab-4-unity-high-fidelity-simulation","label":"Lab 4: Unity High-Fidelity Simulation","docId":"module-2-digital-twin/labs/lab-4-unity-high-fidelity-simulation","unlisted":false}]}]},{"type":"category","label":"Module 3 - AI-Robot Brain","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/chapter-1","label":"Chapter 1: NVIDIA Isaac Sim Fundamentals","docId":"module-3-ai-robot-brain/chapter-1","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/chapter-2","label":"Chapter 2: Perception and Sensor Simulation","docId":"module-3-ai-robot-brain/chapter-2","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/chapter-3","label":"Chapter 3: VSLAM and Navigation (Isaac ROS + Nav2)","docId":"module-3-ai-robot-brain/chapter-3","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/chapter-4","label":"Chapter 4: Sim-to-Real Transfer Concepts","docId":"module-3-ai-robot-brain/chapter-4","unlisted":false},{"type":"category","label":"Hands On Labs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-1-isaac-sim-environment-setup","label":"Lab 1: Isaac Sim Environment Setup","docId":"module-3-ai-robot-brain/labs/lab-1-isaac-sim-environment-setup","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline","label":"Lab 2: AI-Based Perception Pipeline","docId":"module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration","label":"Lab 3: Navigation and VSLAM Integration","docId":"module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer","label":"Lab 4: Sim-to-Real Transfer","docId":"module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer","unlisted":false}]}]},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-1","label":"Chapter 1: Introduction to Vision-Language-Action (VLA) Systems","docId":"module-4-vision-language-action/chapter-1","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-2","label":"Chapter 2: Voice Command Processing and Natural Language Understanding","docId":"module-4-vision-language-action/chapter-2","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-3","label":"Chapter 3: Cognitive Planning with LLMs","docId":"module-4-vision-language-action/chapter-3","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-4","label":"Chapter 4: Vision-Language Integration for Robot Perception","docId":"module-4-vision-language-action/chapter-4","unlisted":false},{"type":"category","label":"Hands On Labs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-1-voice-command-recognition-system","label":"Lab 1: Voice Command Recognition System","docId":"module-4-vision-language-action/labs/lab-1-voice-command-recognition-system","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline","label":"Lab 2: Cognitive Planning Pipeline","docId":"module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-3-vision-language-perception-integration","label":"Lab 3: Vision-Language Perception Integration","docId":"module-4-vision-language-action/labs/lab-3-vision-language-perception-integration","unlisted":false},{"type":"link","href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid","label":"Lab 4: Capstone - The Autonomous Humanoid","docId":"module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid","unlisted":false}],"href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/category/hands-on-labs"}],"href":"/hackathon-physical-ai-and-humanoid-robotics-book/docs/category/module-4-vision-language-action-vla"}]},"docs":{"intro":{"id":"intro","title":"Introduction to Physical AI and Humanoid Robotics","description":"Welcome to the Course","sidebar":"tutorialSidebar"},"module-1-ros2/chapter-1":{"id":"module-1-ros2/chapter-1","title":"Chapter 1: ROS 2 Fundamentals","description":"Welcome to the world of ROS 2, the next-generation Robot Operating System that serves as the backbone for modern robotics development. As the \\"nervous system\\" of robotic applications, ROS 2 provides the essential communication infrastructure that enables different components of a robot to work together seamlessly. In this chapter, we\'ll explore the foundational concepts that make ROS 2 the standard framework for robotics development.","sidebar":"tutorialSidebar"},"module-1-ros2/chapter-2":{"id":"module-1-ros2/chapter-2","title":"Chapter 2: ROS 2 Ecosystem and Tools","description":"The ROS 2 ecosystem provides a comprehensive set of tools that make developing, debugging, and managing robotic applications more efficient and intuitive. These tools range from command-line interfaces for system management to sophisticated visualization platforms that help you understand what\'s happening in your robotic system. In this chapter, we\'ll explore the essential tools that form the backbone of the ROS 2 development experience.","sidebar":"tutorialSidebar"},"module-1-ros2/chapter-3":{"id":"module-1-ros2/chapter-3","title":"Chapter 3: Advanced ROS 2 Concepts","description":"As you advance in your ROS 2 journey, understanding the more sophisticated concepts becomes crucial for building robust, efficient, and safe robotic systems. This chapter delves into advanced topics that are essential for professional robotics development, including Quality of Service policies, real-time considerations, multi-robot systems, and security concepts.","sidebar":"tutorialSidebar"},"module-1-ros2/labs/lab-1-installation-and-pubsub":{"id":"module-1-ros2/labs/lab-1-installation-and-pubsub","title":"Lab 1: Installation and Basic Publisher/Subscriber","description":"Overview","sidebar":"tutorialSidebar"},"module-1-ros2/labs/lab-2-custom-messages-and-services":{"id":"module-1-ros2/labs/lab-2-custom-messages-and-services","title":"Lab 2: Custom Messages and Services","description":"Overview","sidebar":"tutorialSidebar"},"module-1-ros2/labs/lab-3-multi-node-robot-system":{"id":"module-1-ros2/labs/lab-3-multi-node-robot-system","title":"Lab 3: Multi-Node Robot System","description":"Overview","sidebar":"tutorialSidebar"},"module-1-ros2/labs/lab-4-actions-and-navigation":{"id":"module-1-ros2/labs/lab-4-actions-and-navigation","title":"Lab 4: Actions and Navigation","description":"Overview","sidebar":"tutorialSidebar"},"module-2-digital-twin/chapter-1":{"id":"module-2-digital-twin/chapter-1","title":"Chapter 1: What is a Digital Twin?","description":"Introduction","sidebar":"tutorialSidebar"},"module-2-digital-twin/chapter-2":{"id":"module-2-digital-twin/chapter-2","title":"Chapter 2: Gazebo for Robotics Simulation","description":"Introduction","sidebar":"tutorialSidebar"},"module-2-digital-twin/chapter-3":{"id":"module-2-digital-twin/chapter-3","title":"Chapter 3: Robot Modeling with URDF & Sensors","description":"Introduction","sidebar":"tutorialSidebar"},"module-2-digital-twin/chapter-4":{"id":"module-2-digital-twin/chapter-4","title":"Chapter 4: Unity for High-Fidelity Interaction","description":"Introduction","sidebar":"tutorialSidebar"},"module-2-digital-twin/labs/lab-1-digital-twin-fundamentals":{"id":"module-2-digital-twin/labs/lab-1-digital-twin-fundamentals","title":"Lab 1: Digital Twin Fundamentals","description":"Overview","sidebar":"tutorialSidebar"},"module-2-digital-twin/labs/lab-2-gazebo-environment-setup":{"id":"module-2-digital-twin/labs/lab-2-gazebo-environment-setup","title":"Lab 2: Gazebo Environment Setup","description":"Overview","sidebar":"tutorialSidebar"},"module-2-digital-twin/labs/lab-3-robot-modeling-with-urdf":{"id":"module-2-digital-twin/labs/lab-3-robot-modeling-with-urdf","title":"Lab 3: Robot Modeling with URDF","description":"Overview","sidebar":"tutorialSidebar"},"module-2-digital-twin/labs/lab-4-unity-high-fidelity-simulation":{"id":"module-2-digital-twin/labs/lab-4-unity-high-fidelity-simulation","title":"Lab 4: Unity High-Fidelity Simulation","description":"Overview","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/chapter-1":{"id":"module-3-ai-robot-brain/chapter-1","title":"Chapter 1: NVIDIA Isaac Sim Fundamentals","description":"Welcome to the NVIDIA Isaac ecosystem, a comprehensive AI-powered robotics platform that revolutionizes how we develop, test, and deploy intelligent robotic systems. As the cornerstone of NVIDIA\'s robotics solution, Isaac Sim provides a photorealistic simulation environment that bridges the gap between virtual development and real-world deployment. In this chapter, we\'ll explore the fundamental concepts that make Isaac Sim the premier choice for developing AI-powered robots, particularly humanoid and physical AI systems.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/chapter-2":{"id":"module-3-ai-robot-brain/chapter-2","title":"Chapter 2: Perception and Sensor Simulation","description":"Building upon the foundational concepts of NVIDIA Isaac Sim, this chapter delves into the critical domain of perception and sensor simulation. Perception is the cornerstone of intelligent robotic behavior, enabling robots to understand and interact with their environment. In this chapter, we\'ll explore how Isaac Sim provides realistic simulation of various sensor types and how to implement sophisticated perception pipelines using Isaac ROS components.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/chapter-3":{"id":"module-3-ai-robot-brain/chapter-3","title":"Chapter 3: VSLAM and Navigation (Isaac ROS + Nav2)","description":"Navigation is a fundamental capability for mobile robots, enabling them to move autonomously through complex environments. Visual Simultaneous Localization and Mapping (VSLAM) combined with the Navigation2 (Nav2) stack provides a powerful framework for achieving this capability. In this chapter, we\'ll explore how Isaac ROS integrates with Nav2 to create sophisticated navigation systems that leverage GPU acceleration for real-time performance.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/chapter-4":{"id":"module-3-ai-robot-brain/chapter-4","title":"Chapter 4: Sim-to-Real Transfer Concepts","description":"The ultimate goal of simulation-based robotics development is to create systems that perform effectively in the real world. Sim-to-real transfer represents the critical bridge between virtual development environments and physical robot deployment. In this chapter, we\'ll explore the sophisticated techniques and methodologies that enable successful transfer of robotic behaviors, perception systems, and control algorithms from simulation to reality.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/labs/lab-1-isaac-sim-environment-setup":{"id":"module-3-ai-robot-brain/labs/lab-1-isaac-sim-environment-setup","title":"Lab 1: Isaac Sim Environment Setup","description":"Objective","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline":{"id":"module-3-ai-robot-brain/labs/lab-2-ai-based-perception-pipeline","title":"Lab 2: AI-Based Perception Pipeline","description":"Objective","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration":{"id":"module-3-ai-robot-brain/labs/lab-3-navigation-and-vslam-integration","title":"Lab 3: Navigation and VSLAM Integration","description":"Objective","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer":{"id":"module-3-ai-robot-brain/labs/lab-4-sim-to-real-transfer","title":"Lab 4: Sim-to-Real Transfer","description":"Objective","sidebar":"tutorialSidebar"},"module-4-vision-language-action/chapter-1":{"id":"module-4-vision-language-action/chapter-1","title":"Chapter 1: Introduction to Vision-Language-Action (VLA) Systems","description":"Learning Objectives","sidebar":"tutorialSidebar"},"module-4-vision-language-action/chapter-2":{"id":"module-4-vision-language-action/chapter-2","title":"Chapter 2: Voice Command Processing and Natural Language Understanding","description":"Learning Objectives","sidebar":"tutorialSidebar"},"module-4-vision-language-action/chapter-3":{"id":"module-4-vision-language-action/chapter-3","title":"Chapter 3: Cognitive Planning with LLMs","description":"Learning Objectives","sidebar":"tutorialSidebar"},"module-4-vision-language-action/chapter-4":{"id":"module-4-vision-language-action/chapter-4","title":"Chapter 4: Vision-Language Integration for Robot Perception","description":"Learning Objectives","sidebar":"tutorialSidebar"},"module-4-vision-language-action/labs/lab-1-voice-command-recognition-system":{"id":"module-4-vision-language-action/labs/lab-1-voice-command-recognition-system","title":"Lab 1: Voice Command Recognition System","description":"Overview","sidebar":"tutorialSidebar"},"module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline":{"id":"module-4-vision-language-action/labs/lab-2-cognitive-planning-pipeline","title":"Lab 2: Cognitive Planning Pipeline","description":"Overview","sidebar":"tutorialSidebar"},"module-4-vision-language-action/labs/lab-3-vision-language-perception-integration":{"id":"module-4-vision-language-action/labs/lab-3-vision-language-perception-integration","title":"Lab 3: Vision-Language Perception Integration","description":"Overview","sidebar":"tutorialSidebar"},"module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid":{"id":"module-4-vision-language-action/labs/lab-4-capstone-the-autonomous-humanoid","title":"Lab 4: Capstone - The Autonomous Humanoid","description":"Overview","sidebar":"tutorialSidebar"}}}}')}}]);